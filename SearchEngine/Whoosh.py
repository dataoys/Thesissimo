import os
import ijson
from whoosh.index import create_in
from whoosh import index
from whoosh.fields import Schema, TEXT, ID
from whoosh.scoring import TF_IDF, BM25F
from whoosh.qparser import OrGroup, MultifieldParser
from whoosh.analysis import StemmingAnalyzer
from whoosh import query
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer
import nltk
import yake  

# Add this function after the imports
def setup_nltk():
    """
    Downloads required NLTK resources if not already present.
    """
    try:
        # Test if WordNet is available by trying to access a basic attribute or method
        # Using synsets as a more robust check than all_lemma_names for initialization
        nltk.corpus.wordnet.synsets('test')
        print("âœ… NLTK resources (including WordNet) appear to be loaded.")
    except (LookupError, AttributeError) as e: # Catch AttributeError as well
        print(f"â³ NLTK resources not found or WordNet not fully initialized ({e}). Downloading/Reloading...")
        try:
            nltk.download('wordnet', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            nltk.download('omw-1.4', quiet=True)  # Open Multilingual WordNet
            print("ðŸ‘ NLTK resources downloaded.")
            # Attempt to force load WordNet after download
            from nltk.corpus import wordnet as wn
            wn.ensure_loaded()
            print("âœ… NLTK WordNet explicitly loaded.")
        except Exception as e_download:
            print(f"âŒ Error downloading or loading NLTK resources: {e_download}")
            # Consider raising an error or handling it if essential for app startup
            # For now, we'll let it proceed, but functionality might be impaired.

# Function to create index schema
def create_schema():
    """
    Schema generation function.

    According to our document structure, this function generates a schema for the Whoosh index.

    Returns:
        scheme: Schema for the Whoosh index.
    """
    # Eseguiamo stemming di soli campi testuali significativi
    return Schema(
        id=ID(stored=True),
        title=TEXT(stored=True, analyzer=StemmingAnalyzer()),
        abstract=TEXT(stored=True, analyzer=StemmingAnalyzer()),
        corpus=TEXT(stored=True, analyzer=StemmingAnalyzer()),
        keywords=TEXT(stored=True),
        url=TEXT(stored=True)
    )

# Creazione dell'indice
def create_index(index_dir):
    """
    Creating whoosh index function.

    This function takes the path of the index directory and creates an index with the schema generated by the create_schema function.

    Arguments:
        index_dir (str): Path to the index directory.

    Returns:
        Index: Index object.
    """
    if not os.path.exists(index_dir):
        os.mkdir(index_dir)
    schema = create_schema()
    ix = create_in(index_dir, schema)
    return ix

# Indicizzazione dei documenti in batch
def index_documents(index_dir, json_file, batch_size=1000):
    """
    Indexing documents function.

    This function takes the path of the index directory and the path of the JSON file containing the documents to be indexed.
    It reads the JSON file and writes the documents to the index in batches.

    Arguments:
        index_dir (str): Path to the index directory.
        json_file (str): Path to the JSON file containing the documents.
        batch_size (int): Number of documents to index per batch.
    """
    # Crea un nuovo indice
    ix = create_index(index_dir)
    doc_count = 0
    
    try:
        # Leggere il file JSON in modo incrementale con ijson
        with open(json_file, 'r', encoding='utf-8', errors='ignore') as f:
            parser = ijson.parse(f)
            current_doc = {}
            batch = []
            
            for prefix, event, value in parser:
                if prefix.endswith('.id'):
                    if current_doc:  # Se abbiamo un documento completo, aggiungiamolo al batch
                        batch.append(current_doc)
                        current_doc = {}
                    current_doc['id'] = str(value)
                elif prefix.endswith('.title'):
                    current_doc['title'] = value
                elif prefix.endswith('.abstract'):
                    current_doc['abstract'] = value
                elif prefix.endswith('.corpus'):
                    current_doc['corpus'] = value
                elif prefix.endswith('.keywords'):
                    current_doc['keywords'] = value
                elif prefix.endswith('.url'):
                    current_doc['url'] = value

                # Se il batch raggiunge la dimensione specificata, scrivilo nell'indice
                if len(batch) >= batch_size:
                    with ix.writer() as writer:
                        for doc in batch:
                            writer.add_document(
                                id=doc['id'],
                                title=doc.get('title', ''),
                                abstract=doc.get('abstract', ''),
                                corpus=doc.get('corpus', ''),
                                keywords=doc.get('keywords', ''),
                                url=doc.get('url', '')
                            )
                    doc_count += len(batch)
                    print(f"ðŸ“¦ Batch di {len(batch)} documenti indicizzati. Totale: {doc_count}")
                    batch = []

            # Gestisci l'ultimo documento e batch
            if current_doc:
                batch.append(current_doc)
            if batch:
                with ix.writer() as writer:
                    for doc in batch:
                        writer.add_document(
                            id=doc['id'],
                            title=doc.get('title', ''),
                            abstract=doc.get('abstract', ''),
                            corpus=doc.get('corpus', ''),
                            keywords=doc.get('keywords', ''),
                            url=doc.get('url', '')
                        )
                doc_count += len(batch)
                print(f"ðŸ“¦ Ultimo batch di {len(batch)} documenti indicizzati. Totale finale: {doc_count}")

    except Exception as e:
        print(f"âŒ Errore durante l'indicizzazione: {e}")
        raise
    
    print(f"âœ… Indicizzazione completata! Totale documenti indicizzati: {doc_count}")
    return ix

# Funzione per cercare nei documenti indicizzati
def parse_advanced_query(query_string, schema):
    """
    Parse a query string that may contain field-specific searches.
    Example: "title:neural networks AND corpus:python" or "abstract:law OR title:justice"
    """
    try:
        from whoosh import qparser
        
        # Create a QueryParser that supports multiple fields and phrase queries
        parser = qparser.MultifieldParser(['title', 'abstract', 'corpus'], 
                                        schema,
                                        group=qparser.OrGroup)
        
        # Enable phrase query support
        parser.add_plugin(qparser.PhrasePlugin())
        parser.add_plugin(qparser.OperatorsPlugin())
        
        # Clean up the query string to ensure proper parsing
        query_string = query_string.replace('title:', 'title:/')
        query_string = query_string.replace('abstract:', 'abstract:/')
        query_string = query_string.replace('corpus:', 'corpus:/')
        
        # Parse and return the query
        return parser.parse(query_string)
        
    except Exception as e:
        print(f"Error parsing advanced query: {e}")
        return None

def search_documents(index_dir, query_string, title_true, abstract_true, corpus_true, ranking_type):
    """
    Search Engine Whoosh Function.
    """
    if not query_string.strip():
        return []
    
    if not any([title_true, abstract_true, corpus_true]) and ':' not in query_string:
        return []

    # Choose scoring function
    if ranking_type == "TF_IDF":
        scorer = TF_IDF()
    else:
        scorer = BM25F()
    
    ix = index.open_dir(index_dir)
    try:
        with ix.searcher(weighting=scorer) as searcher:
            # Field-specific search
            if ':' in query_string:
                query_obj = parse_advanced_query(query_string, ix.schema)
                if query_obj is None:
                    return []
            else:
                # Checkbox-based search with expanded query
                processed_query = process_natural_query(query_string)
                fields = []
                if title_true:
                    fields.append("title")
                if abstract_true:
                    fields.append("abstract")
                if corpus_true:
                    fields.append("corpus")
                
                parser = MultifieldParser(fields, ix.schema, group=OrGroup)
                parser.add_plugin(qparser.PhrasePlugin())
                query_obj = parser.parse(processed_query)
            
            # Execute search with highlighting
            results = searcher.search(query_obj, limit=100)
            
            # Format results
            formatted_results = []
            for result in results:
                formatted_results.append((
                    result['id'],
                    result['title'],
                    result['abstract'],
                    result['corpus'],
                    result['keywords'],
                    result.score,
                    result['url']
                ))
            
            return formatted_results
            
    except Exception as e:
        print(f"Error during search: {e}")
        return []

# Funzione per verificare se l'indice esiste e Ã¨ valido
def index_exists_and_valid(index_dir):
    """
    Check if the index exists and is valid.

    This function takes the path of the index directory and checks if the index exists and contains documents.

    Arguments:
        index_dir (str): Path to the index directory.

    Returns:
        bool: True if the index exists and contains documents, False otherwise.
    """
    
    if not os.path.exists(index_dir):
        return False
    try:
        # Prova ad aprire l'indice
        ix = index.open_dir(index_dir)
        with ix.searcher() as searcher:
            # Verifica che ci siano documenti
            return searcher.doc_count() > 0
    except:
        return False

def create_or_get_index(index_dir, json_file, force_rebuild=False):
    """
    Create or get the index function only if it's necessary.

    Arguments:
        index_dir (str): Path to the index directory.
        json_file (str): Path to the JSON file containing the documents.
        force_rebuild (bool): Force the rebuild of the index.

    Returns:
        Index: Index object.
    """
    # Rimuovi eventuali lock residui
    lock_path = os.path.join(index_dir, 'LOCK')
    if os.path.exists(lock_path):
        os.remove(lock_path)

    try:
        if not force_rebuild and index_exists_and_valid(index_dir):
            print("ðŸ”„ Utilizzando l'indice esistente...")
            return index.open_dir(index_dir)
        
        print("ðŸ”¨ Creazione nuovo indice...")
        if not os.path.exists(index_dir):
            os.makedirs(index_dir)
            
        # Indicizza i documenti e ritorna il nuovo indice
        return index_documents(index_dir, json_file)

    except Exception as e:
        print(f"âŒ Errore durante la creazione/apertura dell'indice: {e}")
        raise

def get_wordnet_pos(tag):
    """
    Map POS tag to WordNet POS tag for lemmatization
    """
    tag_dict = {
        'N': wordnet.NOUN,
        'V': wordnet.VERB,
        'R': wordnet.ADV,
        'J': wordnet.ADJ
    }
    return tag_dict.get(tag[0], wordnet.NOUN)

def process_natural_query(query_string):
    """
    Process natural language query.
    
    This function takes a natural language query and processes it to create
    a more effective search query using NLP techniques.
    
    Arguments:
        query_string (str): The natural language query
        
    Returns:
        str: Processed query string
    """
    # Inizializza gli strumenti NLP
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))
    
    # 1. Keyword extraction with YAKE
    kw_extractor = yake.KeywordExtractor(
        lan="en",
        n=3, 
        dedupLim=0.9,
        dedupFunc='seqm',
        windowsSize=1,
        top=5, # Estrai le top 5 keyword
        features=None
    )
    keywords = [kw[0].lower() for kw in kw_extractor.extract_keywords(query_string)]

    # Tokenizzazione e rimozione stopwords dalla query originale
    tokens = word_tokenize(query_string.lower())
    
    # Combina keyword e token originali per la lemmatizzazione e l'espansione
    # per assicurarsi che le keyword multi-parola siano considerate
    base_terms_for_nlp = list(set(keywords + tokens))
    base_terms_for_nlp = [term for term in base_terms_for_nlp if term.isalnum() and term not in stop_words]

    pos_tags = pos_tag(base_terms_for_nlp)
    
    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) 
                  for word, tag in pos_tags]
    
    # Espansione con sinonimi, iperonimi e iponimi
    expanded_terms = set()
    # Aggiungi le keyword estratte da YAKE (giÃ  in minuscolo) e i termini lemmatizzati originali
    expanded_terms.update(keywords)
    expanded_terms.update(lemmatized)
        
    # Termini su cui basare l'espansione WordNet (keyword + lemmi originali)
    terms_for_wordnet_expansion = set(keywords + lemmatized)

    for term in terms_for_wordnet_expansion:
        # Aggiungi il termine stesso (giÃ  fatto, ma per sicurezza)
        expanded_terms.add(term)
        synsets = wordnet.synsets(term)
        for synset in synsets[:2]:  # Limita a 2 synset per termine per evitare rumore
            # Aggiungi sinonimi
            expanded_terms.update(lemma.name().lower().replace('_', ' ') for lemma in synset.lemmas() if lemma.name().lower() not in stop_words)
            
            # Aggiungi iperonimi
            for hypernym in synset.hypernyms():
                expanded_terms.update(lemma.name().lower().replace('_', ' ') for lemma in hypernym.lemmas() if lemma.name().lower() not in stop_words)

            # Aggiungi iponimi (termini piÃ¹ specifici)
            for hyponym in synset.hyponyms()[:1]: # Limita a 1 iponimo per synset per non allargare eccessivamente
                expanded_terms.update(lemma.name().lower().replace('_', ' ') for lemma in hyponym.lemmas() if lemma.name().lower() not in stop_words)
    
    # Pulisci i termini finali
    final_expanded_terms = {
        term.strip() for term in expanded_terms if term.strip() and term.strip() not in stop_words and len(term.strip()) > 1
    }
    
    if not final_expanded_terms: # Fallback se nessun termine Ã¨ stato generato
        # Usa le keyword o i lemmi originali se disponibili
        fallback_terms = set(keywords + lemmatized)
        fallback_terms = {t for t in fallback_terms if t and t not in stop_words and len(t) > 1}
        if fallback_terms:
            processed_query = ' OR '.join(fallback_terms)
        else: # Fallback estremo: usa la query originale tokenizzata e senza stopwords
            original_tokens_no_stops = [token for token in word_tokenize(query_string.lower()) if token.isalnum() and token not in stop_words and len(token)>1]
            if original_tokens_no_stops:
                 processed_query = ' OR '.join(original_tokens_no_stops)
            else:
                 processed_query = query_string # Ultimo fallback, query originale
    else:
        processed_query = ' OR '.join(final_expanded_terms)
        
    print(f"Whoosh - Query elaborata: {processed_query}")
    return processed_query


#RIMUOVERE COMMENTO ED ESEGUIRE PER GENERARE INDICE
if __name__ == "__main__":
    setup_nltk()
    # Esempio di utilizzo
    # project_root = Path(__file__).parent.parent
    # index_dir = str(project_root / "WhooshIndex")  
    # json_file = str(project_root / "WebScraping/results/Docs_cleaned.json") 
    
    # print("ðŸš€ Avvio indicizzazione...")
    # # Crea o ottieni l'indice
    # ix = create_or_get_index(index_dir, json_file, force_rebuild=True)
    
    # # Verifica il numero di documenti nell'indice
    # with ix.searcher() as searcher:
    #     doc_count = searcher.doc_count()
    #     print(f"ðŸ“Š Numero totale di documenti nell'indice: {doc_count}")
