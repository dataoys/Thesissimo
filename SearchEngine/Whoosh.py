import os
import ijson
from whoosh.index import create_in
from whoosh import index
from whoosh.fields import Schema, TEXT, ID
from whoosh.scoring import TF_IDF, BM25F
from whoosh.qparser import OrGroup, MultifieldParser
from whoosh.analysis import StemmingAnalyzer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer
from pathlib import Path
import sys
import nltk

# Add this function after the imports
def setup_nltk():
    """
    Downloads required NLTK resources if not already present.
    """
    try:
        # Test if WordNet is available
        wordnet.all_lemma_names()
    except (LookupError, AttributeError):
        print("‚è≥ Downloading required NLTK resources...")
        try:
            nltk.download('wordnet')
            nltk.download('averaged_perceptron_tagger')
            nltk.download('punkt')
            nltk.download('stopwords')
            nltk.download('omw-1.4')  # Open Multilingual WordNet
            print("‚úÖ NLTK resources downloaded successfully")
        except Exception as e:
            print(f"‚ùå Error downloading NLTK resources: {e}")
            sys.exit(1)

# Funzione per creare lo schema dell'indice
def create_schema():
    """
    Schema generation function.

    According to our document structure, this function generates a schema for the Whoosh index.

    Returns:
        scheme: Schema for the Whoosh index.
    """
    # Eseguiamo stemming di soli campi testuali significativi
    return Schema(
        id=ID(stored=True),
        title=TEXT(stored=True, analyzer=StemmingAnalyzer()),
        abstract=TEXT(stored=True, analyzer=StemmingAnalyzer()),
        corpus=TEXT(stored=True, analyzer=StemmingAnalyzer()),
        keywords=TEXT(stored=True),
        url=TEXT(stored=True)
    )

# Creazione dell'indice
def create_index(index_dir):
    """
    Creating whoosh index function.

    This function takes the path of the index directory and creates an index with the schema generated by the create_schema function.

    Arguments:
        index_dir (str): Path to the index directory.

    Returns:
        Index: Index object.
    """
    if not os.path.exists(index_dir):
        os.mkdir(index_dir)
    schema = create_schema()
    ix = create_in(index_dir, schema)
    return ix

# Indicizzazione dei documenti in batch
def index_documents(index_dir, json_file, batch_size=1000):
    """
    Indexing documents function.

    This function takes the path of the index directory and the path of the JSON file containing the documents to be indexed.
    It reads the JSON file and writes the documents to the index in batches.

    Arguments:
        index_dir (str): Path to the index directory.
        json_file (str): Path to the JSON file containing the documents.
        batch_size (int): Number of documents to index per batch.
    """
    # Crea un nuovo indice
    ix = create_index(index_dir)
    doc_count = 0
    
    try:
        # Leggere il file JSON in modo incrementale con ijson
        with open(json_file, 'r', encoding='utf-8', errors='ignore') as f:
            parser = ijson.parse(f)
            current_doc = {}
            batch = []
            
            for prefix, event, value in parser:
                if prefix.endswith('.id'):
                    if current_doc:  # Se abbiamo un documento completo, aggiungiamolo al batch
                        batch.append(current_doc)
                        current_doc = {}
                    current_doc['id'] = str(value)
                elif prefix.endswith('.title'):
                    current_doc['title'] = value
                elif prefix.endswith('.abstract'):
                    current_doc['abstract'] = value
                elif prefix.endswith('.corpus'):
                    current_doc['corpus'] = value
                elif prefix.endswith('.keywords'):
                    current_doc['keywords'] = value
                elif prefix.endswith('.url'):
                    current_doc['url'] = value

                # Se il batch raggiunge la dimensione specificata, scrivilo nell'indice
                if len(batch) >= batch_size:
                    with ix.writer() as writer:
                        for doc in batch:
                            writer.add_document(
                                id=doc['id'],
                                title=doc.get('title', ''),
                                abstract=doc.get('abstract', ''),
                                corpus=doc.get('corpus', ''),
                                keywords=doc.get('keywords', ''),
                                url=doc.get('url', '')
                            )
                    doc_count += len(batch)
                    print(f"üì¶ Batch di {len(batch)} documenti indicizzati. Totale: {doc_count}")
                    batch = []

            # Gestisci l'ultimo documento e batch
            if current_doc:
                batch.append(current_doc)
            if batch:
                with ix.writer() as writer:
                    for doc in batch:
                        writer.add_document(
                            id=doc['id'],
                            title=doc.get('title', ''),
                            abstract=doc.get('abstract', ''),
                            corpus=doc.get('corpus', ''),
                            keywords=doc.get('keywords', ''),
                            url=doc.get('url', '')
                        )
                doc_count += len(batch)
                print(f"üì¶ Ultimo batch di {len(batch)} documenti indicizzati. Totale finale: {doc_count}")

    except Exception as e:
        print(f"‚ùå Errore durante l'indicizzazione: {e}")
        raise
    
    print(f"‚úÖ Indicizzazione completata! Totale documenti indicizzati: {doc_count}")
    return ix

# Funzione per cercare nei documenti indicizzati
def search_documents(index_dir, query_string, title_true, abstract_true, corpus_true, ranking_type):
    """
    Search Engine Whoosh Function.

    This function takes the user's input string from the search bar, and 3 boolean values that represent
    the user's choice of where to search (title, abstract, corpus) and the index directory.

    Arguments:
        index_dir (str): Path to the index directory.
        query_string (str): The user's input.
        title_true (bool): First filter.
        abstract_true (bool): Second filter.
        corpus_true (bool): Third filter.
        

    Returns:
        list: list of the document matching the user query.
    """
    if not query_string.strip():
        return []
        
    if not any([title_true, abstract_true, corpus_true]):
        return []

    # Elabora la query in linguaggio naturale
    processed_query = process_natural_query(query_string)
    
    if ranking_type == "TF_IDF":
        rank = TF_IDF()
    else:
        rank = BM25F()
    
    ix = index.open_dir(index_dir)
    with ix.searcher(weighting=rank) as searcher:
        fields = []
        if title_true:
            fields.append("title")
        if abstract_true:
            fields.append("abstract")
        if corpus_true:
            fields.append("corpus")
            
        parser = MultifieldParser(fields, ix.schema, group=OrGroup)
        query = parser.parse(processed_query)
        
        results = searcher.search(query, limit=100)  # Nessun limite ai risultati
        return [(result['id'], 
                result['title'], 
                result['abstract'], 
                result['corpus'], 
                result['keywords'], 
                result.score,
                result['url']) 
                for result in results]

# Funzione per verificare se l'indice esiste e √® valido
def index_exists_and_valid(index_dir):
    """
    Check if the index exists and is valid.

    This function takes the path of the index directory and checks if the index exists and contains documents.

    Arguments:
        index_dir (str): Path to the index directory.

    Returns:
        bool: True if the index exists and contains documents, False otherwise.
    """
    
    if not os.path.exists(index_dir):
        return False
    try:
        # Prova ad aprire l'indice
        ix = index.open_dir(index_dir)
        with ix.searcher() as searcher:
            # Verifica che ci siano documenti
            return searcher.doc_count() > 0
    except:
        return False

def create_or_get_index(index_dir, json_file, force_rebuild=False):
    """
    Create or get the index function only if it's necessary.

    Arguments:
        index_dir (str): Path to the index directory.
        json_file (str): Path to the JSON file containing the documents.
        force_rebuild (bool): Force the rebuild of the index.

    Returns:
        Index: Index object.
    """
    # Rimuovi eventuali lock residui
    lock_path = os.path.join(index_dir, 'LOCK')
    if os.path.exists(lock_path):
        os.remove(lock_path)

    try:
        if not force_rebuild and index_exists_and_valid(index_dir):
            print("üîÑ Utilizzando l'indice esistente...")
            return index.open_dir(index_dir)
        
        print("üî® Creazione nuovo indice...")
        if not os.path.exists(index_dir):
            os.makedirs(index_dir)
            
        # Indicizza i documenti e ritorna il nuovo indice
        return index_documents(index_dir, json_file)

    except Exception as e:
        print(f"‚ùå Errore durante la creazione/apertura dell'indice: {e}")
        raise

def get_wordnet_pos(tag):
    """
    Map POS tag to WordNet POS tag for lemmatization
    """
    tag_dict = {
        'N': wordnet.NOUN,
        'V': wordnet.VERB,
        'R': wordnet.ADV,
        'J': wordnet.ADJ
    }
    return tag_dict.get(tag[0], wordnet.NOUN)

def process_natural_query(query_string):
    """
    Process natural language query.
    
    This function takes a natural language query and processes it to create
    a more effective search query using NLP techniques.
    
    Arguments:
        query_string (str): The natural language query
        
    Returns:
        str: Processed query string
    """
    # Inizializza gli strumenti NLP
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))
    
    # Tokenizzazione e rimozione stopwords
    tokens = word_tokenize(query_string.lower())
    tokens = [token for token in tokens if token not in stop_words]
    
    # POS tagging
    pos_tags = pos_tag(tokens)
    
    # Lemmatizzazione con POS
    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) 
                  for word, tag in pos_tags]
    
    # Espansione con sinonimi e iperonimi
    expanded_terms = set()
    for term in lemmatized:
        # Aggiungi il termine originale
        expanded_terms.add(term)
        
        # Trova sinonimi e iperonimi
        synsets = wordnet.synsets(term)
        for synset in synsets[:2]:  # Limita a 2 synset per termine per evitare rumore
            # Aggiungi sinonimi
            expanded_terms.update(lemma.name() for lemma in synset.lemmas())
            
            # Aggiungi iperonimi
            if synset.hypernyms():
                expanded_terms.update(
                    lemma.name() 
                    for hypernym in synset.hypernyms() 
                    for lemma in hypernym.lemmas()
                )
    
    # Pulisci i termini (rimuovi underscore e stopwords)
    expanded_terms = {
        term.replace('_', ' ') 
        for term in expanded_terms 
        if term not in stop_words
    }
    
    # Costruisci la query per Whoosh
    processed_query = ' OR '.join(expanded_terms)
    print(f"Query elaborata: {processed_query}")  # Debug
    return processed_query#


#RIMUOVERE COMMENTO ED ESEGUIRE PER GENERARE INDICE
#if __name__ == "__main__":
    setup_nltk()
    # Esempio di utilizzo
    project_root = Path(__file__).parent.parent
    index_dir = str(project_root / "WhooshIndex")  
    json_file = str(project_root / "WebScraping/results/Docs_cleaned.json") 
    
    print("üöÄ Avvio indicizzazione...")
    # Crea o ottieni l'indice
    ix = create_or_get_index(index_dir, json_file, force_rebuild=True)
    
    # Verifica il numero di documenti nell'indice
    with ix.searcher() as searcher:
        doc_count = searcher.doc_count()
        print(f"üìä Numero totale di documenti nell'indice: {doc_count}")
