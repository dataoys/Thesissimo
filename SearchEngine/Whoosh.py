import os
import ijson
from whoosh.index import create_in
from whoosh import index
from whoosh.fields import Schema, TEXT, ID
from whoosh.scoring import TF_IDF, BM25F
from whoosh.qparser import OrGroup, MultifieldParser
from whoosh.analysis import StemmingAnalyzer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer
from pathlib import Path
import sys
import nltk
import yake  # Aggiungi questo import

# Add this function after the imports
def setup_nltk():
    """
    Downloads required NLTK resources if not already present.
    """
    try:
        # Test if WordNet is available by trying to access a basic attribute or method
        # Using synsets as a more robust check than all_lemma_names for initialization
        nltk.corpus.wordnet.synsets('test')
        print("✅ NLTK resources (including WordNet) appear to be loaded.")
    except (LookupError, AttributeError) as e: # Catch AttributeError as well
        print(f"⏳ NLTK resources not found or WordNet not fully initialized ({e}). Downloading/Reloading...")
        try:
            nltk.download('wordnet', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            nltk.download('omw-1.4', quiet=True)  # Open Multilingual WordNet
            print("👍 NLTK resources downloaded.")
            # Attempt to force load WordNet after download
            from nltk.corpus import wordnet as wn
            wn.ensure_loaded()
            print("✅ NLTK WordNet explicitly loaded.")
        except Exception as e_download:
            print(f"❌ Error downloading or loading NLTK resources: {e_download}")
            # Consider raising an error or handling it if essential for app startup
            # For now, we'll let it proceed, but functionality might be impaired.

# Function to create index schema
def create_schema():
    """
    Schema generation function.

    According to our document structure, this function generates a schema for the Whoosh index.

    Returns:
        scheme: Schema for the Whoosh index.
    """
    # Eseguiamo stemming di soli campi testuali significativi
    return Schema(
        id=ID(stored=True),
        title=TEXT(stored=True, analyzer=StemmingAnalyzer()),
        abstract=TEXT(stored=True, analyzer=StemmingAnalyzer()),
        corpus=TEXT(stored=True, analyzer=StemmingAnalyzer()),
        keywords=TEXT(stored=True),
        url=TEXT(stored=True)
    )

# Creazione dell'indice
def create_index(index_dir):
    """
    Creating whoosh index function.

    This function takes the path of the index directory and creates an index with the schema generated by the create_schema function.

    Arguments:
        index_dir (str): Path to the index directory.

    Returns:
        Index: Index object.
    """
    if not os.path.exists(index_dir):
        os.mkdir(index_dir)
    schema = create_schema()
    ix = create_in(index_dir, schema)
    return ix

# Indicizzazione dei documenti in batch
def index_documents(index_dir, json_file, batch_size=1000):
    """
    Indexing documents function.

    This function takes the path of the index directory and the path of the JSON file containing the documents to be indexed.
    It reads the JSON file and writes the documents to the index in batches.

    Arguments:
        index_dir (str): Path to the index directory.
        json_file (str): Path to the JSON file containing the documents.
        batch_size (int): Number of documents to index per batch.
    """
    # Crea un nuovo indice
    ix = create_index(index_dir)
    doc_count = 0
    
    try:
        # Leggere il file JSON in modo incrementale con ijson
        with open(json_file, 'r', encoding='utf-8', errors='ignore') as f:
            parser = ijson.parse(f)
            current_doc = {}
            batch = []
            
            for prefix, event, value in parser:
                if prefix.endswith('.id'):
                    if current_doc:  # Se abbiamo un documento completo, aggiungiamolo al batch
                        batch.append(current_doc)
                        current_doc = {}
                    current_doc['id'] = str(value)
                elif prefix.endswith('.title'):
                    current_doc['title'] = value
                elif prefix.endswith('.abstract'):
                    current_doc['abstract'] = value
                elif prefix.endswith('.corpus'):
                    current_doc['corpus'] = value
                elif prefix.endswith('.keywords'):
                    current_doc['keywords'] = value
                elif prefix.endswith('.url'):
                    current_doc['url'] = value

                # Se il batch raggiunge la dimensione specificata, scrivilo nell'indice
                if len(batch) >= batch_size:
                    with ix.writer() as writer:
                        for doc in batch:
                            writer.add_document(
                                id=doc['id'],
                                title=doc.get('title', ''),
                                abstract=doc.get('abstract', ''),
                                corpus=doc.get('corpus', ''),
                                keywords=doc.get('keywords', ''),
                                url=doc.get('url', '')
                            )
                    doc_count += len(batch)
                    print(f"📦 Batch di {len(batch)} documenti indicizzati. Totale: {doc_count}")
                    batch = []

            # Gestisci l'ultimo documento e batch
            if current_doc:
                batch.append(current_doc)
            if batch:
                with ix.writer() as writer:
                    for doc in batch:
                        writer.add_document(
                            id=doc['id'],
                            title=doc.get('title', ''),
                            abstract=doc.get('abstract', ''),
                            corpus=doc.get('corpus', ''),
                            keywords=doc.get('keywords', ''),
                            url=doc.get('url', '')
                        )
                doc_count += len(batch)
                print(f"📦 Ultimo batch di {len(batch)} documenti indicizzati. Totale finale: {doc_count}")

    except Exception as e:
        print(f"❌ Errore durante l'indicizzazione: {e}")
        raise
    
    print(f"✅ Indicizzazione completata! Totale documenti indicizzati: {doc_count}")
    return ix

# Funzione per cercare nei documenti indicizzati
def search_documents(index_dir, query_string, title_true, abstract_true, corpus_true, ranking_type):
    """
    Search Engine Whoosh Function.

    This function takes the user's input string from the search bar, and 3 boolean values that represent
    the user's choice of where to search (title, abstract, corpus) and the index directory.

    Arguments:
        index_dir (str): Path to the index directory.
        query_string (str): The user's input.
        title_true (bool): First filter.
        abstract_true (bool): Second filter.
        corpus_true (bool): Third filter.
        ranking_type (str): The ranking algorithm to use ("TF_IDF" or "BM25F").
        

    Returns:
        list: list of the document matching the user query.
    """
    if not query_string.strip():
        return []
        
    if not any([title_true, abstract_true, corpus_true]):
        return []

    # Elabora la query in linguaggio naturale
    processed_query = process_natural_query(query_string)
    #processed_query = query_string  # Per ora non elaboriamo la query
    if ranking_type == "TF_IDF":
        rank = TF_IDF()
    else:
        rank = BM25F()
    
    ix = index.open_dir(index_dir)
    with ix.searcher(weighting=rank) as searcher:
        fields = []
        field_boosts = {} # Dizionario per i boost dei campi

        if title_true:
            fields.append("title")
            field_boosts["title"] = 2.0  # Assegna un boost di 2.0 al titolo
        if abstract_true:
            fields.append("abstract")
            field_boosts["abstract"] = 1.3 # Boost di default per l'abstract
        if corpus_true:
            fields.append("corpus")
            field_boosts["corpus"] = 1.0 # Boost di default per il corpus
            
        # Se nessun campo è selezionato, non ha senso continuare
        if not fields:
            return []

        parser = MultifieldParser(fields, ix.schema, group=OrGroup, fieldboosts=field_boosts)
        query = parser.parse(processed_query)
        
        results = searcher.search(query, limit=100)  # Nessun limite ai risultati
        return [(result['id'], 
                result['title'], 
                result['abstract'], 
                result['corpus'], 
                result['keywords'], 
                result.score,
                result['url']) 
                for result in results]

# Funzione per verificare se l'indice esiste e è valido
def index_exists_and_valid(index_dir):
    """
    Check if the index exists and is valid.

    This function takes the path of the index directory and checks if the index exists and contains documents.

    Arguments:
        index_dir (str): Path to the index directory.

    Returns:
        bool: True if the index exists and contains documents, False otherwise.
    """
    
    if not os.path.exists(index_dir):
        return False
    try:
        # Prova ad aprire l'indice
        ix = index.open_dir(index_dir)
        with ix.searcher() as searcher:
            # Verifica che ci siano documenti
            return searcher.doc_count() > 0
    except:
        return False

def create_or_get_index(index_dir, json_file, force_rebuild=False):
    """
    Create or get the index function only if it's necessary.

    Arguments:
        index_dir (str): Path to the index directory.
        json_file (str): Path to the JSON file containing the documents.
        force_rebuild (bool): Force the rebuild of the index.

    Returns:
        Index: Index object.
    """
    # Rimuovi eventuali lock residui
    lock_path = os.path.join(index_dir, 'LOCK')
    if os.path.exists(lock_path):
        os.remove(lock_path)

    try:
        if not force_rebuild and index_exists_and_valid(index_dir):
            print("🔄 Utilizzando l'indice esistente...")
            return index.open_dir(index_dir)
        
        print("🔨 Creazione nuovo indice...")
        if not os.path.exists(index_dir):
            os.makedirs(index_dir)
            
        # Indicizza i documenti e ritorna il nuovo indice
        return index_documents(index_dir, json_file)

    except Exception as e:
        print(f"❌ Errore durante la creazione/apertura dell'indice: {e}")
        raise

def get_wordnet_pos(tag):
    """
    Map POS tag to WordNet POS tag for lemmatization
    """
    tag_dict = {
        'N': wordnet.NOUN,
        'V': wordnet.VERB,
        'R': wordnet.ADV,
        'J': wordnet.ADJ
    }
    return tag_dict.get(tag[0], wordnet.NOUN)

def process_natural_query(query_string):
    """
    Process natural language query.
    
    This function takes a natural language query and processes it to create
    a more effective search query using NLP techniques.
    
    Arguments:
        query_string (str): The natural language query
        
    Returns:
        str: Processed query string
    """
    # Inizializza gli strumenti NLP
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))
    
    # 1. Keyword extraction with YAKE
    kw_extractor = yake.KeywordExtractor(
        lan="en",
        n=3, 
        dedupLim=0.9,
        dedupFunc='seqm',
        windowsSize=1,
        top=5, # Estrai le top 5 keyword
        features=None
    )
    keywords = [kw[0].lower() for kw in kw_extractor.extract_keywords(query_string)]

    # Tokenizzazione e rimozione stopwords dalla query originale
    tokens = word_tokenize(query_string.lower())
    
    # Combina keyword e token originali per la lemmatizzazione e l'espansione
    # per assicurarsi che le keyword multi-parola siano considerate
    base_terms_for_nlp = list(set(keywords + tokens))
    base_terms_for_nlp = [term for term in base_terms_for_nlp if term.isalnum() and term not in stop_words]

    pos_tags = pos_tag(base_terms_for_nlp)
    
    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) 
                  for word, tag in pos_tags]
    
    # Espansione con sinonimi, iperonimi e iponimi
    expanded_terms = set()
    # Aggiungi le keyword estratte da YAKE (già in minuscolo) e i termini lemmatizzati originali
    expanded_terms.update(keywords)
    expanded_terms.update(lemmatized)
        
    # Termini su cui basare l'espansione WordNet (keyword + lemmi originali)
    terms_for_wordnet_expansion = set(keywords + lemmatized)

    for term in terms_for_wordnet_expansion:
        # Aggiungi il termine stesso (già fatto, ma per sicurezza)
        expanded_terms.add(term)
        synsets = wordnet.synsets(term)
        for synset in synsets[:2]:  # Limita a 2 synset per termine per evitare rumore
            # Aggiungi sinonimi
            expanded_terms.update(lemma.name().lower().replace('_', ' ') for lemma in synset.lemmas() if lemma.name().lower() not in stop_words)
            
            # Aggiungi iperonimi
            for hypernym in synset.hypernyms():
                expanded_terms.update(lemma.name().lower().replace('_', ' ') for lemma in hypernym.lemmas() if lemma.name().lower() not in stop_words)

            # Aggiungi iponimi (termini più specifici)
            for hyponym in synset.hyponyms()[:1]: # Limita a 1 iponimo per synset per non allargare eccessivamente
                expanded_terms.update(lemma.name().lower().replace('_', ' ') for lemma in hyponym.lemmas() if lemma.name().lower() not in stop_words)
    
    # Pulisci i termini finali
    final_expanded_terms = {
        term.strip() for term in expanded_terms if term.strip() and term.strip() not in stop_words and len(term.strip()) > 1
    }
    
    if not final_expanded_terms: # Fallback se nessun termine è stato generato
        # Usa le keyword o i lemmi originali se disponibili
        fallback_terms = set(keywords + lemmatized)
        fallback_terms = {t for t in fallback_terms if t and t not in stop_words and len(t) > 1}
        if fallback_terms:
            processed_query = ' OR '.join(fallback_terms)
        else: # Fallback estremo: usa la query originale tokenizzata e senza stopwords
            original_tokens_no_stops = [token for token in word_tokenize(query_string.lower()) if token.isalnum() and token not in stop_words and len(token)>1]
            if original_tokens_no_stops:
                 processed_query = ' OR '.join(original_tokens_no_stops)
            else:
                 processed_query = query_string # Ultimo fallback, query originale
    else:
        processed_query = ' OR '.join(final_expanded_terms)
        
    print(f"Whoosh - Query elaborata: {processed_query}")
    return processed_query


#RIMUOVERE COMMENTO ED ESEGUIRE PER GENERARE INDICE
if __name__ == "__main__":
    setup_nltk()
    # Esempio di utilizzo
    # project_root = Path(__file__).parent.parent
    # index_dir = str(project_root / "WhooshIndex")  
    # json_file = str(project_root / "WebScraping/results/Docs_cleaned.json") 
    
    # print("🚀 Avvio indicizzazione...")
    # # Crea o ottieni l'indice
    # ix = create_or_get_index(index_dir, json_file, force_rebuild=True)
    
    # # Verifica il numero di documenti nell'indice
    # with ix.searcher() as searcher:
    #     doc_count = searcher.doc_count()
    #     print(f"📊 Numero totale di documenti nell'indice: {doc_count}")
