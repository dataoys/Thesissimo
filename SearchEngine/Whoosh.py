from whoosh.index import create_in
from whoosh.fields import Schema, TEXT, ID
from whoosh.scoring import TF_IDF, BM25F
from whoosh.qparser import OrGroup, MultifieldParser
from whoosh.analysis import StemmingAnalyzer
import os
import json
from whoosh import index
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk

# Assicurati di scaricare le risorse necessarie di NLTK
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/wordnet')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt')
    nltk.download('wordnet')
    nltk.download('stopwords') 

def create_schema():
    """
    Schema generation function.

    According to our document structure, this function generates a schema for the Whoosh index.

    Returns:
        scheme: Schema for the Whoosh index.
    """
    #Eseguiamo stemming di soli campi testuali significanti.
    return Schema(
        id=ID(stored=True),
        title=TEXT(stored=True, analyzer=StemmingAnalyzer()),
        abstract=TEXT(stored=True, analyzer=StemmingAnalyzer()),
        corpus=TEXT(stored=True, analyzer=StemmingAnalyzer()),
        keywords=TEXT(stored=True),
        url=TEXT(stored=True)
    )

#Creiamo l'indice
def create_index(index_dir):
    """
    Creating whoosh index function.

    This function takes the path of the index directory and creates an index with the schema generated by the create_schema function.

    Arguments:
        index_dir (str): Path to the index directory.

    Returns:
        Index: Index object.
    """
    if not os.path.exists(index_dir):
        os.mkdir(index_dir)
    schema = create_schema()
    return create_in(index_dir, schema)

# Indicizza i documenti
def index_documents(index_dir, json_file):
    """
    Indexing documents function.

    This function takes the path of the index directory and the path of the JSON file containing the documents to be indexed.
    It reads the JSON file and writes the documents to the index.

    Arguments:
        index_dir (str): Path to the index directory.
        json_file (str): Path to the JSON file containing the documents.
    """
    index = create_index(index_dir)
    
    # Leggiamo prima il file JSON
    with open(json_file, 'r', encoding='utf-8') as f:
        documents = json.load(f)
    
    # Poi scriviamo nell'indice
    with index.writer() as writer:
        for doc in documents:
            writer.add_document(
                id=str(doc['id']),
                title=doc['title'],
                abstract=doc['abstract'],
                corpus=doc['corpus'],
                keywords=doc.get('keywords', ''),
                url=doc['url']
            )

def expand_query(query_string):
    """
    Query expansion function.

    This function takes a query string and expands it with related terms using WordNet.

    Arguments:
        query_string (string): The user's query string.

    Returns:
        string: The expanded query string.
    """
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(query_string.lower())
    expanded_terms = set()

    # Aggiungi i termini originali
    expanded_terms.update(tokens)

    # Per ogni token, trova sinonimi e termini correlati
    for token in tokens:
        if token not in stop_words:
            # Trova i synset (gruppi di sinonimi)
            synsets = wordnet.synsets(token)
            for synset in synsets:
                # Aggiungi lemmi (sinonimi)
                expanded_terms.update(lemma.name() for lemma in synset.lemmas())
                
                # Aggiungi iperonimi (termini più generali)
                if synset.hypernyms():
                    expanded_terms.update(
                        lemma.name() 
                        for hypernym in synset.hypernyms() 
                        for lemma in hypernym.lemmas()
                    )

    # Rimuovi underscore e stop words dal risultato
    expanded_terms = {
        term.replace('_', ' ') 
        for term in expanded_terms 
        if term not in stop_words
    }
    
    return ' OR '.join(expanded_terms)

def search_documents(index_dir, query_string, title_true, abstract_true, corpus_true, ranking_type):
    """
    Search Engine Whoosh Function.

    This function takes the user's input string from the search bar, and 3 boolean values that represent
    the user's choice of where to search (title, abstract, corpus) and the index directory.

    Arguments:
        index_dir (str): Path to the index directory.
        query_string (str): The user's input.
        title_true (bool): First filter.
        abstract_true (bool): Second filter.
        corpus_true (bool): Third filter.
        

    Returns:
        list: list of the document matching the user query.
    """
    if not query_string.strip():
        return []
        
    if not any([title_true, abstract_true, corpus_true]):
        return []

    # Espandi la query con termini correlati
    expanded_query = expand_query(query_string)
    if ranking_type == "TF_IDF":
        rank = TF_IDF()
    else:
        # Non alteriamo i valori default della funzine (normalizzazione e sensibilità)
        rank = BM25F()
    
    ix = index.open_dir(index_dir)
    with ix.searcher(weighting=rank) as searcher:
        # Determina i campi da cercare
        fields = []
        if title_true:
            fields.append("title")
        if abstract_true:
            fields.append("abstract")
        if corpus_true:
            fields.append("corpus")
            
        # Usa MultifieldParser invece di QueryParser per cercare in più campi
        parser = MultifieldParser(fields, ix.schema, group=OrGroup)
        query = parser.parse(expanded_query)
        

        # Esegui la ricerca con il ranking selezionato

        results = searcher.search(query)
        return [(result['id'], 
                result['title'], 
                result['abstract'], 
                result['corpus'], 
                result['keywords'], 
                result.score,
                result['url']) 
                for result in results]

def index_exists_and_valid(index_dir):
    """
    Check if the index exists and is valid.

    This function takes the path of the index directory and checks if the index exists and contains documents.

    Arguments:
        index_dir (str): Path to the index directory.

    Returns:
        bool: True if the index exists and contains documents, False otherwise.
    """
    
    if not os.path.exists(index_dir):
        return False
    try:
        # Prova ad aprire l'indice
        ix = index.open_dir(index_dir)
        with ix.searcher() as searcher:
            # Verifica che ci siano documenti
            return searcher.doc_count() > 0
    except:
        return False

def create_or_get_index(index_dir, json_file, force_rebuild=False):
    """
    Create or get the index function only if it's necessary.

    This function takes the path of the index directory, the path of the JSON file containing the documents to be indexed,
    and a boolean value to force the rebuild of the index, false by default.

    Arguments:
        index_dir (str): Path to the index directory.
        json_file (str): Path to the JSON file containing the documents.
        force_rebuild (bool): Force the rebuild of the index.

    Returns:
        Index: Index object.
    """
    # Rimuovi eventuali lock residui
    lock_path = os.path.join(index_dir, 'LOCK')
    if os.path.exists(lock_path):
        os.remove(lock_path)

    if not force_rebuild and index_exists_and_valid(index_dir):
        return index.open_dir(index_dir)
        
    # Se l'indice non esiste o force_rebuild è True, crealo
    if not os.path.exists(index_dir):
        os.makedirs(index_dir)
            
    # Indicizza i documenti
    index_documents(index_dir, json_file)
    return index.open_dir(index_dir)
