[
    {
        "id": 1,
        "title": "Sector Rotation by Factor Model and Fundamental Analysis",
        "abstract": "",
        "corpus": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on. Authors: achieve the best HTML results from your LaTeX submissions by selecting from this list of supported packages. This study presents an analytical approach to sector rotation, leveraging both factor models and fundamental metrics. We initiate with a systematic classification of sectors, followed by an empirical investigation into their returns. Through factor analysis, the paper underscores the significance of momentum and short-term reversion in dictating sectoral shifts. A subsequent in-depth fundamental analysis evaluates metrics such as PE, PB, EV-to-EBITDA, Dividend Yield, among others. Our primary contribution lies in developing a predictive framework based on these fundamental indicators. The constructed models, post rigorous training, exhibit noteworthy predictive capabilities. The findings furnish a nuanced understanding of sector rotation strategies, with implications for asset management and portfolio construction in the financial domain. Keywords: US Industrial Sectors, Factor Analysis, Fundamental Analysis, Trading Strategy. Sector is composed by a basket of stocks that representing companies in certain business class, which has unique features according to the business. Under certain conditions, such as economic cycles, sectors may behave accordingly due to the different characteristics of businesses. In this report, we are exploring how to capture returns by finding the hidden features behind different sectors and determining the leading sectors in some particular market conditions or social environments. Generally, this report covers a brief exploration of market and fundamental factors, explaining the meaning of each factors and how they are related to some sectors.Then we applied a neural network model to do a classification and prediction using the fundamental factors as inputs. At the end of the report, we also covers how sectors behaved under global events. There are many different ways to / sectors. For the purpose of common acceptance and convenience for future data acquirement, we used the MSCI Global Industry Classification Standard, which includes 11 level one sectors, 24 level two industry groups, 69 level three industries, and 158 sub-industries. We use the 11 level one sectors as our main target. They are Energy, Materials, Industrials, Consumer Discretionary, Consumer Staples, Health Care, Financials, Information Technology, Communication Services, Utilities, and Real Estate. In order to track the performance of each sector, we use the S& GICS Indices which are constructed exactly as the MSCI classification.Before working on any strategies further, we need to determine if there are actually possible profits. In our case, we need to check how big the differences between sectors™ returns are. For each observation time period, Define: Return Difference =(top 3subscripttop 3 3}} _ top 3 Sector Return - bottom 3subscriptbottom 3 3}} _ bottom 3 Sector Return )/3 Based on a monthly frequency, we calculate the return difference and get the following plot. Also, by calculation, the quarterly return difference has a mean of 0.1306, median of 0.1185, standard deviation of 0.0523. We can reach to a result that half of the quarterly return difference is more than 11.85%percent It is easy to see that there does exist potential investment opportunity by capturing the return differences between sectors. Momentum premium was first recognized by UCLA scholars Narasimhan Jegadeesh and Sheridan Titman in 1993. The momentum premium is established on the observation that assets that have performed well in the past have the trend to persist good performance in the future. Though the momentum effect is considered to be a market anomaly, it has been recognized widely among many asset classes. We will explore the momentum effect based on the sector indices introduced above. First of all, we need to construct the momentum factor. Typically, the momentum factor is constructed by the past 6 or 12 months cumulative return and excludes the most recent month™s return, considering that there are also short-term reversion effects based on the mean-reversion effects. However, without a clear idea of how the sector indices carry the momentum effect, we need to explore through time intervals to find the best possible momentum factor. Then we constructed 12 different momentum factors using the past 1 to 12 month™s return and excluding the most recent 0.1 portion trading days of each time period to avoid short-term reversion. For each of the factors with the period of n months where Rdsubscriptð…ðR_{d}R _ d is the daily return. For the 12 factors we got, we normalized them cross sections. Then we rank the factor exposures for each sector and take long positions of sectors with the highest two factor exposures, take short position of sectors with the lowest two factor exposures. Then we trade our portfolio under a monthly frequency. Here are the results from 2002 to 2022 February: Since there are several market crashes where the momentum factor led to negative returns, we also take a look at the most recent five years from 2017 to 2022 February: From this table, we can tell that by using the MOM__ factor, we can reach a maximum annual return rate of 21.19%percent and a maximum Sharpe ratio of 0.62. It is also interesting that we find the momentum factor with a short time period, for example, MOM__ and MOM__ have a very small even negative return rate. However, it exactly conforms to the short term reversion effect that the typical momentum factor would exclude. Short term reversion factor follows the simple principle that asset™s price will have the trend to stay on an average level. Since we can see from the previous results of the momentum factor that there does exist short term reversion effect, we can try different reversion factors and find out what would be the best short term reversion observation period. Similarly, we can define several reversion factors with different time periods. And we take the negative number of the past n days cumulative return as the factor exposures. For the purpose of exploring the optimal time period, we take 5-day time interval and create 12 reversion factors from 5 trade days to 55 trade days. By using the same method, we compute the rank of each sector™s factor exposure, and long the top two sectors, short the last two sectors on a monthly observation frequency. Between 2002 and 2022 February, the results are: From this table, we can tell that for the time between 2002 to recent time, the short term reversion effect is optimal for taking the past 30 days cumulative return. It has an optimal annual return rate of 8.77%percent on average and leads to a sharp ratio of 0.8735. Fundamental Analysis are always a good aspect to look at for investing. We collected quarterly data for all 11 indices from Bloomberg, including their P/E ratio, EV/EBIT, Profit Margin, etc. Our fundamental analysis would start from discovering features for each of the fundamental ratio,then we are trying to predict the sector performance by constructing using some of the features we found. The P/E is one of the most widely used tools to determine a stock™s relative valuation. The purpose of analyzing the ratio is to show whether certain sector is worth to be invested because P/E ratio can reflect the investment risk in this sector. The figure below shows distribution of P/E ratios in different sectors. By comparing cross-sectional data, it is obvious that P/E of Real Estate Sector and Consumer Discretionary Sector are higher than others. The reason is that earning growth in the future is expected to grow fast in the two sectors or these sectors have some special advantages that guarantee long-term profitability with low risk. On the other hand, Financials Sector™s ratio is relatively low compared with other sectors, which may result from its high volatility so investors are reluctant to pay for it. We also notice that the ratio in Energy Sector surged in 2015, which is related to some changes in the sector. The end of the oil age and emergence of alternative energy have reduced the earnings of the original sector. As a result, its relative price becomes higher than before. The change in EV/EBIT and EV/EBITDA is also due to this reason. The P/B ratio provides a valuable reality check for investors who are seeking growth at a reasonable price. For those sectors with more assets, their book value and market value are close, so P/B ratio is more useful when we analyze Real Estate sector and Financials sector. The figure below shows distribution of P/B ratio in different sectors. As the picture shows, Consumer Discretionary sector and IT sector have higher P/B ratio while Financials sector and Energy sector have relatively low ratios. What™s more, Real Estate sector with high P/E ratio has relatively lower P/B ratio. EV/Sales can help investors better understand cost relative to unit sales and whether the company is overvalued or undervalued. If EV/Sales is relatively high, the company or sector is less attractive to investors. The figure below shows distribution of EV/Sales in different sectors. The result shows that Real Estate sector™s ratio is higher than others™, which means that index in this sector is overvalued. On the other hand, ratio in Energy sector is low, which can attract more investors. EV/EBIT and EV/EBITDA are independent of the capital structure of the company, whereas multiples like P/E ratio are impacted by financing decisions. Because of this reason, the two are the most commonly relied-upon multiples in relative valuation. However, one obvious distinction is that EV/EBIT considers depreciation and amortization. In some capital-intensive industries which have significant differences in D& EV/EBIT may make it a more accurate measure of value. But in our analysis, there is no such difference in the comparison of these two ratios under different sectors. The results can show that Real Estate sector has a higher ratio. The three ratio EV/Sales, EV/EBIT and EV/EBITDA can give a consensus conclusion that Real Estate sector is overvalued in the market. Dividend Yield is used to measure the amount of cash flow investors are getting back for each dollar. It is essentially the return on investment for a stock without any capital gains. The figure below shows distribution of Dividend yield in different sectors. The ratio in Communication Services sector is higher before 2018 while Energy sector™s ratio is higher after that time. This is because communication services sector took place a reorganization of S& index in 2018. It now includes at least eighteen companies from IT and Consumer Discretionary sectors. Due to this reshuffling, /nd yield of this sector is impacted. Gross margin equals net sales less the cost of goods sold (COGS). Net sales are equivalent to the total revenue from sales, and COGS is the direct cost associated with producing goods. By calculating gross margin, we could measure one company™s retain revenue after subtracting the production cost. The higher the gross margin, the more capital a company retains, which it can then use to pay other costs or satisfy debt obligations. Generally, companies with good gross margins would have a relatively sustainable competitive advantage. By analyzing gross margin data across sectors, we may observe some sectors that have more stable development in the long run. For our 11 sectors™ gross margin data, the line chart above shows significant differences between the sectors. Overall, each industry index is relatively flat on its own, and have gaps between each others. Utilities, Communication Services and Information Technology(IT) have been among the top spears for last 10 years, occupying the first, second and third positions respectively, all above 40%percent On the contrary, the energy sector has been an under-performer for the past decade, ranking at the bottom, with gross margins consistently below 20%percent Gross margins in the rest industries are concentrated in the 25%percent range and have not fluctuate much. At the same time, by observing the comparison of fluctuations between industries, it is not difficult to see that the gross margin fluctuations of the energy industry and the utilities industry maybe relatively high in the past decade, and their peaks correspond to each other. During 2016, the utilities industry grew significantly, while energy declined comparatively. The trend was even more pronounced in 2020, with utilities reaching its highest level and the energy industry fell to the bottom. Generally, the gross margin feature maybe a significant indicator for Utilities, Communication Services and IT sectors. And our conjecture about the correlation between utilities and energy sectors will need further observation and verification. Operating margin equals operating income /d by revenue, it is a profitability ratio measuring revenue after covering operating and non-operating expenses of a business. And profit margin measures the profit ratio after paying for variable costs of production. It is calculated by the formula: Both operating margin and profit margin are used to gauge the degree of the company™s activity makes money. Higher ratios are generally better, illustrating the company is efficient in its operations and is good at turning sales into profits. In our analysis, there is not a very big difference in the comparison of these two ratios under different sectors, which is determined by their definition. For these two ratios, Real Estate sector, IT sector and Financial sector have the top three high ratios.And Energy sector has the relatively lowest ratio. Also, both operating margin and profit margin for almost all sectors have similar trends in the last decade curves. This is attributed to the definition difference between the two features, and that™s why the operating margin was slightly higher than the profit margin. Another thing that is worth to mentioning is that for Energy Sector, not just operating margin and profit margin, but also the gross margin, it always has the relatively lowest ratios and similar curve fluctuation, with sharp declines in 2016 and 2020. The two time nodes may consistent with some big revolution in the energy industry, which we will analyze later. Return on equity (ROE) and return on assets (ROA) are two of the most important measures for evaluating how effectively a company™s management team is doing its job of managing the capital entrusted to it. ROE equals to generally net income /d by equity, while Return on Assets (ROA) is net income /d by average assets. So the primary differentiator between ROE and ROA is financial leverage or debt. ROE measures profitability and ROA is an efficiency measure of how well a company is using its assets. Investors may prefer to observe ROE, since equity represents the owner™s interest in the business. Compared to other sources of fund, equity capital tends to be the most expensive source of funding and carries the largest risk premium of all financing options. Therefore, in our analysis, ROE may be a better feature that it could reflect the trend of market investment. As shown in the picture, IT sector has the highest ROA, the Consumer Staples sector and Consumer Discretionary sector also have a relatively higher ratio. In contrast, Financial sector has a lower ROA. The past ten years, or even twenty years, has been an era of rapid development of information technology. And compared with traditional industry and commerce, information technology is more flexible in the time and form of investment assets, that™s the reason why IT will have the highest ratio. Also for the the Consumer Staples sector and Consumer Discretionary sector,they are all industries with fast innovation and short production cycle. Generally, these three will have constantly higher ratio for the long run. Therefore, for these three industries, if the ROA indicator fluctuates significantly, it may have an impact on the investment trend. For ROE ratio, similarly, IT, Consumer Staples stay high, and Consumer Discretionary sectors is also at a slightly higher level, except that the IT sector lost its prominence in ROA ratio. By comparing cross-sectional data, the Consumer Discretionary Sector and Industrials Sector have similar patterns in the last decade for both ROA and ROE ratios. They both have a low peak in 2020. It is conceivable that this is affected by the general environment of the epidemic. And as we mentioned before, the ROE and ROA curves of the energy sector still have a similar pattern, falling sharply in 2016 and 2020. In 2016, it was affected by changes in energy policy since 2015, reducing oil production while encouraging the development of clean and new energy. For 2020, we attribute this decline to the outbreak of the COVID-19 pandemic. Having these fundamental data, next step is to find out what quantitative relationships they have to futures sector returns. For fundamental factors, they are usually exposed in the company report with annual, semi-annual, or quarterly frequency. Our fundamental factors for each sector are reported quarterly, leading to a problem that the sample size for each individual sector is very small. To have a better performance of the prediction model, we need to combine all the sectors together and make a uniformed and comparable large sample. We neutralized each factor cross-sectional for the factor to have a mean of 0 and standard deviation of 1. If Xi,tsubscriptð‹ðð¡X_{i,t}X _ i , t denotes one specific factor exposure for iðii-th sector at time tð¡tt, in this case would be at tð¡tt-th quarter, then for each individual tð¡tt we have the neutralized exposure to be: Then we used the next quarter™s cross-sectional normalized return as the corresponding return. First, we want to have a general view of the relations. The scatter plots between neutralized factors and future returns are as following: From the scatter plots, the relations between all factors and their future returns cannot be well interpreted by simple linear models. However, it is very common in the financial field that the sample will have a very low signal-noise ratio. As we observed before, the relations between each factor and its future return cannot be interpreted very well by linear models. Also, we have no idea what model would exactly best fit the data. Therefore, converting prediction of future returns to a classification problem and fitting the training sample with a neural network model which has comparably good performance with non-linear relations would be a great start point. [height=10] bias=false, title=Input layer, text=x _ [count=5, bias=false, title=Hidden layer 1, text=h ^ ( 1 ) _ bias=false, title=Hidden layer 2, text=h ^ ( 2 ) _ title=Output layer, text=y^ y _ Neural network takes a vector as the input, and goes to each of the neuron in the first hidden layer and gains new activation vectors which act as the input for next hidden layer. After the last hidden layer, neural network model would pass out the probability for each of the prediction class and we choose the one with the highest probability as the prediction. This process is called front propagation. After comparing the prediction to the actual results, we adjust the weights of the nodes by using back propagation for each training pair in the training samples. Also, we use the rectified linear unit function as the activation function for hidden layers and sigmoid function as the activation function for final output. Since we only have a sample of size 200, choosing quasi-Newton methods as the solver has better performance for small sample training. Then we need to construct the training, validation, and test sets. Since the fundamental factors are already neutralized (normalized) within each sector, we / the sample data to 60%percent 20%percent 20%percent by convention. Without shuffling, we will have the historical data /d where test set contains the most recent data. For the corresponding output value, we assign 1 to samples with positive future return and 0 with negative returns. The complexity of neural network directly related to the number and sizes of hidden layers. For the purpose of avoiding overfitting or under-fitting, we need to find proper hyper parameters for neural network model. We start from a simple model with two layers. Let NðNN denote the number of nodes in each hidden layer, alpha is the hyper parameter for L2 regularization penalty function. With larger NðNN, the model is more complex. If alpha increases, the penalty for large weights increases, which makes the model tend to be more simple. Considering our sample size is small, intuitively we need to focus more on the overfitting problem. For a range of alpha and NðNN, we train the model using the training set data, and get the score for prediction on validation set. The score represents the probability of making a right prediction. Here are the results: To better understand how the hyperparameters influence model performance, we visualize the data by using NðNN and alpha as the bottom coordinates, and use the corresponding probability as the height. From the figure, we can tell that the model have several local optimal pairs. And the scores at the optimal points with relative large N values are also combined with small alpha values. For example, the combination of 14 nodes and alpha equals 0.01 has a local optimal score of 0.6. Since we are training with a small sample, using such a complex model with a high score is highly likely overfitting. Therefore, we start from the simple model by looking at models with 5 nodes model and check how the score varies with alpha. For model with 5 nodes, we see there are two local peaks with alpha equal to 1 and 0.25, then we pick the middle value 0.5 as the value of alpha considering the trade-off between variance and bias. Constructed and trained the model, next we would test the model by feeding a new data set to the model. On the test set, the score of the model is 0.64, which means the model predicts 64%percent of the results correctly. More detailed results are showed in the following table: On the test set, we have a 0.59 winning rates on the positive predictions and 0.72 on the negative predictions, which gives an overall winning rate of 0.64. By using the predictions from the trained model, we used the data from validation set to get trade signals. Instead of having signals of 1 or 0 as the model™s output, we choose the probability of the prediction output being 1, which is given by the activation sigmoid function. Then we will have a time series of the probability for each sector, and rank the probability from highest to lowest where the highest probability will have a rank 1. For each cross-sectional ranking, we equally-weighted long sectors with rank 1 to 3 and short sectors with rank 9 to 11 to construct a dollar-neutral portfolio. On the test set, which is from September in 2020 to September in 2021, we have a Sharpe ratio of 2.21. The cumulative return plot is following: There are still issues that need to be considered carefully in the future. First is the factor neutralization. In previous model, we neutralized the factor exposure cross-sectionally, where the exposures reflect the relative level of factor exposure for one sector compared to other sectors at a given time. However, different sectors may have inner trends of higher exposures than others for some factors, especially for fundamental factor. What™s more, we only have quarterly fundamental data available from 2017 and it is hard to implement time series normalization for each sector. Therefore, how to modify the factor exposures to make them comparable is a difficult problem. Secondly, as the sample size is small, the model might not be applicable on a wider range of time since we only trained and tested on the most recent five years. One possible way to improve this model is to use daily factors such as volume, close price as input, and convert fundamental factors to daily frequency by the corresponding quarter. Then we would have a sample size of approximately 1250 for each sector and over 13000 samples for training. However, the model might depends more on the daily factors rather than fundamental factors since their exposures would be the same value for each quarter. 1. Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency Narasimhan Jegadeesh; Sheridan Titman The Journal of Finance, Vol. 48, No. 1. (Mar., 1993), pp. 65-91. 2.The Global Industry Classification Standard, MSCI (1999)",
        "keywords": ""
    },
    {
        "id": 2,
        "title": "Prompt emission of relativistic protons up to GeV energies from M6.4-class solar flare on July 17, 2023",
        "abstract": "AbstractWe show evidence of particle acceleration at GEV energies associated directly with protons from the prompt emission of a long-duration M6-class solar flare on July 17, 2023, rather than from protons acceleration by shocks from its associated Coronal Mass Ejection (CME), which erupted with a speed of 1342 km/s. Solar Energetic Particles (SEP) accelerated by the blast have reached Earth, up to an almost S3 (strong) category of a radiation storm on the NOAA scale. Also, we show a temporal correlation between the fast rising of GOES-16 proton and muon excess at ground level in the count rate of the New-Tupi muon detector at the central SAA region. A Monte Carlo spectral analysis based on muon excess at New-Tupi is consistent with the acceleration of electrons and protons (ions) up to relativistic energies (GeV energy range) in the impulsive phase of the flare. In addition, we present another two marginal particle excesses (with low confidence) at ground-level detectors in correlation with the solar flare prompt emission.",
        "corpus": "We show evidence of particle acceleration at GEV energies associated directly with protons from the prompt emission of a long-duration M6-class solar flare on July 17, 2023, rather than from protons acceleration by shocks from its associated Coronal Mass Ejection (CME), which erupted with a speed of 1342 km/s. Solar Energetic Particles (SEP) accelerated by the blast have reached Earth, up to an almost S3 (strong) category of a radiation storm on the NOAA scale. Also, we show a temporal correlation between the fast rising of GOES-16 proton and muon excess at ground level in the count rate of the New-Tupi muon detector at the central SAA region. A Monte Carlo spectral analysis based on muon excess at New-Tupi is consistent with the acceleration of electrons and protons (ions) up to relativistic energies (GeV energy range) in the impulsive phase of the flare. In addition, we present another two marginal particle excesses (with low confidence) at ground-level detectors in correlation with the solar flare prompt emission. Since 1950 the observation of solar energetic particles from the solar flares and coronal mass ejections (CMEs) have been done with ground-level experiments, such as the neutron monitors (NMs) (Meyer et al., 1956; Simpson, 2000; Moraal et al., 2000) as well as the solar neutron telescope network (Hu & Semones, 2022; ValdÃ©s-Galicia et al., 2009), all around the world. These observations have yielded a lot of new information. For instance, the existence of a prompt and gradual emission of solar energetic particles (SEP) in flares and CMEs, respectively, the correlations of the cosmic ray intensity with CMEs and other solar disturbances crossing the Earth, etc. (Chupp et al., 1987; Moraal et al., 2000) Also, the solar modulation of galactic cosmic rays is inversely correlated with solar activity, inferred through the number of sunspots, which can be the key to understanding more about space weather (Cade III & Chan-Park, 2015). Nowadays, particles accelerated to near the Sun can be detected by space-borne instruments such as the High-Energy Proton and Alpha Detector (HEPAD) on the Geostationary Operations Environmental Satellite (GOES) and the Advanced Composition Explorer (ACE) spacecraft at Lagrange L1 point, through the Electron Proton Alpha Monitor (EPAM) and the Solar Isotope Spectrometer (SIS), among others. Not all of the solar energetic particles can be measured at ground level. Even those SEPs from solar events with a good geoeffectiveness can be dissipated by the IMF, or deflected or captured by the Earth™s magnetic field or until absorbed by atmosphere. On the other hand, ground-level enhancements (GLEs), typically in the MeV-GeV energy range, are sudden increases in cosmic ray intensities registered in most cases by NMs. GLEs are quite rare events, and fewer than 100 GLEs have been observed by NMs in the last 70 years. In most cases, the NMs that observed GLEs are located at regions with small geomagnetic rigidity cutoff, that is, at high latitudes (Shea & Smart, 2012). The GLEs follow the solar radiation storms, solar energetic particles (mostly protons) observed by GOES. They occur when a large-scale magnetic eruption, a coronal mass ejection and associated solar flare, accelerates charged particles in the solar atmosphere to high energies. However, in the present case, despite a radiation storm reaching above the S2-class on the NOAA scale on July 18, 2023, it did not generate a GLE, only a prompt emission of relativistic protons (ions) above GeV energies, during the phase eruptive, and observed by ground-level detectors strategically located, within the SAA central region (New-Tupi muon detector) and by the (Yangbagin muon telescope) at the Yangbajing Cosmic Ray Observatory (Tibet 4440 m a.s.l) (Zhang et al., 2010). Also, we looked for any signal in the counting rate at the Neutron Monitor™s (NM) network around the world from Neutron Monitor Data Base (NMDB) https://www.nmdb.eu/nest/, with negative results. However, we found a low confidence signal only at Kerguelen NM, at geographical coordinates (49.3S, 70.3E), altitude of 33 m a.s.l, and an effective vertical cutoff rigidity of 1.14 GV. We present details of these observations. The New-Tupi muon detector is completely unmoderated (without no surrounding lead or other material). The muon detection energy threshold is about 200 MeV (see Appendix A). That contrasts with other muon detectors that have, in most cases, a surrounding lead material with a thickness of up to 5 cm. The shielding effect of the Earth™s magnetic field on cosmic ray particles is quantified by the magnetic rigidity cutoff from a specific location (Smart & Shea, 2009). The smaller the rigidity cutoff, the lower the energy cosmic ray particles penetrate the magnetosphere. On the other hand, a restricted area between latitudes 20 and 40 of the southern hemisphere, over South America and the Atlantic Ocean poses a geomagnetic field with an anomalously lower intensity (around 22,000 nT). The region is known as the South Atlantic Anomaly (SAA) (PavÃ³n-Carrasco & De Santis, 2016). According to Swarm™s satellite observations (Finlay et al., 2020), the SAA appears splitting into two, a smaller area over the Atlantic Ocean in southwest Africa and a larger area over eastern South America. Fig. 1 (top panel) summarizes the situation. We would like to point out that the location of the New-Tupi telescope coincides with the central part of the SAA indicated by the arrow on the left of Fig. 1 (top panel). The main effect of the SAA is on the satellites since the ™70s. We know the frequent failures when they pass through the SAA region. A large amount of charged particles precipitation in this region damages and perturbates the satellites™ electronics. Also, according to the results from the PAMELA detector at satellite™s altitudes (Casolino et al., 2009), the effect of geomagnetic cutoff on low-energy particles is present in high latitudes close to the poles and also in the SAA region, composed mostly of low energy cosmic protons (E <<< 200 MeV ). In other words, the Pamella satellite has shown that the SAA introduces a sub-cutoff in the magnetic rigidity, below the Stormer™s magnetic rigidity cutoff. We show that the SAA also affects secondary cosmic rays detected at ground level. As the horizontal magnetic component on Earth™s surface is smaller on the SAA, the magnetic lateral dispersion of the secondary particles forming an air shower is smaller too. The effect increases the number of particles reaching a detector. In other words, this behavior mimics a magnetic rigidity sub-cutoff below the Stormer™s rigidity cutoff. We show that effect through a Monte Carlo simulation based on CORSIKA-Fluka code (Heck et al., 2012; Battistoni et al., 2008), where 1.0Ã1061.0superscript1061.0 10^{6}1.0 Ã 10 ^ 6 proton air-showers are simulated, taking into account the magnetic coordinates (latitude, longitude) and height of several places where detectors are installed (mostly neutron monitors). Fig. 1 bottom left panel shows the lateral particle distribution in air-showers of cosmic rays, as detected from several ground-level detectors. In all cases, there is a fast rise of particles with the shower lateral development until reach um maximum value that happens for different values of R, called hereafter as Rmaxsubscript…¥R_{max}R _ m a x We can see that the number of shower particles at Rmaxsubscript…¥R_{max}R _ m a x in the SAA central region (SAA-CR) rigidity 9.6 GV is higher than at Rome and Athens, both with the rigidity of 6.3 GV, and 8.5 GV, respectively, i.e., minors than the SAA-CR. Already Fig. 1 bottom right panel, shows a correlation between Rmaxsubscript…¥R_{max}R _ m a x versus the geomagnetic Stormer rigidity cutoff of six different places (black circles), including the SAA-CR (blue square). The solid red line is a linear fit, and the two dotted red lines delimit the region with significance of ±1ƒplus-or-minus1 1 1 ƒ. Only two places are out from the ±1.0ƒplus-or-minus1.0 1.0 1.0 ƒ significance region, the Thule (Groenlandia) in the lowest rigidity region and SAA-CR in the highest rigidity region. The high Stormer™s rigidity of SAA-CR does not correspond to the high value of Rmaxsubscript…¥R_{max}R _ m a x expected by the correlation. From an interpolation, it is possible to see that the small value of Rmaxsubscript…¥R_{max}R _ m a x at SAA-CR correspond to the rigidity of only 3.11.7+3.0subscriptsuperscript3.13.01.73.1^{+3.0}_{-1.7}3.1 ^ + 3.0 _ - 1.7 GV, within a confidence of ±1.0ƒplus-or-minus1.0 1.0 1.0 ƒ. This behavior of having a location close to the Equator, with a nominal lower magnetic rigidity cutoff, favors the observation of phenomena such as the SEPs. On July 17, 2023, at ¼similar-to UT, the active region AR 13363 had an explosion, reaching an M6-class solar flare followed by a resplendent coronal mass ejection. Fig. 2 left panel shows the image from the Solar Dynamo observatory of the blaze of fire responsible for the X-ray flux reaching M6-class flare. Already the right panel shows the LASCO-C2 coronograph image of its associated CME on July 18, 2023, at 00:42 UT. NOAA prediction models confirmed that a CME originated in the powerful M6-class flare from sunspot AR3363 would pass through the magnetosphere on July 20, triggering at least a G1-class (minor) geomagnetic storm. However, no magnetic storms were observed. Fig. 3 shows the GOES-18 X-ray flux (upper panel) and the GOES-16 proton flux (bottom panel). The X-ray flux peaks at 18:00 UT, while the proton flux has two peaks. The first (in orange) is due to the acceleration of protons during the impulsive fast-rising phase of the flare peaking at 18:09 UT. The delay between the X-ray and proton flux peaks is because the proton velocity is slightly less than c, and the proton path is longer. The second peak are the protons accelerated by CME shocks, peaking at 18:14 UT, and it™s the so-called gradual phase and is characterized by its long duration, up to several days. Fig. 4 shows the temporal coincidence between the GOES proton flux in the impulsive phase and New-Tupi muon excess. Particles (mostly proton) are accelerated in this phase exclusively by the flare, during the fast-rising until to reach the first peak (orange sector) in Fig. 4. However, in the so-called gradual phase, protons accelerated by CME™s shocks, the proton flux does not reach the GeV energy range because there are no excess muons at ground level. We perform a Monte Carlo simulation of air showers initiated by SEP (protons) using the CORSIKA code (Heck et al., 2012), together with the FLUKA interaction model (Battistoni et al., 2008), that works well at GeV and sub-GeV energies, including secondary particle decay. The surviving particles are tracked through the atmosphere until they reach ground level (sea level). Most particles are muons with a small contribution of electrons and nucleons. The aim is to obtain the yield function, SÎ¼(EP)subscript†subscriptƒS_{ _ Î¼ ( E _ P ), that is, the number of muons at sea level per primary proton, for an estimate of the upper limit of the integral proton flux in the GeV energy range, associated with the impulsive phase of M-6-class flare with onset on July 17, 2023, at ¼similar-to 18 UT. Fig. 6 (black squares) shows the Monte Carlo output under the New-Tupi geomagnetic conditions and vertical proton incidence, and fitting as where AÎ¼=(6.8±1.4)Ã103subscriptplus-or-minus6.81.4superscript103A_{ 1.4) 10^{-3}A _ Î¼ = ( 6.8 ± 1.4 ) Ã 10 ^ - 3 , Î½=1.18±0.24plus-or-minus1.180.24 0.24Î½ = 1.18 ± 0.24, and E0=10.2±2.1subscript0plus-or-minus10.22.1E_{0}=10.2 2.1E _ 0 = 10.2 ± 2.1 GeV. Fig. 6 shows the fits (red dot line). In addition, we assume here that the energy spectrum of solar protons in the GeV energy range, which is in the high-energy tail of the SEP spectrum, can be fitted by a single power-law function. There are two unknown quantities in the above power-law function: the coefficient APsubscriptƒA_{P}A _ P and the spectral index Î²½ A convolution between the yield function SÎ¼(EP)subscript†subscriptƒS_{ _ Î¼ ( E _ P ) and the proton spectrum JP(EP)subscript½ƒsubscriptƒJ_{P}(E_{P})J _ P ( E _ P ) gives the response function (Augusto et al., 2016b), which is the number of muons in the excess signal at New-Tupi detector generated by the SEP during the period T. We express this convolution as where F(Î)¼exp¡(Î/C)similar-to¹ƒƒ¶F( ( Î ) ¼ roman_exp ( Î / C ) is the pitch angle distribution (Shea & Smart, 1982; Miroshnichenko et al., 2005). In the central region of SAA (New-Tupi), the transverse geomagnetic component is only B‚=18.3subscriptµperpendicular-to18.3B_{ _ ‚ = 18.3 mT, that is, almost 80% smaller than the transverse component, at the same latitude, but outside from the SAA region, favoring the focusing factor of the geomagnetic parallel geomagnetic on the incident solar protons (small pitch angles). For the present event, we found exp¡(Î/C)¼1similar-toƒ¶1 1roman_exp ( Î / C ) ¼ 1. The muon excess associated with protons emitted during the impulsive phase (see Fig. 4), and considering an effective angular aperture of 60 degrees around the zenith of the New-Tupi detector, the counting rate excess is Furthermore, we also obtain the integrated time primary fluence as For the present case, the GOES-proton fluence in the high-energy region (Ep¥ MeV is The terms on the left side of Eq. 1 and Eq. 5 are known. Thus, we can consider all possible values of Î²½ and APsubscriptƒA_{P}A _ P compatible with the observed muon counting rate excess, JÎ¼subscript½J_{ _ Î¼ , and the integrated GOES-proton fluence F¹FF. Fig. 7 summarizes the situation. Giving: Ap=(1.20±0.96)Ã103/(cm2ssrGeV)plus-or-minus1.200.96superscript103superscript2 ºAp=(1.20 0.96) 10^{-3}/(cm^{2}s p = ( 1.20 ± 0.96 ) Ã 10 ^ - 3 / ( c m ^ 2 s s r G e V ) and Î²=1.89±1.10½plus-or-minus1.891.10 1.10Î² = 1.89 ± 1.10. To make a comparison with satellite GOES data, we obtain the integral proton flux in the GeV energy range as Fig. 8 shows the results of the integral proton flux obtained from the New-Tupi muon excess observed in coincidence with the radiation. The red circles represent the GOES-16 data, and the black squares represent the expected proton flux in the GeV energy range obtained from Monte Carlo, muon excess at the New-Tupi detector, and the GOES-proton fluence. The origin of this transient event was the solar eruption, an M6-class flare (prompt emission), accelerating protons (ions) up to relativistic energies, GeV energy range. We have reported evidence of SEPs accelerated up to GeV energies during the eruptive phase of the M6-class solar flare on July 17, 2023. The result comes from a timing analysis of a muon excess from the New-Tupi detector at the SAA central region. Muons at New-Tupi are produced by protons (ions) interaction in the upper atmosphere reaching the Earth with a magnetic rigidity above 3 GV (¼similar-to 3 GeV for protons). In most cases, SEP (mostly protons) detected by the GOES-16 spacecraft shows two steps. An impulsive phase, where the acceleration of protons (ions) is by the prompt emission of flare, followed by a gradual phase of long duration, where the acceleration of protons (ions) is by the associated CME shock waves. We want to point out that the muon excess produced by SEPs with an effective rigidity above the cutoff (¼similar-to 3 GV) at the New-Tupi muon detector is in temporal coincidence only with the GOES proton flux from the impulsive phase (see Fig. 4) . Consequently, in the gradual phase, the protons accelerated by CME™s shocks do not reach the GeV energy range because a muon excess is absent at ground level. A marginal muon excess also appears on the Yan ba Jing S-21(pointing 21 degrees south) muon telescope (in Tibet). Also, a marginal particle excess is seen only in the French Kerguel NM (close to the South polar region). In both cases, the excesses are in (temporal) coincidence with the GOES proton flux (impulsive phase). However, it is hard to verify whether these excesses are genuine due to low confidence or simply fluctuations in the detectors™ count rate. From a Monte Carlo analysis, we show that the SAA central region is favourable to the observation of transient solar events, especially SEP, because the magnetosphere has a dip in this region, weakening the geomagnetic field strength and allowing the entrance of charged particles at large deeps in a region not far from the geographic Equator, giving a rigidity sub-cutoff around 3.1 GV in a place where the conventional Stormer geomagnetic rigidity cutoff is around 10 GV. This work is supported by the Rio de Janeiro Research Foundation (FAPERJ) under Grant E-26/010.101128/2018. We thank to NMDB Database (www.nmdb.eu), founded under the European Union FP7 Program (Contract No. 213007) by provide NMs data and the Space Weather Prediction Center from NOAA for its open data policy. The New-Tupi telescope is built with four identical particle detectors, forming two telescopes, as shown in Fig. 8 from Augusto et al. (2016a). Each detector consisting of an Eljen EJ-208 plastic scintillator slab of 150 cm x 75 cm x 5 cm and a Hamamatsu R877 photomultiplier of 127 millimeters in diameter, packaged in a pyramidal box. The PMT high voltage /r, amplifier, and high voltage power supplier are in the ORTEC ScintiPackTM Photomultiplier Base 296. From February 6, 2023, we have implemented a data acquisition system using a VERTILON high-speed pulse counting system (MCPC618-8 Channel). allowing for direct connection with the PMTs without the need for external preamplifiers, with a 250 MHz count rate per channel. Now the detector is working only in scaler mode or single particle technique (Aglietta et al., 1996), where the single hit rates of all four PMTs, are recorded once a second. However, so far, only two detectors are working. The coincidences among these detectors of each telescope will be implanted. Also, the barometric coefficients for cosmic muon fluxes at the Earth™s surface can be obtained using the CORSICA code in Kovylyaeva et al. (2013). For New-Tupi detector conditions and at sea level, the barometric coefficient is about -0.14% per mb, about eight to nine times less than the typical barometric coefficient in NMs.",
        "keywords": "sun:activity, high-speed stream, cosmic rays modulation"
    },
    {
        "id": 3,
        "title": "Generative Inverse Design of Metamaterials with Functional Responses by Interpretable Learning",
        "abstract": "AbstractMetamaterials with functional responses, such as wave-based responses or deformation-induced property variation under external stimuli, can exhibit varying properties or functionalities under different conditions. Herein, we aim at rapid inverse design of these metamaterials to meet target qualitative functional behaviors. This inverse problem is challenging due to its intractability and the existence of non-unique solutions. Past works mainly focus on deep-learning-based methods that are data-demanding, require time-consuming training and hyperparameter tuning, and are non-interpretable. To overcome these limitations, we propose the Random-forest-based Interpretable Generative Inverse Design (RIGID), a single-shot inverse design method to achieve the fast generation of metamaterial designs with on-demand functional behaviors. Unlike most existing methods, by exploiting the interpretability of the random forest, we eliminate the need to train an inverse model mapping responses to designs. Based on the likelihood of target satisfaction derived from the trained forward model, one can sample design solutions using Markov chain Monte Carlo methods. The RIGID method therefore functions as a generative model that captures the conditional distribution of satisfying solutions given a design target. We demonstrate the effectiveness and efficiency of RIGID on both acoustic and optical metamaterial design problems where only small datasets (less than 250 training samples) are available. Synthetic design problems are created to further illustrate and validate the mechanism of likelihood estimation in RIGID. This work offers a new perspective on solving on-demand inverse design problems, showcasing the potential for incorporating interpretable machine learning into generative design and eliminating its large data requirement.",
        "corpus": "Metamaterials with functional responses, such as wave-based responses or deformation-induced property variation under external stimuli, can exhibit varying properties or functionalities under different conditions. Herein, we aim at rapid inverse design of these metamaterials to meet target qualitative functional behaviors. This inverse problem is challenging due to its intractability and the existence of non-unique solutions. Past works mainly focus on deep-learning-based methods that are data-demanding, require time-consuming training and hyperparameter tuning, and are non-interpretable. To overcome these limitations, we propose the Random-forest-based Interpretable Generative Inverse Design (RIGID), a single-shot inverse design method to achieve the fast generation of metamaterial designs with on-demand functional behaviors. Unlike most existing methods, by exploiting the interpretability of the random forest, we eliminate the need to train an inverse model mapping responses to designs. Based on the likelihood of target satisfaction derived from the trained forward model, one can sample design solutions using Markov chain Monte Carlo methods. The RIGID method therefore functions as a generative model that captures the conditional distribution of satisfying solutions given a design target. We demonstrate the effectiveness and efficiency of RIGID on both acoustic and optical metamaterial design problems where only small datasets (less than 250 training samples) are available. Synthetic design problems are created to further illustrate and validate the mechanism of likelihood estimation in RIGID. This work offers a new perspective on solving on-demand inverse design problems, showcasing the potential for incorporating interpretable machine learning into generative design and eliminating its large data requirement. Metamaterials with functional responses are engineered materials that exhibit varying properties or behaviors under different conditions. One example is metamaterials whose electromagnetic, acoustic, or elastic wave propagation behaviors change with wavelengths or frequencies [1]. Another example is metamaterials that exhibit changing properties or functionalities due to deformation in response to external stimuli like temperature [2] and magnetic fields [3]. Tailoring functional responses of these metamaterials is of interest to applications such as sound and vibration control, analog computing, medical imaging, sensing, communication, and soft robotics. In many use cases, rather than precisely controlling the complete functional responses, we only care about qualitative behaviors under certain conditions. For example, acoustic metamaterials were usually designed to have bandgaps at specified frequencies to achieve functionalities like wave-guiding [4, 5], focusing [6, 7], and vibration mitigation [8, 9, 10]. However, it is unnecessary and computationally expensive to design for the whole dispersion relation [11, 12, 13, 14, 15]. Similarly, we may design optical metamaterials to qualitatively manipulate optical properties (e.g., high or low absorption/reflection/transmission) under certain wavelengths, without requiring the entire spectral response to match an exact target [16, 17]. Identifying metamaterial designs from a given target forms an inverse design problem. Unlike many forward problems where one can obtain solutions (e.g., spectral responses or material properties under external stimuli) by modeling the physics or conducting experiments, inverse design problems are usually intractable. Traditionally, these problems are solved by iterative optimization (i.e., minimizing the difference between the actual quantity of interest and the target) [11, 12, 14]. This, however, requires repeatedly updating the design solution and solving forward problems. When the design target changes, one needs to rerun the entire optimization process. Thus, inverse design by iterative optimization becomes impractical if solving the forward problem (by simulations or experiments) is time-consuming or if the design target needs to change frequently. To accelerate the optimization approach, prior works replaced simulations or experiments with machine learning models [18, 19]. However, the efficiency and quality of final solutions are highly dependent on both the machine learning model and the optimization algorithm. On the other hand, a single run of optimization usually only returns one final solution, although multiple designs might satisfy a given target (i.e., the non-uniqueness of solutions). For example, multiple acoustic metamaterial designs may have bandgaps within the same target frequency range. This non-uniqueness nature of inverse design problems was also shown for optical metasurfaces [20, 21, 22]. The optimization approach eliminates the opportunity to explore diverse alternative solutions. To avoid iterative optimization and enable fast on-demand inverse design, prior research attempted to realize single-shot (iteration-free) inverse design using machine learning. There are three mainstream models (their schematic diagrams are shown in Appendix, Fig. 7). The most straightforward approach is to learn a direct inverse mapping from the response to design variables. Neural networks are the most commonly used machine learning model for this purpose, due to their high flexibility in approximating arbitrary nonlinear input-output relationships [23, 13]. Despite the simplicity of the direct inverse mapping, its underlying assumption of the response-design mapping being one-to-one does not hold in many cases due to the non-uniqueness of solutions, as mentioned earlier. Such non-uniqueness will cause conflicting training instances where the same input (response) is associated with distinct outputs (designs), which will destabilize the convergence during neural network training [20, 24]. To avoid this issue, past work proposed the Tandem Neural Network (T-NN) that cascades an inverse-design network with a forward-modeling network [20, 25, 26, 27, 28]. Its training is split into two steps: (1) pretraining the forward-modeling network to approximate the design-response mapping and (2) training the cascaded network by freezing the weights of the pretrained forward-modeling network. There is no loss function that forces designs at the intermediate layer to match data (which contains conflicting instances), hence the training convergence issue is avoided. Nonetheless, the original T-NNs still learn a one-to-one response-design mapping and cannot account for the non-uniqueness of design solutions. To fundamentally solve this problem, one needs to learn a one-to-many mapping. Bastek et al. [28] integrated stochastic sampling into the inverse-design network to allow the generation of multiple feasible solutions. A large body of recent works achieved the goal of learning one-to-many mapping by using conditional generative models, typically conditional generative adversarial networks (cGANs) [29, 30, 31, 32, 22], conditional variational autoencoders (cVAEs) [21], and conditional diffusion models [33]. These models can generate multiple designs given a target response by learning the distribution of designs conditioned on the response. Different generative models have distinct ways of learning conditional distributions. In general, this is realized by training neural networks to transform responses and random noise (or latent variables) into designs, so that the trained network can generate a non-deterministic design solution from a given target response and randomly sampled noise, which is equivalent to sampling from a conditional distribution. Although conditional generative models have demonstrated success in solving inverse design problems, they still have issues, such as high data demand, exhaustive hyperparameter tuning, slow training, and low interpretability, especially compared to traditional machine learning models like decision trees and random forests. On the other hand, Elzouka et al. [34] proposed to use the decision tree as a more interpretable model to solve both the forward prediction and inverse design problem. After training a decision tree for forward prediction, one can identify explicit design rules (i.e., feasible regions in the design space) by tracing from target leaf nodes to the root node. This approach also captures the one-to-many mapping nature of inverse design problems since it gives feasible design variable ranges rather than a single solution. However, there remain some limitations. Firstly, for solutions identified by the design rules, the method does not differentiate their likelihood of target satisfaction. Yet in reality, solutions always have different likelihoods due to the uncertainty of model estimation. Secondly, the method has to train two models: a random forest was trained first to ensure model accuracy and robustness, and then a large decision tree was trained to emulate the performance of the random forest and provide design rules. This is due to the challenge of deriving explicit design rules from an ensemble model like the random forest. Finally, the method was demonstrated on a problem with more than 104superscript10410^{4}10 ^ 4 training data, while the effectiveness on smaller datasets (i.e., data with orders of magnitude smaller sample sizes) was not studied. This work aims to address the aforementioned problems by proposing a method called Random-forest-based Interpretable Generative Inverse Design (RIGID). Figure 1 shows an overview of this method. Specifically, we first train a forward prediction random forest. Then given a design target, we can probe the trained random forest to infer the likelihood of any design satisfying the target. To generate new designs tailored to the target, we can sample from the design space according to the likelihood. Compared to the most widely studied neural-network-based methods, RIGID has a much lower cost in training and hyperparameter tuning, and works more robustly on small-size datasets (as random forests are less prone to overfitting). Similar to deep generative models, it can generate a desired number of solutions, allowing one to explore alternative solutions that might have desired properties or functionalities beyond the ones considered as the target. The explicit likelihood estimation also offers an interpretable characterization of a design™s target satisfaction probability and allows an exploitation-exploration trade-off when selecting generated designs. We validate the RIGID method on two metamaterial design examples ” an acoustic metamaterial design example, where the target is to generate metamaterials with specific bandgaps, and an optical metasurface design example, where the target is to generate metasurfaces with high absorbance at specified wavelengths. Our contributions are three-fold. First, we propose a single-shot inverse design method that is fast, generative, interpretable, and small-data-compatible. Secondly, we demonstrate the effectiveness of the proposed method on acoustic and optical metamaterial design examples, and propose both qualitative and quantitative ways of assessing our method. Finally, we create two synthetic test cases for fast examination and validation of model performance. These test cases can be used for future benchmarking studies of related methods. The functional response of metamaterials can be modeled as y=f(±,s)¦“± y=f( = f ( x , s ), where ±± denotes metamaterial design variables (e.g., materials and geometry parameters), s ss is an auxiliary variable representing the independent variable (or the x¥xx-axis) of the response (e.g., the frequency/wavelength or the external stimuli such as temperature), and y¦yy indicates the value of the response associated with our design target. In this paper, we assume y{0,1}¦01y { 0 , 1 } since we only focus on qualitative behaviors at specified frequencies (e.g., for an acoustic metamaterial or an optical metamaterial design, whether a bandgap exists or whether the energy absorbance is higher than a threshold within a range of frequencies). We leave the more challenging problem of tailoring quantitative behaviors as future work. We use a random forest to approximate the function f“ff. A random forest is an ensemble learning method that combines the predictions of multiple decision trees to improve accuracy and reduce overfitting [35]. The trained random forest serves as a forward prediction model that predicts the outcome y¦yy given design variables ±± and the auxiliary variable s ss. Compared to the widely used neural networks, the random forest as a forward prediction model offers (1) significantly faster training, (2) less hyperparameters to tune, (3) less susceptible to overfitting on small data, and (4) interpretability (i.e., the decision-making of each tree in the random forest is transparent). More importantly, this interpretability also allows us to realize inverse design without training a separate inverse model. Figure 2 shows how, by probing the trained random forest, one can estimate a likelihood distribution for target satisfaction of solutions over the entire design space and sample (generate) new designs based on this likelihood distribution. Since we target qualitative (binary) behaviors at specified s ss (e.g., a bandgap in 3-4 MHz frequency or high absorption at a wavelength of 400-500 nm), we first identify the leaf nodes (on each decision tree in the random forest) that are relevant to the s ss in the target (Fig. 2, Step 1). We do this by tracing down each tree, checking only the nodes that use s ss as the splitting feature, and pruning the branches that are irrelevant to the s ss in the target. For example, as shown in Fig. 2, there are two tree nodes using s ss as the splitting feature, with splitting criteria at s¤5 5s 5s ¤ 5 and s¤7 7s 7s ¤ 7. Given the target frequency range of 3¤s¤43 43 s 43 ¤ s ¤ 4, we can remove the right branches of both nodes as these branches are only relevant to s>5 5s>5s > 5 and s>7 7s>7s > 7, respectively, which conflicts with the target range of 3¤s¤43 43 s 43 ¤ s ¤ 4. After pruning these branches, we end up with a set of leaves relevant to the target (highlighted in Fig. 2, Step 1). When we have a combined target (e.g., bandgaps in both 3-4 MHz and 6-7 MHz, as shown in Fig. 2), we need to get the intersection of all the sets of relevant leaves and use that as the final set of relevant leaves (highlighted in Fig. 2, Step 2). Note that a combined target includes cases where there are multiple nonadjacent target ranges (e.g., 3-4 MHz and 6-7 MHz) or when a target range is split by a tree node (e.g., a target range of 4-6 MHz can be split by the node s¤5 5s 5s ¤ 5, thus we need to consider it as the combination of two target ranges ” 4-5 MHz and 5-6 MHz). A more detailed discussion of this step is in Appendix, Sec. B. The next step is to trace up the tree from the NNN relevant leaves, obtained by Step 2, to the root node (Fig. 2, Step 3). This will result in NNN decision paths, along which are nodes indicating splitting criteria for design variables ±± Thus, each decision path represents a set of design variable ranges, or in other words, a region in the design space. We assign each region a score equal to the predicted probability at each corresponding leaf. This probability is learned from the training data and equals the proportion of positive data in a leaf. It indicates the tree™s belief in the probability of a design ±± satisfying the target ¯¯ if the design falls in the design space region corresponding to the leaf. Therefore, with a single decision tree iii, we already have the map of likelihood m(±|¯)=™m(¯|±)subscriptconditional±¯subscript™conditional¯± _ m ( x | caligraphic_T ) = P _ m ( caligraphic_T | x ) for target satisfaction: each of the NNN regions has a uniformly distributed likelihood equal to the predicted probability at the corresponding leaf, and the rest of the design space has a likelihood of 0 (Fig. 2, Step 3). Since a single decision tree usually lacks accuracy, robustness, and a way to quantify estimation uncertainty, we still want to take advantage of the random forest as an ensemble model for inverse design. We use Steps 1-3 to derive the likelihood distribution for each of the MMM trees in the random forest, and simply use the average of these MMM likelihood distributions as the final likelihood for target satisfaction, (±|¯)=mMm(±|¯)/Mconditional±¯superscriptsubscriptsubscriptconditional±¯ ( x | caligraphic_T ) = _ m ^ M caligraphic_L _ m ( x | caligraphic_T ) / M, which is a more complex and smooth function (Fig. 2, Step 4). If more trees believe a design ±± has a higher likelihood of satisfying the target, then the design will have a higher likelihood (±|¯)conditional±¯ ( x | caligraphic_T ). Finally, to generate new designs, we can sample from (±|¯)conditional±¯ ( x | caligraphic_T ) using Markov chain Monte Carlo (MCMC) methods such as Metropolis-Hastings [36] (Fig. 2, Step 5). Compared to prior works, RIGID provides the following unique benefits: It is effective on small data problems as the random forest is less susceptible to overfitting. The training is fast (in seconds of wall time) and does not require computationally-demanding hyperparameter tuning. Once the training is done, no further training or iterative optimization is required to generate designs for different targets. The model is interpretable as one can easily probe the trained model to understand its reasoning behind any decision-making. It estimates the explicit likelihood of target satisfaction for every possible solution in the design space. Given a design target of specific functional behavior, we can generate an unlimited number of solutions based on the likelihood, allowing us to explore alternative solutions that might have desired properties or functionalities beyond the ones considered as the target. When generating design solutions, one can use a single parameter ” the sampling threshold ” to easily tune the trade-off between exploitation (i.e., generated designs have higher chances of satisfying the target) and exploration (i.e., generated designs cover a broader area of the design space), as we will demonstrate in Results. We demonstrate our RIGID method on an acoustic metamaterial design problem, an optical metasurface design problem, and two synthetic design problems. Based on a recent review article by Lee et al. [24] and other related works (e.g., [28]), existing single-shot inverse design methods were demonstrated on training data size ranging from 103superscript10310^{3}10 ^ 3 to 106superscript10610^{6}10 ^ 6 in scale. Here we show that our method can work with much smaller-scale datasets (less than 250 training samples). For all the test problems, we used the same random forest hyperparameter settings and did not perform hyperparameter tuning. Specifically, each random forest contains 1,000 trees with a minimum of 2 samples required to split an internal node and a minimum of 1 sample required to be at a leaf node. Gini impurity [37] was used as the splitting criterion at tree nodes. The train-test split ratio was 4:1. Since the positive/negative training data can be highly imbalanced (e.g., the frequency ranges with bandgaps are much narrower than those without), we used the Synthetic Minority Over-sampling TEchnique (SMOTE) [38] to over-sample the positive class. For all the case studies, the random forest training took less than 10 seconds on an Intel Core i5-9300H CPU 2.4GHz and 8GB memory. After training, we generate new designs by sampling from the resulting likelihood distribution using Metropolis-Hastings. In practice, Metropolis-Hastings can generate identical samples, which provides no benefits for design purposes. Thus in this work, we reject the designs identical to previous ones during sampling. Here we consider acoustic metamaterials that can control elastic wave propagation at ultrasound (MHz) frequencies. Varying the microscale geometries of acoustic metamaterials changes the dynamic properties of a material, such as bandgaps [10] (i.e., forbidden frequency ranges of a material) and wave propagation direction [4]. These materials promise applications in waveguides [4, 5], lenses [6, 7], and vibration mitigation [9]. We present the braced cubic design framework (Fig. 3A-B) as a method to tune the size and location of bandgaps (Fig. 3C). In particular, spherical micro-inertia are added to the center and corner of a braced cubic unit cell with strut radius rstrutsubscriptstrutr_{ _ strut . Micro-inertia placed at the center of the brace has radius rcentersubscriptcenterr_{ _ center while micro-inertia placed at the corner of the cubic unit cell has radius rcornersubscriptcornerr_{ _ corner . We randomly created 284 sets of geometric parameters ±=(rstrut,rcenter,rcorner)±subscriptstrutsubscriptcentersubscriptcorner = ( r _ strut , r _ center , r _ corner ) with 4 ¤rstrut¤absentsubscriptstrutabsent r_{ r _ strut ¤ 6.41, 0 ¤rcenter¤absentsubscriptcenterabsent r_{ r _ center ¤ 20, and 0 ¤rcorner¤absentsubscriptcornerabsent r_{ r _ corner ¤ 20 (unit: µm). The unit cell size was set at a=6060a=60a = 60 µm. For each of these designs, we performed Bloch-wave analysis to compute its acoustic dispersion relation. Bandgap location and width were extracted for each design based on its dispersion relation. Out of the 284 sets of design variables and bandgap data, we used 227 samples as training data. We first discretized the entire frequency range into 100 intervals, and trained a random forest to predict bandgap existence y{0,1}¦01y { 0 , 1 } at a specific interval s ss for a given design ±± The trained model has a test F1 score of 0.82. The resulting confusion matrix on test data is shown in Appendix, Tab. 1. To test the inverse design capability of RIGID, we randomly created 10 targets, each containing 1-2 frequency ranges in which bandgap(s) should exist. We generated 30 designs for each target by sampling from the resulting likelihood distribution over the design space111Note that it is possible for the likelihood to be zero everywhere in the design space when the model believes the target is unachievable. We ignore these cases as it is meaningless and impossible to sample designs from such likelihood distribution.. Bandgaps were identified from dispersion relations computed using Bloch-wave analysis. Figure 3D shows the kernel density estimation (KDE) for the likelihood of the 300 generated designs, conditioned on their target satisfaction. We use and feassubscriptfeas _ feas to represent the complete set of generated designs and the set of generated designs that actually satisfy the target, respectively. Then caligraphic_D _ feas denotes the set of generated designs that cannot fulfill the target in reality. In an ideal scenario, all solutions in would satisfy the target, which means =feassubscriptfeas = caligraphic_D _ feas , and their density profiles should coincide. However, this ideal scenario is not possible due to limited model accuracy. Conveniently, the estimation of target satisfaction likelihood offers us an indicator of what solution is more likely to violate the target. For a reasonable model, most designs in caligraphic_D _ feas should have low estimated likelihood values. Consequently, the density of feassubscriptfeas _ feas ™s likelihood is a result of shifting some of density from left (low likelihood) to right (high likelihood). This expectation aligns with the observation in Fig. 3D. When sampling new designs or selecting solutions from generated designs, we can put a sampling threshold (0,1)01 ( 0 , 1 ) on the likelihood values to filter out less promising solutions. To further examine model behavior and quantify how affects the inverse design outcome, we define the following metrics: where •¥subscriptitalic-• _ • ¥ is the set of generated designs with the likelihood of at least (i.e., the selected designs) and qisubscriptq_{i}q _ i denotes the percentage overlap between the target and the actual behavior of selected designs. The satisfaction rate evaluates how many selected designs satisfy the target based on a binary criterion (i.e., whether or not a design satisfies the complete target), whereas the average score provides a soft measure where partial satisfaction is also counted. The average score is lower-bounded by the satisfaction rate. As shown in Fig. 3E, the selection rate decreases when increases since more solutions are filtered out. On the other hand, both the satisfaction rate and the average score increase with which indicates a high correlation between the estimated likelihood of a solution and its probability of actually achieving the target. As reaches 0.6, the satisfaction rate and the average score reach 1, indicating that all generated designs satisfy their targets. When sampling or selecting new solutions, we can use the sample threshold to tune the trade-off between exploitation and exploration ” a low favors exploration as sampled solutions will cover a larger area of the design space, while a high favors exploitation as sampled solutions will have a higher chance of satisfying the target. Figure 3F visualizes the geometries and dispersion relations of designs generated based on a randomly created bandgap target. Only the top five designs with the highest likelihood values are shown. In this example, our method generates geometrically different designs that have a high probability of achieving target bandgaps, each yielding a slightly different dispersion relation. This is promising in design applications requiring other material properties, such as dynamic wave velocity or quasi-static stiffness, in which the user can select from a menu of designs with the same target bandgap but other varying properties. Generated designs based on the other nine bandgap targets can be found in Appendix, Figs. 8-10. Optical metasurfaces are artificially engineered systems that can support exotic light propagation building on subwavelength inclusions [39, 40, 41, 42, 43, 44]. Among a diverse array of devices, metamaterial absorbers [45, 46, 47, 48, 49, 50, 51] have been intensely studied for medical imaging, sensing, and wireless communications. In this case study, we consider four types of cross-sections (c{1,2,3,4}1234c { 1 , 2 , 3 , 4 }) chosen from the literature (Fig. 4B). It is assumed that a 3D geometric instance is composed of a stack of three layers of prismatic unit cells, each of which is vertically extruded and stacked (Fig. 4A). The geometries constructed in this way can be regarded as an instantiation of multilayered metasurfaces [52, 53, 54, 55, 56], which offer richer design freedom than the single-layer counterpart. The height of each layer (hl,l=1,2,3formulae-sequencesubscript™™123h_{l},l=1,2,3h _ l , l = 1 , 2 , 3) is allowed to continuously vary between 50 and 150 nm. Herein we do not consider parametric variations of a given type of unit cell cross-section; yet those can be trivially incorporated in the proposed design framework if necessary. We also design the material of each layer (ml,l=1,2,3formulae-sequencesubscript™™123m_{l},l=1,2,3m _ l , l = 1 , 2 , 3). Three dielectric materials of interest, each of which is assigned to a different color in Fig. 4A, are Ti (red), Si (blue), and Ge (yellow). In general, a dielectric material is characterized through a complex refractive index n~‚~‚ n C defined as n~=n+jk~ n = n + j k, where j=11j= = square-root - 1 is the imaginary unit, nn R involves the speed at which the light propagates through the material, and kk R is the extinction coefficient that dictates the energy loss due to the material. Within the frequency regime of interest, those exhibit nonlinear dispersion; both the real and imaginary terms in general are a non-analytic function of excitation wavelength s ss, i.e., n(s) n(s)n ( s ) and k(s) k(s)k ( s ). In addition, the impact of the same material choice on the spectral response A(s) A(s)A ( s ) varies depending on the layer location at which the material is placed. Thus the highlight of this case study is the combinatorial nature of design, whose spectral responses are affected by the joint contributions of geometry and material. Based on the above configuration, we randomly sampled 258 sets of design variables ±=(c,h1,h2,h3,m1,m2,m3)±subscript1subscript2subscript3subscript1subscript2subscript3 = ( c , h _ 1 , h _ 2 , h _ 3 , m _ 1 , m _ 2 , m _ 3 ) and computed their corresponding absorbance spectra using wave analysis. We set t=0.9¡0.9t=0.9t = 0.9 as the absorbance threshold, so that high absorbance means the absorbance A(s) A(s)A ( s ) is no less than 0.9. We trained a random forest on 206 training data (i.e., 80% of the 258 designs and corresponding absorbance spectra) to predict whether high absorbance is presented (i.e., the binary indicator y=1¦1y=1y = 1) at a wavelength s ss for a given design ±± The trained random forest has a test F1 score of 0.83. The confusion matrix on test data is shown in Appendix, Tab. 2. Note that this problem involves inverse design with both continuous and categorical variables, which common optimization and generative modeling-based inverse design cannot handle well without special treatment [57, 58, 59]. On the other hand, our random forest-based method can naturally address such mixed-variable problems without any issues. Similar to the acoustic metamaterial design problem, we use 10 randomly created targets to evaluate the inverse design performance of RIGID, except that here a target is represented as the wavelength range(s) within which absorbance should be at least 0.9. We generated 100 designs for each target by sampling from the estimated likelihood distribution. Among the 1,000 generated solutions, we successfully conducted wave analysis for 911 designs and obtained their absorbance spectra. Figure 4D shows the KDE for the likelihood of these 911 designs, conditioned on their target satisfaction. The densities share similar behavior as in the acoustic problem (Fig. 3D) ”unsatisfied/infeasible designs caligraphic_D _ feas are concentrated at low likelihood regions, which causes the likelihood density of satisfied/feasible designs feassubscriptfeas _ feas to be a result of shifting some of density from left (low likelihood) to right (high likelihood). The sampling threshold and metrics relation shown in Fig. 4E also follow the same trend as in the acoustic problem (Fig. 3E), which again demonstrates a strong positive correlation between the estimated likelihood and the probability of generated designs actually achieving their targets. Figure 4F shows generated optical metasurface designs with the top five likelihood estimations for a randomly created target. While the materials, cross-section geometries, and layer heights of generated designs can be different, all the designs satisfy the target (Fig. 4F, right panel). To further enhance the diversity of final solutions, we can use sampling strategies such as the one proposed in Ref. [60, 61] to identify a subset (of generated solutions) that simultaneously exhibits high likelihood and high diversity. Generated designs based on the other nine targets can be found in Appendix, Figs. 11-12. While the above metamaterial design problems represent practical use cases, the validation study is time-consuming due to the expensive computation of metamaterials™ responses such as dispersion relations and absorbance spectra. To allow fast validation of the proposed method and easier inspection of the estimated likelihood in the design space, we create two synthetic case studies. Both problems have 2-dimensional design spaces that allow easy visualization. To construct the first synthetic problem, we used a squared exponential function with tunable parameters aaa and bbb to mimic the quantitative functional response of metamaterials. The qualitative response (e.g., high or low energy absorption at a wavelength) is defined as: where z§zz represents quantitative response and t¡tt is a threshold that converts z§zz into a qualitative response I(a,b;s)¼ I(a,b;s)I ( a , b ; s ). Specifically, I(a,b;s)=1¼ 1I(a,b;s)=1I ( a , b ; s ) = 1 can mean the existence of a bandgap or high absorbance at a frequency s ss. Then {s|I(a,b;s)=1}conditional-set ¼ 1 s | I ( a , b ; s ) = 1 } represents a range of s ss that mimics our design targets, such as the bandgap or the frequency range of high absorbance. By varying aaa and bbb, we can produce different synthetic responses and ranges. Therefore, we can use aaa and bbb as synthetic design variables. There is a clear relation between these design variables and the range that Eq. 2 creates ” aaa and bbb control the center location and the width of the range, respectively. In this design problem, we sampled 100 sets of aaa and bbb uniformly at random. We set t¡tt as 0.9. Based on Eq. 2, we obtained the corresponding responses (Fig. 5A). These sets of aaa, bbb, and responses constitute a dataset for training and testing our model. Another synthetic design problem was constructed by replacing the squared exponential function in the SqExp problem with a superposed sine function. Given synthetic design variables aaa and bbb, we can produce qualitative responses using the following equation: Same as in the SqExp problem, we set t=0.9¡0.9t=0.9t = 0.9 and created a dataset with 100 sets of synthetic design variables and corresponding ranges derived from synthetic responses (Fig. 5B). Unlike the squared exponential function, the superposed sine function can be multimodal, which means it can result in multiple synthetic ranges to mimic, for example, multiple bandgaps. The bandgap locations are controlled by aaa and bbb. For each synthetic example, we split the data into 80 training data and 20 test data, and trained a random forest, with the same hyperparameter settings as the other problems, to predict the binary indicators I(a,b;s)¼ I(a,b;s)I ( a , b ; s ). The F1 scores are 0.85 and 0.86 for the SqExp and the SupSin problems, respectively. The resulting confusion matrices are shown in Appendix, Tables 3-4. We evaluated the inverse design performance with the trained models. Due to the fast evaluation of Equations 2 and 3, we can exhaust all the possible solutions in the design space to obtain the ground-truth feasible region(s) for a target. Figure 6A shows the estimated likelihood values and the ground-truth feasible regions under randomly created targets. In general, high-likelihood regions match actual feasible regions well, which further demonstrates the effectiveness of RIGID. We can also observe that feasible regions in the SqExp and the SupSin problems follow distinct patterns. In the SqExp problem, aaa and bbb control the center location and the width of the output range, respectively. Therefore, the position of the feasible region along the aaa-axis moves with the location of the target range, while the feasible region gradually shrinks as bbb decreases since the decrease of bbb (i.e., output range width) causes the choice of aaa (i.e., output range center location) to be more restricted to fit the target range. In the SupSin problem, there might be multiple bandgaps appearing at the peaks of the superposed sine function in Eq. 3. Design variables aaa and bbb control bandgap locations by translating each sine function. Due to the sine function™s periodicity, we can obtain multiple feasible regions along both aaa- and bbb-axes. Figure 6A shows that the likelihood estimation by RIGID successfully captured the above-mentioned patterns of feasible regions. Figure 6B demonstrates how the estimated likelihood varies when increasing the number of trees in a random forest. With a single decision tree, the estimated likelihood function is almost a binary function and highly inaccurate. The likelihood in the SqExp case is even zero everywhere, which makes it impossible to sample designs based on the likelihood. As the number of trees increases, the likelihood function becomes smoother and eventually converges. Besides these qualitative visual inspections, we also calculated the metrics proposed in Eq. 1, as shown in Fig. 6C. For each of the two synthetic problems, these metrics were computed on 500 designs generated by giving five random target ranges. Again, the satisfaction rate and the average score increase with the sampling threshold, indicating a strong correlation between the sampling threshold and the probability of generated designs actually achieving their targets. In both problems, all the selected designs satisfy their targets (i.e., the satisfaction rates and average scores reach 1) when the sampling threshold reaches 0.8. We proposed RIGID, a single-shot inverse design method that generates metamaterials to satisfy qualitative behaviors of functional responses. Such qualitative behaviors are important design targets in many applications such as tailoring bandgaps of acoustic metamaterials for wave-guiding, focusing, and vibration mitigation, or tailoring the absorption level of optical metasurfaces at certain wavelengths for medical sensing, imaging, and communication applications. Unlike most existing inverse design methods that require training an inverse model to map targets to designs, the RIGID method takes advantage of the random forest™s interpretability and derives the likelihood of target satisfaction by probing the trained forward model. Incorporated with MCMC, one can sample a desired number of new designs based on the estimated likelihood. Therefore, RIGID functions as a generative model that can capture the conditional distribution of satisfying designs given a target, or in other words, the one-to-many mapping from the target to satisfying designs. Using both real-world and synthetic design problems, we demonstrated that RIGID is efficient and effective on datasets with training sample sizes smaller than 250. Thus, RIGID is particularly useful when data collection is expensive, as in many cases where high-fidelity simulation or experimental data are needed. We used both qualitative and quantitative approaches to validate the proposed method. The quantitative results revealed a strong correlation between the estimated likelihood of a solution and its probability of actually achieving the target, which demonstrated the effectiveness of the likelihood estimation. Due to the fast evaluation of output responses and the transparency of ground-truth solutions, the proposed synthetic problems can be used for future benchmarking studies of metamaterial design problems. While we address qualitative design targets in this study, the idea of using random forest-based models for inverse design has the potential to generalize to quantitative targets. Such problems can be, for example, generating optical metasurface designs with specific optical spectra [62, 21], generating functional materials with target nonlinear constitutive relations [33, 63], or generating programmable metamaterials with prescribed functional responses [64, 65]. It is also straightforward to adjust the target to achieve multifunctionality (e.g., negative/positive Poisson™s ratio under low/high compression rate [66]). Although this study only demonstrates the RIGID method on parametric design (i.e., designs are represented by geometric and/or material parameters), the method also applies to shape or topological design problems where the shape or topology of designs can vary without being restricted to a limited number of geometric parameters [62, 29, 21, 31, 67]. In those cases, as valid designs only lie on a lower-dimensional manifold of the design space, the likelihood of target satisfaction will be zero almost everywhere in the original design space. Thus before applying RIGID, we need to obtain a latent representation that compactly captures the manifold of valid designs [68, 69], and use the latent representation as design variables for inverse design. This work was supported by the startup funds from the J. Mike Walker ™66 Department of Mechanical Engineering at Texas A&M University, the National Science Foundation (NSF) BRITE Fellow program (CMMI 2227641), the NSF CSSI program (OAC 1835782), the Kansas City National Security Campus (PDRD #705288), and NSF CAREER Award (CMMI-2142460). R.S. acknowledges financial support from the NSF Graduate Research Fellowship Program. Figure 7 shows the schematic diagrams of three mainstream machine learning models for single-shot inverse design of metamaterials. The purpose of Step 2 is to obtain the intersection of relevant design space regions for all the ranges of s ss in a target. We are approximating this goal by simply obtaining the intersection of relevant leaves. However, some non-intersecting leaves may still have intersecting design space regions. When assigning the probability to the intersecting region of two non-intersecting leaves AAA and BµBB, we need to consider the predicted probabilities at both leaves (PAsubscriptƒP_{A}P _ A and PBsubscriptƒµP_{B}P _ B ). Specifically, the assigned probability at this intersecting region should be PAPBsubscriptƒsubscriptƒµP_{A}P_{B}P _ A P _ B , which can be small. Therefore, we adopt the simplification of only considering the intersection of relevant leaves and ignoring the intersecting regions associated with non-intersecting leaves. The results also demonstrate that this is a reasonable approximation. We performed Bloch-wave analysis in COMSOL Multiphysics to compute the dispersion relations of acoustic metamaterials. Poisson™s ratio of 0.49, Young™s modulus of 2.7 GPa, and density of 1170 kg/m33{{}^{3}}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT were set as material properties with ¼1.5Ã104similar-toabsent1.5superscript104 1.5 10^{4}¼ 1.5 Ã 10 ^ 4 mesh elements per unit cell. We used Floquet-Bloch periodic boundary conditions to obtain the first 60 eigenfrequencies along all symmetry domains of the cubic irreducible Brillouin zone (Fig. 3B) for all lattices, thus generating a dispersion relation. We computed the absorbance spectra for optical metasurfaces using wave analysis inspired by Zhang et al. [56]. The RF Module of COMSOL Multiphysics® [70] was used to evaluate the spectral response of concern, which is the energy absorbance A(s) A(s)A ( s ) in the visible regime (380-700 nm). An absorbance spectrum is computed with respect to 33 wavelength components sksubscript s_{k}s _ k that are uniformly discretized over the specified range. An incident plane wave is assumed to be given from the port, located at the top face of the analysis domain. We set the periodicity of the analysis domain as 400 nm. The periodic boundary condition on electromagnetic fields is imposed on the lateral faces of the analysis domain. A substrate made of SiO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT is placed right below a given unit cell instance (the black layers in Fig. 4A-B). With full electric fields computed through the wave analysis, the energy absorbance at a single wavelength s ss is quantified as A(s)=1|S11(s)|2 1superscriptsubscript†11 2A(s)=1-|S_{11}(s)|^{2}A ( s ) = 1 - | S _ 11 ( s ) | ^ 2 , where Sijsubscript†S_{ij}S _ i j is the component of the S†SS-parameter matrix that specifies energy transfer between ports. We use the data presented in Ref. [71] to set the material dispersion of the dielectric. Test data for all the design problems are designs ±± (that the random forests have never seen during training) and their corresponding qualitative behaviors y{0,1}¦01y { 0 , 1 } based on functional responses. In the acoustic metamaterial design problem, we have 57 test designs, yielding 5,700 test points as the entire frequency range of dispersion relations is discretized into 100 intervals for each design. In the optical metasurface design problem, we have 52 test designs, with each functional response discretized into 33 points, which yields 1,716 test points in total. In each of the synthetic design problems, we have 20 synthetic test designs, with each synthetic response discretized into 100 points. This results in 2,000 test data points in total. Confusion matrices showing test performances are in Tables 1-4. Figures 8-12 show the rest of generated designs and their corresponding responses in addition to those in Figures 3F and 4F.",
        "keywords": ""
    },
    {
        "id": 4,
        "title": "Metabolic scaling in small life forms",
        "abstract": "AbstractMetabolic scaling is one of the most important patterns in biology. Theory explaining the 3/4-power size-scaling of biological metabolic rate does not predict the non-linear scaling observed for smaller life forms. Here we present a new model for cells<108absentsuperscript108<10^{-8}< 10 ^ - 8 m33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTthat maximizes power from the reaction-displacement dynamics of enzyme-catalyzed reactions. Maximum metabolic rate is achieved through an allocation of cell volume to optimize a ratio of reaction velocity to molecular movement. Small cells<1017absentsuperscript1017<10^{-17}< 10 ^ - 17 m33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTgenerate power under diffusion by diluting enzyme concentration as cell volume increases. Larger cells require bulk flow of cytoplasm generated by molecular motors. These outcomes predict curves with literature-reported parameters that match the observed scaling of metabolic rates for unicells, and predicts the volume at which Prokaryotes transition to Eukaryotes. We thus reveal multiple size-dependent physical constraints for microbes in a model that extends prior work to provide a parsimonious hypothesis for how metabolism scales across small life.",
        "corpus": "Metabolic scaling is one of the most important patterns in biology. Theory explaining the 3/4-power size-scaling of biological metabolic rate does not predict the non-linear scaling observed for smaller life forms. Here we present a new model for cells <108absentsuperscript108<10^{-8}< 10 ^ - 8 m33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT that maximizes power from the reaction-displacement dynamics of enzyme-catalyzed reactions. Maximum metabolic rate is achieved through an allocation of cell volume to optimize a ratio of reaction velocity to molecular movement. Small cells <1017absentsuperscript1017<10^{-17}< 10 ^ - 17 m33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT generate power under diffusion by diluting enzyme concentration as cell volume increases. Larger cells require bulk flow of cytoplasm generated by molecular motors. These outcomes predict curves with literature-reported parameters that match the observed scaling of metabolic rates for unicells, and predicts the volume at which Prokaryotes transition to Eukaryotes. We thus reveal multiple size-dependent physical constraints for microbes in a model that extends prior work to provide a parsimonious hypothesis for how metabolism scales across small life. Understanding how and why organisms differ in their demand for and use of resources is a key objective of biologists [1, 2, 3] and critical to understanding the response of biodiversity and ecosystem function to global changes [4, 5]. A fundamental and often-debated pattern is how metabolic rate, BµBB, scales with body mass, MMM [1, 6, 7], followng the form Data for vertebrates and vascular plants [8, 9, 10, 11, 12]show an average interspecific scaling exponent of 3/4. This relationship has inspired diverse theories [10, 11, 13, 14, 15], including the network model, which derives 3/4 from the need for organisms to supply the entire body volume with resources from vascular resource distribution networks that minimize energy dissipation. More recent analyses [16, 1, 17, 18, 19, 20] that include organisms from the smallest 11 orders of magnitude in size that largely lack vascular distribution networks, show a variable metabolic scaling exponent that changes across size ranges from scaling exponents as high as 2 “ super-linear scaling “ for the smallest range and near 3/4 for the largest organisms. Understanding the basis for this variation is important, as many different organism features can be derived from this metabolic scaling, including how maximum growth rates and ribosomal abundances scale with cell size along with key tradeoffs between features [21, 22]. However, there is no general, parsimonious theory derived from physical and chemical principles that addresses this size-dependent scaling for these smallest organisms [16]. Here we propose a combined thermodynamic and reaction description of metabolic rate for a cell. We assume that natural selection will favor organisms that can perform more work per time (power) for material synthesis, replication, repair, locomotion, and other cellular functions [23]. While often co-limited by materials in the environment, optimizing this power faces cellular trade-offs concerning both environmental and intra-cellular physical constraints. Consequently, we optimize cellular features to maximize the free energy produced by the conversion of chemical substrates to products, where heat and products may inhibit metabolism if not moved away from reaction sites. This thermodynamic approach provides a framework for considering the two processes, conversion and displacement, simultaneously. We assume that work is proportional to the volume of reactive surface, where macromolecular catalytic enzymes are freely dispersed in the cytoplasm or attached to the membranes, cytoskeleton, and/or other organelles within the entity [24]. However, a portion of cell volume is needed to allow substrates from the environment to reach reaction surfaces and to allow displacement of reaction products away from reaction structures. This sets up a conflict between the volume devoted to metabolic processes and that devoted to transport. Here we derive a physical and chemical principles-based theory for metabolic scaling for metabolic rate across the roughly 11 orders of magnitude in volume of organisms that lack branching vascular systems. We describe metabolism from a reaction-displacement thermodynamic system centered on or near reaction surfaces and its generation of free energy within a spherical space otherwise obstructed by surfaces at which reactions occur. Our framework applies to unicellular Prokaryotes, Archaea, and Eukaryotes along with eukaryotic organelles. Our basic model, which we expand on in stages, is to consider metabolism in a sphere described by the following reaction-diffusion model where the product, P(r)ƒP ( r ), substrate, A(r)A ( r ), and enzyme, Z(r)Z ( r ), concentrations (mol m33{}^{-3}start_FLOATSUPERSCRIPT - 3 end_FLOATSUPERSCRIPT) are all a function of radius, rrr, inside a sphere, and where the dynamics are one dimensional in spherical coordinates with spherical symmetry. Additionally, k=kcat/KMsubscript¡subscript¾k=k_{cat}/K_{M}k = k _ c a t / K _ M is the reaction constant ((m33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT mol 11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT s11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT)) and D·DD is a displacement coefficient (e.g. molecular diffusivity in some cases) and has units of (m22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT s11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT) so that each rate is a change in concentration (mol m33{}^{-3}start_FLOATSUPERSCRIPT - 3 end_FLOATSUPERSCRIPT s11{}^{-1}start_FLOATSUPERSCRIPT - 1 end_FLOATSUPERSCRIPT). We consider the steady-state dynamics, representing a persistent entity and its metabolism in a fixed environment over time, allowing us to solve for closed-form solutions of P(r)ƒP(r)P ( r ) and A(r)A(r)A ( r ) under a given concentration of ZZZ (see SI). The steady-state free energy production at any location in the cell is given by where Keqsubscript¾K_{eq}K _ e q is the equilibrium constant of the energy producing reaction (which can be interrelated with Î”G0Î”subscriptº0 G_{0}roman_Î” G _ 0 ), TTT is temperature, and R…RR is the ideal gas constant. The entire metabolism (given as a power in watts) of the entire cell is then described by where rcsubscriptr_{c}r _ c is the radius of the cell and VesubscriptV_{e}V _ e is the essential volume required for other for other cellular materials, such as DNA, and is unavailable for energy generation. In this system, we can maximize power (free energy/time), BµBB (Watts), as a function of ZZZ. If there were no tradeoffs, then BµBB would be maximized by the largest feasible ZZZ at a given size (a cell full of enzymes). However, the need to move substrate into different regions of the cell and to displace products away from reaction sites introduces a trade-off between the effective diffusion coefficient, D·DD, and enzyme concentration, ZZZ. An increase in ZZZ corresponds to a decrease in diffusivity [25, 26], and this leads to an optimal enzyme concentration corresponding to a maximal metabolic rate (see SI). Maximizing power over the whole cell, at a given cell size rcsubscriptr_{c}r _ c subject to variation in the concentration of enzymes, ZZZ, we find that the optimal enzyme concentration Z*superscriptZ^{*}Z ^ * follows where Î³¾ is the scaling between diffusivity and enzyme concentration at high enzyme concentrations, and D0subscript·0D_{0}D _ 0 is the normalization constant for that scaling relationship (see SI). Remarkably, this relationship shows that the optimal enzyme concentration in cells can be predicted from a few fundamental thermodynamic, Keqsubscript¾K_{eq}K _ e q , and kinetic, kkk, constants (Figure 1). Note that Z*superscriptZ^{*}Z ^ * depends on a ratio of reaction rate, kkk, to a diffusivity normalization for small molecules in the cytoplasm, D0subscript·0D_{0}D _ 0 . Using the known relationship for diffusivity with enzyme concentration, where Î³3/2¾32 - 3 / 2 for sufficiently high concentrations [26], we obtain Previous studies have shown that total protein count scales with cell size following a power law with an exponent <1absent1<1< 1 [22], but these relationships have no fundamental explanation. Our optimization predicts that protein concentration should become more dilute as cells become larger following an exponent of 4/150.274150.27-4/15 4 / 15 - 0.27, which is indistinguishable from the best fit exponent to data of 0.30±0.06plus-or-minus0.300.06-0.30 0.06- 0.30 ± 0.06 (Figure 1). The prediction for ZZZ was determined by optimizing metabolic rate and so we also predict the relationship for maximum metabolic rate: Here B0subscriptµ0B_{0}B _ 0 is a constant that is a complicated function of diffusivity, kinetic and thermodynamic parameters (see SI), and VesubscriptV_{e}V _ e is the volume of other essential macromolecules which follows Ve=v0VcÎ±subscriptsubscript£0superscriptsubscript¼V_{e}=v_{0}V_{c}^{ _ e = v _ 0 V _ c ^ Î± with Î±=0.83¼0.83 = 0.83 [22]. Equation 7 provides excellent agreement with observed metabolic rates for prokaryotes (Fig. 2). Metabolic rate has strong, non-power law curvature for small cell sizes driven by the volume VeV{e}V e required for macromolecules not involved in reactions, such as DNA, and this is what leads to the apparent super-linear (Î²>1½1 > 1) scaling previously observed for prokaryotes [16]. This curvature quickly relaxes for larger cells due to the sublinear (Î²<1½1 < 1) scaling of VesubscriptV_{e}V _ e . For cell sizes up to about the volume of E. coli (1018absentsuperscript1018 10^{-18} 10 ^ - 18 m33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT) the power law approximation is superlinear (SI Figure 4) with an approximate exponent of 1.681.681.681.68 which agrees well with the previous estimates of 1.751.751.751.75 [16]. For larger bacteria, the scaling of BµBB converges on the sublinear scaling of 11/15=0.7311150.7311/15=0.7311 / 15 = 0.73. For intermediate prokaryote cell sizes, binning the data to remove oversampling of certain cell volumes and scatter, shows exceptionally good agreement between our model and the data (SI Figure 5). These results are not in conflict with the overall superlinear fit to all of the prokaryotic data [16], but show that this is a consequence of a complicated set of scale transitions and shifting constraints. Thus, downstream models that derive results from the scaling exponent of metabolic rate (e.g. [21, 22]) will gain accuracy and predictive power by starting from our more complex model. For example, models of growth which assume a power law for metabolism [21] should incorporate the curvature described in Equation 7. The trade-off between decreasing enzyme concentration and increased diffusivity applies only for high concentrations of the enzyme. Eventually, for large enough cells, sufficiently low enzyme concentrations are reached such that D=Dmax·subscript·¥D=D_{max}D = D _ m a x , the maximum diffusivity of small molecules in water (see SI). Once diffusivity saturates, the optimal concentration follows ZVc2/3proportional-tosuperscriptsubscript23Z V_{c}^{-2/3}Z V _ c ^ - 2 / 3 (see SI) yielding BVc1/3proportional-toµsuperscriptsubscript13B V_{c}^{1/3}B V _ c ^ 1 / 3 (Fig. 2, dashed red curve) and a metabolic rate controlled by diffusion across the cell membrane. Such scaling also avoids thermodynamic shutdown (power = 0) that can occur in the center of larger cells. Shutdown can occur when Dmaxsubscript·¥D_{max}D _ m a x is not fast enough to sufficiently displace reaction products and avoid reaction reversal. These changes may result in body designs, such as large central vacuoles, filamented rods, membrane bound organelles, and extreme polyploidy [28, 29, 22, 30] that minimize the potential for interior thermodynamic shutdown, but yield a relatively slow metabolic rate for prokaryotes larger than 1017superscript101710^{-17}10 ^ - 17 m33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT [22]. With further increases in cell volume, metabolic rate can only be increased by increasing the rate of molecular displacement, as ZZZ is well below the concentration that influences diffusivity. A variety of means for increasing transport in cells exist (e.g. [31, 32, 33, 34, 35, 36, 37, 38, 39, 40]) with most attributable to the addition of structures, such as molecular motors or transport proteins, that each generate active transport of molecules or viscous bulk flow of cytoplasm over at least some local region within the cell. We model this as an enhanced effective diffusivity of Dtranssubscript·¡ D_{trans}D _ t r a n s . For example, the movement of molecules along a cytoskeletal network and randomly arranged regions of bulk flow with a random flow direction can each be thought of as random walks with long single steps, and so displacement in the simplest summary model for all of these processes can be described as an enhanced diffusion (e.g. [31, 41, 42, 35, 36, 34, 39, 40]). A new trade-off is introduced between the space occupied by dedicated transport structures versus the space devoted to catalyzing reactions (enzymes and supporting structures). This effective diffusivity and volume tradeoffs are captured by and where Dtranssubscript·¡ D_{trans}D _ t r a n s is the enhanced diffusivity in the region affected by the active transport or bulk flow, is the fraction of the average cellular volume that is associated with active transport or bulk flow, and µitalic-µ is the ratio of the effective volume of active transport to the volume of the structures generating that transport (e.g. the ratio of the volume of a region of enhanced transport to a molecular motor or transport protein™s volume). We optimize total metabolic rate (see SI) at a given cell volume as a function of and find that grows with cell size following a complicated function (see SI) that scales like Vc0.10absentsuperscriptsubscript0.10 V_{c}^{0.10} V _ c ^ 0.10 for small cell sizes and Vc2/3absentsuperscriptsubscript23 V_{c}^{2/3} V _ c ^ 2 / 3 in the limit of large cell sizes. This density scaling implies that the volume dedicated to transport structures scales super-linearly with overall cell size (Vc1.1absentsuperscriptsubscript1.1 V_{c}^{1.1} V _ c ^ 1.1 to Vc5/3absentsuperscriptsubscript53 V_{c}^{5/3} V _ c ^ 5 / 3 ).Thus, for cell sizes larger than 1017superscript101710^{-17}10 ^ - 17 m33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT, the effective diffusivity parameter can increase with cell volume but at the expense of an ever-increasing proportion of cell volume devoted to transport. This yields a size-dependent curve for maximum metabolic power in logarithmic space (blue curve in Fig. 3) that fits the available data for metabolic rate of Eukaryotes (again with average transport and reaction kinetic parameters). Over the middle size range of the single cell Eukaryotes, metabolic rate should approximate a power law with an exponent of 0.900.900.900.90 which agrees very well with the power law fit to data of 0.90±0.17plus-or-minus0.900.170.90 0.170.90 ± 0.17 [16, 21]. However, the super-linear scaling of the required volume of transport molecules means that motors and transport proteins begin to substantially reduce the volume for reaction structures. This transport volume requirement ultimately may limit metabolic rate, and impose a theoretical upper limit to Eukaryote size of 109superscript10910^{-9}10 ^ - 9 m33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT which compares well to the size of the largest unicellular organisms in the metabolic database (Fig. 2). These analyses produce a series of metabolic scaling curves (Fig. 2) across a succession of cell (or organism) volume ranges. Each curve is associated with a particular mode of reaction volume and molecular displacement mechanisms, with molecular diffusion at the smallest sizes then with an enhanced diffusion within devoted transport regions at the largest sizes (Fig. 3). Across the size spectrum, optimal metabolic rate is set by the tradeoff between volume for transport and volume for metabolic reactions. For prokaryotes, the smallest cells require space for other macromolecules such as DNA, which limit the space available for catalytic reaction enzymes. The largest unstructured prokaryotes face the slow scaling of metabolism set by the maximum diffusivity of small molecules in water. For unicellular eukaryotes, transport molecules provide the needed increase in molecular movement and metabolism scales almost linearly with cell volume. At the largest sizes, unicellular eukaryotes reach a sharp decrease dictated by the required overpacking of transport structures to assure the necessary effective diffusivity throughout the cell. Collectively, these curves merge to form an overall nonlinear relationship between metabolic rate and size that fit observed data very strongly (Fig. 2) and predict the scaling of protein and other material concentrations and the size at which major transitions in body plans occur. These curves clearly depart from the prediction of a single scaling exponent of 3/4343/43 / 4 predicted by the theory for the optimal organization of vascular networks, and support the general hypothesis that the smallest forms of life face different physical constraints, with different accompanying solutions, for maximizing metabolic power. However, the derived mechanisms here tend, as a first approximation, to a sublinear scaling not far from 3/4343/43 / 4. For example, the mid range bacteria are predicted to scale like 11/15=0.7311150.7311/15=0.7311 / 15 = 0.73, and the largest eukaryotes scale like .80absent.80 .80 as curvature sets in. These scalings for both small life and for organisms with vascular systems occur for very different mechanistic reasons, but are linked by the fundamental need for life to allocate and organize space to achieve sufficiently high rates of molecular displacement. Our results thus support more general arguments for why metabolic scaling should not be either surface area to volume scaling (Î²=2/3½23 = 2 / 3) or strictly proportional to volume (isometric, Î²=1½1 = 1). Our work suggests that previous analyses of prokaryote metabolic scaling with all of the data [16], which found super-linear scaling, requires size-dependent shifts in physical constraints that accompany shifts in scaling to approximate a super-linear power law. Our results demonstrate that the overall super-linear pattern is driven by strong curvature away from a power law at the smallest cell sizes. Downstream extension of the fits using all data have been used to derive other features such as the increase in growth rate with cell size and the total abundance of ribosomes [21, 22]. Such models may gain accuracy and improved resolution in size ranges by incorporating the more complicated metabolic scaling derived here. In addition, other cellular constraint perspectives have been proposed for explaining scaling in bacteria such as biosynthetic costs of the membrane [44], ribosome and protein abundances and costs [45, 22], the spatial location and number of organelles and genomes [16, 29], the increasing number of genes and their cost [16, 29, 45, 46], and transporter optimizations associated with the environment (e.g. [23] for a review). In addition, the deployment of phase separation as another means for enhancing reaction rates [47, 48] and the spatial architecture of macromolecules [49, 50] may also be important for cellular scaling. These are all important biophysical, physiological, and evolutionary considerations, and it will be important to integrate these additional constraints with cellular scaling in a more complicated optimization approach [23] that combines these costs with the diffusive tradeoffs employed here. Large prokaryotes also deploy active molecular transport and so the two models presented here represent bounding cases where large prokaryotes (E. coli and bigger) and small unicellular eukaryotes. Multicellular organisms overcome the limit faced by the largest single cells by evolving mechanisms of bulk flow among cells. At its extreme, optimizing such bulk flow through tubular vascular systems to minimize energy dissipation (friction) produces the well-known BM3/4proportional-toµsuperscript34B M^{3/4}B M ^ 3 / 4 [10, 11, 51, 13, 52, 53]. Taken together, our results and vascular network models show that, organisms across the tree of life may face universal tradeoffs between biochemical reactions and the space devoted to transport, where larger organisms must employ increasingly energy-requiring methods to increase molecular transport velocities. Our results also predict the scaling of materials within the cell. For small entities relying on diffusion, we predict the total mass of enzymes (protein) to scale as Vc11/15superscriptsubscript1115V_{c}^{11/15}V _ c ^ 11 / 15 , which closely agrees with observed scaling proportional to Vc0.70superscriptsubscript0.70V_{c}^{0.70}V _ c ^ 0.70 [22]. We also predict the minimum size and volume scaling at which cells should produce molecular motors, microtubules and other cytoskeletal material and other mechanisms to enhance diffusivity (Fig. 2). These scaling predictions cannot yet be tested, as they predate the measurement of such volumes or masses for diverse species and cell sizes, with just recently-developed microscopy techniques. However, the largest prokaryotes are known for unusual storage capacity [28, 22], many copies of the genome [29], and surprising amount of cytoskeleton [54]. All of these characteristics may give hints to the eukaryotic transition, and indeed Asgard archaea, which are suggested to be the eukaryotic ancestor, have several atypical cellular morphological characteristics [54, 55, 56]. Our results suggest that active transport in and/or bulk flow of the cytoplasm is likely required to generate free energy even in single cell organisms such as larger Prokaryotes and single-cell Eukaryotes. Extrapolation of the required molecular movement in our models further suggests that smaller multicellular organisms may require mechanisms such as contraction of tissues and/or locomotion to generate bulk transport of intercellular fluids without an organized single source vascular system. Extending further, branching pressurized circulatory networks may be required to sustain metabolism for organisms >1absent1>1> 1 g, a hypothesis supported by evidence that smallest mechanical pumping circulatory systems are limited by damping of pulsatile flow [10]. Our results now provide a physical and chemical explanation for the unique metabolic scaling exhibited by the smallest 11111111 orders of magnitude in organism size. We demonstrate that these life forms face significant design constraints, set by limited space for macromolecules and diffusive molecular movement, and the continuing need for faster molecular displacement mechanisms for the largest unicellulars. The predicted scaling relationships as organisms shift across the different transport limitation domains suggest that natural selection has addressed this sequence of problems during the evolution of larger cell body sizes. Such evolution has led to cells with clustered reaction surfaces, channels of cytoplasm for transporting molecules, molecular motors and cytoskeletal structures to enhance diffusion and then finally by mechanical pumping mechanisms that preview the evolution of powered branched circulatory systems. Our work now extends prior theory to provide a parsimonious hypothesis for how metabolism and the materials that support such activity scale across all life. The coupled diffusion equations to be solved for steady state are with the boundary conditions The solution to these equations is Following a similar derivation, the corresponding equation for PƒPP is Returning to the integral which represents the full metabolic rate of the cell, can can use these solutions for AAA and PƒPP set constraints on ZZZ. To avoid thermodynamic shutdown we need KeqA(r)P(r)>1subscript¾ƒ1 K _ e q A ( r ) P ( r ) > 1 which, given the solutions above, leads to the condition Thermodynamic shutdown is a problem for the interior of the cell and so we are interested in checking this condition for r=00r=0r = 0. Evaluating the above inequality at r=00r=0r = 0 and expanding in small rcsubscriptr_{c}r _ c leads to This equation isn™t fully solved because the diffusivity, D·DD, depends on ZZZ. Previous work has shown that for large enough ZZZ changes in diffusivity will approximately follow a power law [26], and so we can consider D(Z)=D0ZÎ³·subscript·0superscript¾D ( Z ) = D _ 0 Z ^ Î³ which yields which for P0ƒ0P 0P 0 is This equation states how the concentration of enzyme should either concentrate or dilute with cell size to avoid thermodynamic limitations. Given that Î³3/2¾32 - 3 / 2 [26] we have that Zrc4/5proportional-tosuperscriptsubscript45Z r_{c}^{-4/5}Z r _ c ^ - 4 / 5 or It should be noted that given Equation 20, ZZZ is bounded by a concentration that is diluting out as cells increase in size. This should set a hard upper bound where the cellular concentrations start to reach discrete numbers of enzymes. Thus, under a fixed tradeoff for diffusivity an upper bound on cell volume is predicted by the dilution of enzyme concentrations required to avoid thermodynamic shutdown. It should be noted that once ZZZ reaches sufficiently low concentrations diffusivity saturates to a maximum value set by molecular motion in the fluid [26]. At this point D(Z)=Dmax·subscript·¥D ( Z ) = D _ m a x and we have In this limit ZZZ will scale like Vc2/3superscriptsubscript23V_{c}^{-2/3}V _ c ^ - 2 / 3 to avoid thermodynamic shutdown. More generally, we are interested in the concentrations of enzymes that would optimize cellular metabolic rate. The above arguments give us a bounding expectation for the scaling of enzyme concentration, but we can replace these with the complete optimization of cellular metabolic rate, BµBB. Specifically, we are interested in where the total metabolism integrated over a cell is Considering cases where the environmental concentration of the product is low, P00subscriptƒ00P_{0} 0P _ 0 0, the integrand is given by We approximate this integral considering small rcsubscriptr_{c}r _ c and with, D(Z)=D0ZÎ³·subscript·0superscript¾D ( Z ) = D _ 0 Z ^ Î³ , the dependence of diffusivity on ZZZ . Given these assumptions integrand is well approximated by leading to the solution of the integral as In the small radius limit the derivative, ‚B/‚Zµ B/ Z‚ B / ‚ Z, is then well approximated by Setting this to zero leads to This optimal solution of Zoptsubscript¡Z_{opt}Z _ o p t can be substituted back into the approximate solution for total metabolic rate, BµBB, which we find does a good job of approximating the full numerical integral. If we take Î³=3/2¾32 = - 3 / 2 this gives This solution shows that the optimal ZZZ scales like in agreement with our considerations above for thermodynamic shutdown. The corresponding solution for metabolic rate is given by which scales like predicting that metabolic rate will scale slightly sublinearly for sufficiently large size. It should be note that this is the approximate scaling of BµBB over a range of larger cell sizes, but the full form of BµBB is more complicated a small cells sizes, especially considering the factor of (1Ve(rc)/Vc)1subscriptsubscriptsubscript(1-V_{e} 1 - V _ e ( r _ c ) / V _ c ) multiplying the entire scaling where Ve=v0VcÎ±subscriptsubscript£0superscriptsubscript¼V_{e}=v_{0}V_{c}^{ _ e = v _ 0 V _ c ^ Î± with Î±=0.83¼0.83 = 0.83 [22]. Figure 4 shows the full solution of BµBB with more strongly superlinear scaling because of the small-size curvature. It should be noted that at very low ZZZ the diffusivity saturates to a constant maximum behavior and stops following a power law. In this case diffusivity reaches the maximum molecular diffusivity, D=Dmax·subscript·¥D=D_{max}D = D _ m a x , and the above equations lead a optimal enzyme concentration of and corresponding metabolic rate of Here enzyme concentration will scale like ZVc2/3proportional-tosuperscriptsubscript23Z V_{c}^{-2/3}Z V _ c ^ - 2 / 3 and BVc1/3proportional-toµsuperscriptsubscript13B V_{c}^{1/3}B V _ c ^ 1 / 3 . However, these scalings are unlikely to be observed because of the shift to employing active transport, and these results are only shown to illustrate the extreme limitation faced by large cells if they only used molecular diffusion. Our model for active transport treats diffusion in the cell as with a modified effective diffusivity, D²superscript·²D^{ ^ ² , that follows where is the fraction of cell volume occupied by transport structures. Here we define an effective volume (region with enhanced transport) of a transport structure, and is the ratio of the total of these effective volumes to the total cell volume. The optimization of metabolism now depends on tradeoffs between the adjusted diffusivity and the space occupied by the transport structures, where total metabolism follows where µitalic-µ is the ratio of the volume of a transport structure to the effective volume of enhanced transport that structure creates. The solutions for A(r)A ( r ) and P(r)ƒP ( r ) are the same as in Equations 15 and 16. We are again interested in optimizing cellular metabolic rate, BµBB, where now we are looking for solutions to The integration of BµBB is well approximated by for small values of rcsubscriptr_{c}r _ c . Again we are considering the regime where transport, now represented by the adjusted diffusivity D²superscript·²D^{ ^ ² , is large relative to the scale of the cell. Taking the derivative of this this solution and again approximating it for small rcsubscriptr_{c}r _ c and also for small Dmaxsubscript·¥D_{max}D _ m a x (the maximum molecular diffusivity) we have which gives the optimal fraction of transport structures as where WŠWW is the Lambert WŠWW function (the product logarithm). For small rcsubscriptr_{c}r _ c this is well approximated by Similar to our solutions above for the enzyme concentration tradeoffs in bacteria, this function again highlights that optimal metabolic values depend on DKeqkrc2·subscript¾superscriptsubscript2 D K _ e q k r _ c ^ 2 . Taking all of this together we have that the optimal metabolic rate follows This scales like Brc3Vcproportional-toµsuperscriptsubscript3proportional-tosubscriptB r_{c}^{3} V_{c}B r _ c ^ 3 V _ c with a logarithmic correction term. This explains why the scaling of metabolism in Eukaryotes is close to linear with cell volume but actually sublinear due to the logarithmic terms. Figure 6 gives the approximate local exponent of BµBB with cell size. The full range of Eukaryotes is well approximated by Î²=0.90½0.90 = 0.90 which agrees with data as discussed in the maintext. This function also comes with an asymptotic limit that occurs because transportors begin to overpack the cell. This limit occurs when which corresponds to an maximum single cell eukaryote of This solution mirrors the molecular diffusion limit but with an altered diffusivity.",
        "keywords": ""
    },
    {
        "id": 5,
        "title": "Gradient Estimate for Fisher-KPP Equation on Finsler metric measure spaces",
        "abstract": "AbstractIn this manuscript, we study the positive solutions of the Finslerian Fisher-KPP equationut=Î”uu+cu(1u).subscript¡superscriptÎ”1u_{t}=\\Delta^{\\nabla u}u+cu(1-u).u _ t = roman_Î” ^ u u + c u ( 1 - u ) .The Fisher-KPP equation is widely applied and connected to many mathematical branches. We establish the global gradient estimates on compact Finsler metric measure manifold with the traditionalCD(K,N)¶·¾CD(K,N)C D ( K , N )condition, which is developed by S. Ohta and K.-T. Sturm in Finsler geometry. Furthermore, With the assistance of a new comparison theorem developed by the first author, we also give the gradient estimate on forward complete noncompact locally finite misalignment Finsler metric measure spaces with the mixed weighted curvature bounded below and some non-Riemannian curvatures norm-bounded.",
        "corpus": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on. Authors: achieve the best HTML results from your LaTeX submissions by following these best practices. In this manuscript, we study the positive solutions of the Finslerian Fisher-KPP equation The Fisher-KPP equation is widely applied and connected to many mathematical branches. We establish the global gradient estimates on compact Finsler metric measure manifold with the traditional CD(K,N)¶·¾CD(K,N)C D ( K , N ) condition, which is developed by S. Ohta and K.-T. Sturm in Finsler geometry. Furthermore, With the assistance of a new comparison theorem developed by the first author, we also give the gradient estimate on forward complete noncompact locally finite misalignment Finsler metric measure spaces with the mixed weighted curvature bounded below and some non-Riemannian curvatures norm-bounded. MSC Classification]35K55, 53C60, 58J35 . The Fisher-KPP equation on a complete Riemannian manifold MMM is given by where uuu is a real-valued function on MÃ[0,)0M Ã [ 0 , ) and ccc is a positive constant. The equation was proposed by R. A. Fisher in 1937 to describe the propagation of an evolutionarily advantageous gene in a population, and was also independently described in a seminal paper by A. N. Kolmogorov, I. G. Petrovskii, and N. S. Piskunov in the same year; for this reason, it is often referred to in the literature as the name of Fisher“KPP equation. Since the two papers in 1937, there have been extensive investigations on traveling wave solutions and asymptotic behavior in terms of spreading speeds for various evolution systems. Traveling waves were adopted to study the nonlinear PDEs, such as the nonlinear reaction-diffusion equations modeling physical and biological phenomena (cf. [22][23]), the integral and integrodifferential population models (cf. [2][7][15][16][30]), the lattice differential systems (cf. [6][11][12][14][21][40]), and the time-delayed reaction-diffusion equations (cf. [29][38]). In 2017, Cao et al. [10] derived differential Harnack estimates for positive solutions to (1.1) on Riemannian manifolds with nonnegative Ricci curvature. The idea comes from [8][9], in which a systematic method was developed to find a Harnack inequality for geometric evolution equations. Actually, they obtained the following theorem. Theorem A. [10] Let (M,g)”(M,g)( M , g ) be an nnn-dimensional complete noncompact Riemannian manifold with nonnegative Ricci curvature, and let u(x,t):MÃ[0,)†Rnormal-:¥¡normal-†0…u(x,t):M Ru ( x , t ) : M Ã [ 0 , ) † R be a positive solution to (1.1), where uuu is C2superscript¶2C^{2}C ^ 2 in x¥xx and C1superscript¶1C^{1}C ^ 1 in t¡tt. let f=logu“™”f=loguf = l o g u, then we have for all x¥xx and t¡tt, provided that 0<Î±<10¼10< < Î± < 1 as well as cn(2+2)4(1Î±)<Î²<min¡{cn(1+Î±)4Î±24Î±+2n,cn(22)4(1Î±)}<02241¼½1¼4superscript¼24¼22241¼0 2}-4 - c n ( 2 + square-root 2 ) 4 ( 1 - Î± ) < Î² < roman_min { / - c n ( 1 + Î± ) 4 Î± ^ 2 - 4 Î± + 2 n , / - c n ( 2 - square-root 2 ) 4 ( 1 - Î± ) } < 0, where •(t)=u(e2Î¼wtvw1Î¼+w)1e2Î¼wt,italic-•¡superscript2¤¡£¤1¤1superscript2¤¡ wt}}{v-w}- wt}},• ( t ) = / u ( / e ^ 2 Î¼ w t v - w - / 1 Î¼ + w ) 1 - e ^ 2 Î¼ w t , with Î¼=Î²c2(1Î±)c(cn8Î²(1Î±)),½21¼8½1¼ c = Î² c square-root / 2 ( 1 - Î± ) c ( - c n - 8 Î² ( 1 - Î± ) ) , v=(4Î²(1Î±)n+c)‹…2(1Î±)c(cn8Î²(1Î±))£normal-‹…4½1¼21¼8½1¼v= cn-8 = ( / 4 Î² ( 1 - Î± ) n + c ) ‹… square-root / 2 ( 1 - Î± ) c ( - c n - 8 Î² ( 1 - Î± ) ) and w=2(1Î±)n.¤21¼w= = square-root / 2 ( 1 - Î± ) n . Utilizing Theorem A, one can integrate along space-time curves to get a Harnack inequality. However, it is different from the classical Li“Yau Harnack inequality [20] in form. Gradient estimates play an important role in studying elliptic and parabolic operators. The method originated first in [44] and [13], and was further developed by Li and Yau [20], Li [19], Negrin [24], P. Souplet and Q. Zhang [37], Y. Yang [43], etc.. Recent gradient estimates under the geometric flow include [4] and [3]. For more results on the nonlinear PDEs, one may refer to [1][18]. In 2018, following the line in [19], X. Geng and S. Hou [17] proved the positive solutions to the Fisher“KPP equation on complete Riemannian manifolds. They derived a gradient estimate, and got the classic Harnack inequality by using it, which extended the recent result of Cao, Liu, Pendleton and Ward [10]. In 2009, upon the foundational investigations of gradient estimation on Riemannian manifolds, S. Ohta initially put forward a sophisticated framework for gradient estimation applicable to the heat equation on compact Finsler manifolds [25]. Subsequently, in the year 2014, C. Xia [41] undertook a comprehensive analysis of harmonic functions within the context of both compact and non-compact (specifically, forward complete) Finsler structures by utilizing an advanced form of Moser™s iterative technique. Progressing further, Q. Xia [42] delivered intricate gradient estimates for positive solutions to the heat equation on forward complete Finsler spaces, employing methodologies analogous to those documented in [41]. The first author [32] proposed a method for global and local gradient estimates on Finsler metric measure spaces, which was also used to solve the Finslerian SchrÃ¶dinger equation. Later, the first author [31] gives the gradient estimates of bounded solutions to the Finslerian Allen-Cahn equation. In this paper, we study the Fisher-KPP equation (1.1) on a Finsler metric measure manifold (M,F,Î¼)¹(M,F, M , F , Î¼ ), where ccc is a positive constant, the solution of (1.1) is a function u(x)¥u(x)u ( x ) on MÃ[0,)0M Ã [ 0 , ). Our main theorems in this paper are as follows Let (M,F,Î¼)¹(M,F, M , F , Î¼ ) be a compact Finsler metric measure space with dimensional n¥22n 2n ¥ 2, whose weighted Ricci curvature satisfies RicN¥K…superscript¾Ric^{N} i c ^ N ¥ - K, for some positive constant K¾KK. Assume the reversibility of MMM has upper bound 0subscript0 _ 0 . Suppose uuu is a bounded positive smooth solution of the Fisher-KPP parabolic equation (1.1) on MÃ[0,)0M Ã [ 0 , ), then we have where, M1=supu(x,t)subscript1supremum¥¡M_{1}= u(x,t)M _ 1 = roman_sup u ( x , t ). Similarily, on a noncompact forward complete Finsler manifold, the following local gradient estimates is obtained. Let (M,F,Î¼)¹(M,F, M , F , Î¼ ) be a complete noncompact Finsler metric measure space. Denote by B(p,2R)µ2…B(p,2R)B ( p , 2 R ) the forward geodesic ball centered at ppp with forward radius 2R…RR. Suppose the mixed weighted Ricci curvature RmicN¥K(2R)superscript…superscript¾2…{}^{m}Ric^{N} m end_FLOATSUPERSCRIPT R i c ^ N ¥ - K ( 2 R ) in B(p,2R)µ2…B(p,2R)B ( p , 2 R ) with K(2R)¥0¾2…0K(2R) 0K ( 2 R ) ¥ 0, and the misalignment Î±¼ satisfies Î±¤A(2R)¼2… A(2R)Î± ¤ A ( 2 R ) in B(p,2R)µ2…B(p,2R)B ( p , 2 R ). Moreover, the non-Riemannian tensors satisfy F(U)+F*(¯)+F(divC(V))¤K0¹superscript¹¯¹£¶subscript¾0F(U)+F^{*}( K_{0}F ( U ) + F ^ * ( caligraphic_T ) + F ( d i v C ( V ) ) ¤ K _ 0 . Assume u(x,t)¥¡u(x,t)u ( x , t ) is a bounded positive smooth solution of the Fisher-KPP parabolic equation (1.1) on MÃ[0,)0M Ã [ 0 , ), then we have on B(p,R)µ…B(p,R)B ( p , R ) for 0<Îµ<1010< < Îµ < 1 , s>1 1s>1s > 1 , q>00q>0q > 0, such that 2(1Îµ)NÎµ(Nn)s1sq¥1Îµ1+(2s1)2821 1 11superscript2 128 2 ( 1 - Îµ ) N - Îµ ( N - n ) / s - 1 s q ¥ / 1 Îµ - 1 + / ( 2 s - 1 ) ^ 2 8 , where C1subscript¶1C_{1}C _ 1 , C2subscript¶2C_{2}C _ 2 are positive constants. . In this section, we introduce the fundamental principles governing Finsler metric measure spaces (refer to [32] for more details). A Finsler metric space is a triple (M,F,Î¼)¹(M,F, M , F , Î¼ ), which indicates that a differential manifold is equipped with a Finsler metric F¹FF and a measure Î¼ Suppose the local coordinate of the tangent bundle is (x,y)¥¦(x,y)( x , y ), where x¥xx is the point on MMM and y¦yy is the direction on TxMsubscript¥T_{x}MT _ x M. A Finsler metric F¹FF is a nonnegative function F:TM†[0,+):¹†0F:TM : T M † [ 0 , + ) satisfies that (i) F¹FF is smooth and positive on TM{0}0TM M { 0 }; (ii) F¹FF is a positive homogenous norm, i.e., F(x,ky)=kF(x,y)¹¥¦¹¥¦F(x,ky)=kF(x,y)F ( x , k y ) = k F ( x , y ) for any (x,y)TM¥¦ TM( x , y ) T M and for any k>00k>0k > 0; (iii) F¹FF is strongly pseudo-convex, namely, for any (x,y)TM{0}¥¦0 TM x , y ) T M { 0 }, the fundamental tensor is a positive definite matrix defined by Unlike the Riemann metric, the Finsler metric is defined locally as the norm on the tangent space at each point, and globally as a metric on the pull-back bundle, so there are a large number of non-Riemannian geometric quantities on a Finsler metric measure space. The Cartan tensor is defined by for any local vector fields X‹XX, YYY, ZZZ. There is a unique almost g-compatible and torsion-free connection on the pull-back tangent bundle *TMsuperscript‹ ^ * T M of the Finsler manifold (M,F)¹(M,F)( M , F ) called the Chern connection. It is determined by for any X,Y,ZTM{0}‹0X,Y,Z TM , Y , Z T M { 0 }, where Cysubscript¶¦C_{y}C _ y is the Cartan tensor. The Chern connection coefficients are locally denoted by Î“jki(x,y)subscriptsuperscriptÎ“¥¦ ^ i _ j k ( x , y ) in a natural coordinate system, which could induce the spray coefficients as Gi=12Î“jkiyjyksuperscriptº12subscriptsuperscriptÎ“superscript¦superscript¦G^{i}= ^ i = / 1 2 roman_Î“ ^ i _ j k y ^ j y ^ k . The spray is given by in which ÎÎxi=‚ÎxiNij‚‚yj¿¿superscript¥¿superscript¥subscriptsuperscriptsuperscript¦ x^{i}}= x^{i}}-N^{j}_{i} y^{j}}/ Î Î x ^ i = / ‚ Î x ^ i - N ^ j _ i / ‚ ‚ y ^ j and the nonlinear connection coefficients are locally induced from the spray coefficients by Nji=‚Gi‚yjsubscriptsuperscriptsuperscriptºsuperscript¦N^{i}_{j}= G^{i}}{ y^{j}}N ^ i _ j = / ‚ G ^ i ‚ y ^ j . The Chern connection can define the Chern Riemannian curvature R…RR and Chern non-Riemannian connection PƒPP. Denote by Î©Î© the curvature form of the Chern connection, so that for any X,Y,ZTM{0}‹0X,Y,Z TM , Y , Z T M { 0 }, where locally Customarily, we denote the horizontal Chern derivative by \"£\"conditional\"\"\" £ \" and the vertical Chern derivative by \";\"\"\"\";\"\" ; \". For example, for any 1-form v=vidxi£subscript£superscript¥v=v_{i}dx^{i}v = v _ i d x ^ i on the pull-back bundle. The angular metric form hysubscript¦h_{y}h _ y is defined by for any y,u,vTxM¦£subscript¥y,u,v T_{x}My , u , v T _ x M with y 0¦0y 0y 0 Thus, for any two linearly independent vectors y,vTxM{0}¦£subscript¥0y,v T_{x}M , v T _ x M { 0 }, which span a tangent plane Î y=span{y,v}subscriptÎ ¦ ¦£ _ y = s p a n { y , v }, the flag curvature with pole y¦yy is defined by which is locally expressed by The Ricci curvature is defined by where e1,¦,en1,vF(v)subscript1¦subscript1£¹£e_{1}, _ 1 , ¦ , e _ n - 1 , / v F ( v ) form an orthonormal basis of TxMsubscript¥T_{x}MT _ x M with respect to gysubscript”¦g_{y}g _ y . The Landsberg curvature of (M,F)¹(M,F)( M , F ) is given by By the zero homogeneity, according to the Euler lemma, it easy to see that H.B. Rademacher introduced the concepts of reversibility and reversible manifolds [28], which are also closely related to the analytical assumptions on Finsler manifolds. A Finsler metric is defined to be reversible if F(x,V)=F¯(x,V)¹¥¯¹¥F(x,V)= ( x , V ) = over¯ F ( x , V ) for any point x¥xx and any vector field VVV, where F¯(x,V):=F(x,V)assign¯¹¥¹¥ F ( x , V ) := F ( x , - V ) is called the reversed Finsler metric of F¹FF. Obviously, F¹FF is reversible if and only if ¡11 1 ¡ 1. A Finsler manifold (M,F)¹(M,F)( M , F ) is said to have finite reversibility if <+ < + . Later, K. Ball, E. Carlen, and E. Lieb introduced the concepts of uniform smoothness and uniform convexity in Banach space theory [5], whose geometric interpretation in Finsler geometry was provided by S. Ohta [26]. We say F¹FF satisfies uniform convexity and uniform smoothness if there exist uniform positive constants Îº*superscript… ^ * and Îº… called the uniform convexity constant and the uniform smoothness constant, respectively, such that for any xM¥x Mx M, VTxM{0}subscript¥0V T_{x}M T _ x M { 0 }, and WTxMŠsubscript¥W T_{x}MW T _ x M, we have where gV=(gij(x,V))subscript”subscript”¥g_{V}=(g_{ij}(x,V))g _ V = ( g _ i j ( x , V ) ) is the Riemannian metric on MMM induced from F¹FF with respect to the reference vector VVV . In this situation, the reversibility could be controlled by Îº… and Îº*superscript… ^ * as F¹FF is Riemannian if and only if Îº=1…1 = 1 if and only if Îº*=1superscript…1 ^ * = 1 [26]. The Riemannian structure is inconsistent when the reference vectors are different. For example, given three different local non-vanishing vector fields around x¥xx, namely, VVV, WŠWW, YYY , the norm of YYY about gVsubscript”g_{V}g _ V and gWsubscript”Šg_{W}g _ W maybe not the same in general case. The ratio gV(Y,Y)=gW(Y,Y)subscript”subscript”Šg_{V}(Y,Y)=g_{W}(Y,Y)g _ V ( Y , Y ) = g _ W ( Y , Y ) is a function about VVV, WŠWW, YYY . Based on this fact, [32] defined an important constant on a Finsler manifold, called the misalignment. ([32]). For a Finsler manifold (M,F)¹(M,F)( M , F ), the misalignment of a Finsler metric at point x¥xx is defined by Moreover, the global misalignment of the Finsler metric is defined by it also provided in [32] some characterizations of the misalignment. Especially, a Finsler manifold (M,F)¹(M,F)( M , F ) is a Riemannian manifold if and only if Î±M=1subscript¼1 _ M = 1. Moreover, a Finsler manifold (M,F)¹(M,F)( M , F ) is uniform convexity and uniform smoothness if and only if it satisfies finite misalignment. Since that, we have gaven an important class of Finsler manifold as the following. We call a Finsler manifold (M,F)¹(M,F)( M , F ) has finite misalignment if there is a positive constant AAA such that Î±¤A¼ AÎ± ¤ A, and has locally finite misalignment if for any compact subset Î©Š‚Mnormal-Î© Mroman_Î© Š‚ M, there is a constant A(Î©)normal-Î©A( ( roman_Î© ) depending on Î©normal-Î© such that Î±(x)¤A¼¥” A( ( x ) ¤ A  for any xÎ©¥normal-Î©x roman_Î©. So far, we have briefly introduced some Riemannian or non-Riemannian local quantities and tensors in Finsler geometry corresponding to Riemannian geometric quantities. Next, we will introduce more tensors related to the measure dÎ¼d Î¼. For any smooth function f:M†R:“†…f:M Rf : M † R, df denotes its differential 1-form and its gradient f“ f f is defined as the dual of the 1-form via the Legendre transformation, namely, f(x):=l1(df(x))TxMassign“¥superscript™1“¥subscript¥ f(x):=l^{-1}(df(x)) T_{x}M f ( x ) := l ^ - 1 ( d f ( x ) ) T _ x M. Locally it can be written as on Mf:=df 0assignsubscript““0M_{f}:={df 0}M _ f := d f 0. The Hessian of f“ff is defined via the Chern connection by It can be shown that 2f(X,Y)superscript2“‹ ^ 2 f ( X , Y ) is symmetric [27]. For any two points ppp, qqq on MMM, the distance function is defined by where the infimum is taken over all the C1superscript¶1C^{1}C ^ 1 curves Î³:[0,1]†M:¾†01 MÎ³ : [ 0 , 1 ] † M such that Î³(0)=p¾0 ( 0 ) = p and Î³(1)=q¾1 ( 1 ) = q. Fixed a base point ppp on MMM, we denote the forward distance function by rrr. That is, r(x)=d(p,x)¥¥r(x)=d(p,x)r ( x ) = d ( p , x ), with d denotes the forward distance. The forward distance function rrr is a function defined on the Finsler manifold MMM. drdrd r is a 1-form on MMM, whose dual is a gradient vector field, noted by r r r. Precisely, r=gij(x,r)‚r‚xi‚‚xjsuperscript”¥superscript¥superscript¥ r=g^{ij}(x, r) r}{ x^{i}} x^{j}} r = g ^ i j ( x , r ) / ‚ r ‚ x ^ i / ‚ ‚ x ^ j Taking the Chern horizontal derivative of r r r yields the Hessian of distance function 2rsuperscript2 ^ 2 r. Locally, in a natural coordinate system In (2.23), the derivative is taken in the direction r r r naturally. Generally, we can take the derivative in any direction. Suppose VVV is a local vector field around x¥xx on MMM. Note that the distance function may not be symmetric about ppp and qqq unless F¹FF is reversible. AC2superscript¶2AC^{2}A C ^ 2 curve Î³¾ is called a geodesic if locally where Gi(x,y)superscriptº¥¦G^{i}(x,y)G ^ i ( x , y ) are the spray coefficients. A forward geodesic ball centered at ppp with radius R…RR can be represented by Adopting the exponential map, a Finsler manifold (M,F)¹(M,F)( M , F ) is said to be forward complete or forward geodesically complete if the exponential map is defined on the entire TMTMT M. Thus, any two points in a forward complete manifold MMM can be connected by a minimal forward geodesic. Moreover, the forward closed balls BR+(p)¯¯subscriptsuperscriptµ… B ^ + _ R ( p ) are compact. A Finsler metric measure space (M,F,dÎ¼)¹(M,F,d M , F , d Î¼ ) is a Finsler manifold equipped with a given measure Î¼ In local coordinates {xi}i=1nsuperscriptsubscriptsuperscript¥1 x ^ i } _ i = 1 ^ n , we can express the volume form as dÎ¼=ƒ(x)dx1¦dxn¥superscript¥1¦superscript¥d dx^{n}d Î¼ = ƒ ( x ) d x ^ 1 ¦ d x ^ n . For any yTxM{0}¦subscript¥0y T_{x}M T _ x M { 0 }, define which is called the distortion of (M,F,dÎ¼)¹(M,F,d M , F , d Î¼ ). The definition of the S-curvature is given in the following. ([35][34]). Suppose (M,F,dÎ¼)¹(M,F,d M , F , d Î¼ ) is a Finsler metric measure space. For any point xM¥x Mx M, let Î³=Î³(t)¾¾¡ = Î³ ( t ) be a forward geodesic from x with the initial tangent vector Î³Ë™(0)=ynormal-Ë™¾0¦ Î³ ( 0 ) = y. The S-curvature of (M,F,dÎ¼)¹(M,F,d M , F , d Î¼ ) is Definition 2.3 means that the S-curvature is the changing of distortion along the geodesic in direction y. Modeled on the definition of ¯¯ curvature in [36], [32] defined the difference of normal- on the tangent sphere, denoted by ¯¯ ([32]). The difference of normal- on the tangent bundle is a tensor denoted by ¯¯ which is given by for vector fields VVV,WŠWW on MMM. Locally, it is ¯(V,W)=¯i(V,w)dxi¯Šsubscript¯¤superscript¥ ( V , W ) = caligraphic_T _ i ( V , w ) d x ^ i , with Obviously, ¯(V,W)¯Š ( V , W ) is anti-symmetric about VVV and WŠWW, that is, ¯(V,W)=¯(W,V)¯Š¯Š ( V , W ) = - caligraphic_T ( W , V ) for any nonvanishing VVV and WŠWW. If in local coordinates {xi}i=1nsubscriptsuperscriptsuperscript¥1 x ^ i } ^ n _ i = 1 , expressing dÎ¼=e•dx1¦dxnsuperscriptitalic-•superscript¥1¦superscript¥d dx^{n}d Î¼ = e ^ • d x ^ 1 ¦ d x ^ n , the divergence of a smooth vector field VVV can be written as The Finsler Laplacian of a function f“ff on MMM could now be given by Î”dÎ¼f:=divdÎ¼(f)assignsubscriptÎ”“subscript£“ f)roman_Î” _ d Î¼ f := d i v _ d Î¼ ( f ), noticing that Î”dÎ¼=Î”ffsubscriptÎ”superscriptÎ”““ f}froman_Î” _ d Î¼ = roman_Î” ^ f f, where Î”dÎ¼ff:=divdÎ¼(ff)assignsubscriptsuperscriptÎ”““subscript£superscript““ f}_{d f}f)roman_Î” ^ f _ d Î¼ f := d i v _ d Î¼ ( ^ f f ) is in the view of weighted Laplacian with A Finsler Laplacian is better to be viewed in a weak sense due to the lack of regularity. Concretely, assuming fW1,p(M)“superscriptŠ1f W^{1,p}(M)f W ^ 1 , p ( M ), for any test function •C0(M)italic-•superscriptsubscript¶0 C_{0}^{ C _ 0 ^ ( M ). On the other hand, the Laplacian of a function f“ff on a Riemannian manifold is the trace of the Hessian of f“ff with respect to the Riemannian metric g”gg. On a Finsler metric measure space (M,F,dÎ¼)¹(M,F,d M , F , d Î¼ ), the weighted Hessian H~(f)~»“ H ( f ) of a function f“ff on Mf={xM:f|x 0}subscript“conditional-set¥evaluated-at“¥0M_{f}= M: f|_{x} 0 _ f = { x M : f | _ x 0 }is defined in [39] by where hfsubscript“h_{ f}h _ f is the angular metric form in the direction f“ f f, given in (2.9) It is clear that H~(f)~»“ H ( f ) is still a symmetric bilinear form with Inspired by this, [32] defined the mixed weighted Hessian where hV,fsubscript“h_{V, f}h _ V , f is the mixed angular metric form in the directions VVV and f“ f f, which is defined by for any vector X‹XX, YYY. It is necessary to remark that hf,f=hfsubscript““subscript“h_{ f, f}=h_{ f}h _ f , f = h _ f , so that H~f(f)=H~(f)superscript~»““~»“ f}(f)= H ^ f ( f ) = over~ H ( f ) for any function f“ff on MMM. With the assistance of the S-curvature, one can present the definition of the weighted Ricci curvature as the following. ([27][34]). Given a unit vector VTxMsubscript¥V T_{x}MV T _ x M and an positive integral number kkk, the weighted Ricci curvature is defined by where the derivative is taken along the geodesic started from x¥xx in the direction of VVV. According to the definition of weighted Ricci curvature, B. Wu defined the weighted flag curvature when k=N(1,n)ª(n,)1k=N = N ( 1 , n ) ª ( n , ) in [39]. We have completely introduced this concept for any kkk in [32]. ([32]). Let (M,F,dÎ¼)¹(M,F,d M , F , d Î¼ ) be a Finsler metric measure space, and V,WTxMŠsubscript¥V,W T_{x}MV , W T _ x M be linearly independent vectors. The weighted flag curvature Kk(V,W)superscript¾ŠK^{k}(V,W)K ^ k ( V , W ) is defined by where the derivative is taken along the geodesic started from x¥xx in the direction of VVV. Moreover, it has also been defined the mixed weighted Ricci curvature in [32], denoted by Rmicksuperscript…superscript{}^{m}Ric^{k}start_FLOATSUPERSCRIPT m end_FLOATSUPERSCRIPT R i c ^ k . ([32])Given two unit vectors V,WTxMŠsubscript¥V,W T_{x}MV , W T _ x M and an positive integral number kkk, the mixed weighted Ricci curvature Rmick(V,W)=mRicWk(V)superscriptsuperscript…superscriptŠ…subscriptsuperscriptŠ{}^{m}Ric^{k}(V,W)=^{m}Ric^{k}_{W}(V)start_FLOATSUPERSCRIPT m end_FLOATSUPERSCRIPT R i c ^ k ( V , W ) = ^ m R i c ^ k _ W ( V )is defined by where the derivative is taken along the geodesic started from x¥xx in the direction of VVV, and trWRV(V)=gij(W)gV(RV(ei,V)V,ej)¡subscriptŠsubscript…superscript”Šsubscript”subscript…subscriptsubscripttr_{W}R_{V}(V)=g^{ij}(W)g_{V}(R_{V}(e_{i},V)V,e_{j})t r _ W R _ V ( V ) = g ^ i j ( W ) g _ V ( R _ V ( e _ i , V ) V , e _ j ) means taking trace of the flag curvature with respect to g(x,W)”¥Šg(x,W)g ( x , W ). The weighted Ricci curvature is a special case of the mixed weighted Ricci curvature, i.e., Rick(V)=mRicVk(V)superscript…superscript…subscriptsuperscriptRic^{k}(V)=^{m}Ric^{k}_{V}(V)R i c ^ k ( V ) = ^ m R i c ^ k _ V ( V ). Defining the function ctc(r)subscript¡ct_{c}(r)c t _ c ( r ) as the following weighted Hessian comparison theorem is cited from Theorem 3.3 in [39]. ([32]). Let (M,F,dÎ¼)¹(M,F,d M , F , d Î¼ ) be a forward complete n-dimensional Finsler metric measure space with finite misalignment Î±¼ Denote the forward distance function by rrr and by VVV a fixed vector field on MMM. Suppose the mixed weighted Ricci curvature RicN(V,r)…superscriptnormal-Ric^{N}(V, r)R i c ^ N ( V , r ) of MMM is bounded from below by K¾-K- K with K>0¾0K>0K > 0, for some N>nN>nN > n, as well as the non-Riemannian curvatures UUU, ¯¯ and divC(V)=Ck|iij(V)VkÎÎxj£¶subscriptsuperscript¶conditionalsuperscript¿¿superscript¥divC(V)=C^{ij}_{k|i}(V)V^{k} x^{j}}d i v C ( V ) = C ^ i j _ k | i ( V ) V ^ k / Î Î x ^ j satisfy the norm bounds by F(U)+F(¯)+F*(divC(V))¤K0¹¹¯superscript¹£¶subscript¾0F(U)+F( K_{0}F ( U ) + F ( caligraphic_T ) + F ^ * ( d i v C ( V ) ) ¤ K _ 0 . Then, by setting l=K/C(N,Î±)™¾¶¼l=K/C(N, = K / C ( N , Î± ) with C(N,Î±)=N+(Î±1)nÎ±¶¼¼1¼C(N, ( N , Î± ) = N + ( Î± - 1 ) n - Î±, wherever rrr is C2superscript¶2C^{2}C ^ 2 , the nonlinear Laplacian of rrr with reference vector VVV satisfies . In this section, we will focus on the global gradient estimates of positive solutions to the Fisher-KPP equation (1.1) on compact Finsler manifolds, with the weighted Ricci curvature bounded from below. This condition of curvature is extensively employed within the realm of Finsler geometric analysis. Suppose that uuu is a positive solution on MÃ[0,)0M Ã [ 0 , ) to the Fisher-KPP equation(1.1) . We define the function W(x,t)=uqŠ¥¡superscriptW(x,t)=u^{-q}W ( x , t ) = u ^ - q , where qqq is a positive constant to be fixed later. We now have the following lemma. Let uuu be a positive solution on MÃ[0,)0M Ã [ 0 , ) to the Fisher-KPP equation (1.1) and W(x,t)=uqŠ¥¡superscriptW(x,t)=u^{-q}W ( x , t ) = u ^ - q , then WŠWW satisfies that The gradient of WŠWW with respect to u u u is the dual of dWŠWW by the pull-back metric gusubscript”g_{ u}g _ u . Precisely, W=quq1uŠsuperscript1 W=-qu^{-q-1} u W = - q u ^ - q - 1 u on MMM and Wt=quq1utsubscriptŠ¡superscript1subscript¡W_{t}=-qu^{-q-1}u_{t}W _ t = - q u ^ - q - 1 u _ t , so we have Then we deduce that in the distributional sense on MusubscriptM_{u}M _ u , wherever u 00 u 0 u 0 Combine with (1.1) and (3.2), we have After the transformation, we obtain the Lemma 3.1. Now, we follow the line in [19]. Define three functions as the follow. where aaa, Î²½ are two positive constants to be determined later. Direct calculations provide that and on MusubscriptM_{u}M _ u , where we have employed the Ricci-type identity in [33]. On the other hand, it satisfies that and Plugging (3.10) and (3.11) into (3.9) yields wherever u 00 u 0 u 0. By a basic computation of Finsler geometry, we could get where we have already employed the fact that C(u,‹…,‹…)=0¶‹…‹…0C( u, ( u , ‹… , ‹… ) = 0 and C2uu(uH0)=u|ikCkij(u)HjC_{ u}( u}H_{0})=u_{|i}^{k}C_{k}^{ij}( u% )H_{j}C _ ^ 2 u ^ u ( ^ u H _ 0 ) = u _ | i ^ k C _ k ^ i j ( u ) H _ j . The notation |||| denotes the horizontal derivative with respect to the Chern connection in the direction u u u. It follows from that (3.13) is equal to Combining it with (3.12), we could find that by noticing that Therefore, H0(x,t)subscript»0¥¡H_{0}(x,t)H _ 0 ( x , t ) satisfies that Because of Wt=(W)tsubscriptŠ¡subscriptŠ¡ W_{t}=( W)_{t} W _ t = ( W ) _ t , it follows for uH1superscriptsubscript»1 u}H_{1} ^ u H _ 1 that It asserts by direct caiculation that arccording to [32]. Thus, for H1subscript»1H_{1}H _ 1 , one may find that so that Moreover, one could calculate that as well as Plugging (3.22)-(3.24) into (3.16) and (3.17) yields It asserts from the hÃ¶lder inequality that Thus, when 0<Îµ<1010< < Îµ < 1, it satisfies that According to the equality (a+b)2n=a2Nb2Nn+N(Nn)n(aN+bNb)2superscript2superscript2superscript2superscript2 }{N}+ ( a + b ) ^ 2 n = / a ^ 2 N - / b ^ 2 N - n + / N ( N - n ) n ( / a N + / b N - b ) ^ 2 , for any N>nN>nN > n, the following inequality holds by substituting aaa, bbb and NNN by Î”uWsuperscriptÎ”Š u}Wroman_Î” ^ u W, SË™(uW)Ë™†superscriptŠ u}W)overË™ S ( ^ u W ) and NÎµ(Nn)N- - Îµ ( N - n ), respectively. Namely, so that Moreover, it™s easy to infer that It is deduced from (3.25) by combining (3.27)-(3.30) and employing the definition of weighted Ricci curvature that on MusubscriptM_{u}M _ u . Let Î²=aq½ = / a q , combining with (3.21), we have Noticing Î”uWW=qaH+(q+1qqa)Fu2(uW)W2superscriptÎ”ŠŠ»1subscriptsuperscript¹2superscriptŠsuperscriptŠ2 u}W}{W}= ^{2}_{ u}( u}W)}{W^{2}}/ roman_Î” ^ u W W = / q a H + ( / q + 1 q - / q a ) / F ^ 2 _ u ( ^ u W ) W ^ 2 , and setting a=sq2 superscript2a=sq^{2}a = s q ^ 2 , we get that Plugging it into (3.32), we arrive at the following lemma. Let (M,F,Î¼)¹(M,F, M , F , Î¼ ) be a forward complete Finsler metric measure space, and denote Mu={xM£u(x) 0}subscriptconditional-set¥normal-¥0M_{u}= M u(x) 0 _ u = { x M £ u ( x ) 0 }. For H=Fu2(uW)W2+ac(1W1q)+Î²WtW»subscriptsuperscript¹2normal-superscriptnormal-normal-ŠsuperscriptŠ21superscriptŠ1½subscriptŠ¡ŠH= u}( u}W)}{W^{2}}+ac(1-W^{- = / F ^ 2 _ u ( ^ u W ) W ^ 2 + a c ( 1 - W ^ - / 1 q ) + Î² / W _ t W with a=sq2 superscript2a=sq^{2}a = s q ^ 2 , Î²=aq½ = / a q , it satisfies on MusubscriptM_{u}M _ u that Employing Lemma 3.2, the following global gradient estimate theorem could be obtained by utilizing the maximum principle argument, which was first adopted in [27] and was applied also in [32][31]. Let (M,F,Î¼)¹(M,F, M , F , Î¼ ) be a compact Finsler metric measure manifold whose weighted Ricci curvature satisfies RicN¥K…superscript¾Ric^{N} i c ^ N ¥ - K, for some positive constant K¾KK. Assume the bound of the reversibility on MMM is 0subscript0 _ 0 . Suppose u(x,t)¥¡u(x,t)u ( x , t ) is a bounded positive smooth solution of the Fisher-KPP parabolic equation (1.1) on MÃ[0,)0M Ã [ 0 , ), then we have where, 0<Îµ<1010< < Îµ < 1, s>1 1s>1s > 1, q>00q>0q > 0, such that 2(1Îµ)NÎµ(Nn)s1sq¥1Îµ1+(2s1)2821 1 11superscript2 128 2 ( 1 - Îµ ) N - Îµ ( N - n ) / s - 1 s q ¥ / 1 Îµ - 1 + / ( 2 s - 1 ) ^ 2 8 , and M1=supu(x,t)subscript1supremum¥¡M_{1}= u(x,t)M _ 1 = roman_sup u ( x , t ). Lemma 3.34 implies that (3.34) holds in the distributional sense on MMM. For any nonnegative test function † we have where Î²½ denotes the RHS of (3.34). Assume y0=(x0,t0)B(p,2R)Ã[0,)subscript¦0subscript¥0subscript¡0µ2…0y_{0}=(x_{0},t_{0}) B(p,2R) _ 0 = ( x _ 0 , t _ 0 ) B ( p , 2 R ) Ã [ 0 , ) be the point where H»HH achieves its maximum. Without loss of generality, we could assume H(x0,t0)¥0»subscript¥0subscript¡00H(x_{0},t_{0}) 0H ( x _ 0 , t _ 0 ) ¥ 0, otherwise the result will be satisfied trivially. We claim that Î²(x0,t0)¤0½subscript¥0subscript¡00 0Î² ( x _ 0 , t _ 0 ) ¤ 0. Otherwise, Î²½ is strictly positive at (x0,t0)subscript¥0subscript¡0(x_{0},t_{0})( x _ 0 , t _ 0 ), so that Î²(x0,t0)½subscript¥0subscript¡0 ( x _ 0 , t _ 0 ) is positive in a small neighborhood of (x0,t0)subscript¥0subscript¡0(x_{0},t_{0})( x _ 0 , t _ 0 ) on MMM, which may be denoted by UUU. Chosen a test function † whose compact support VVV is contained in UUU, we know from (3.36) that H»HH is a weak, local subharmonic function in a neighborhood VŠ‚UV UV Š‚ U. It is a contradiction because (x0,t0)subscript¥0subscript¡0(x_{0},t_{0})( x _ 0 , t _ 0 ) is a inner point of VVV. Because of Î²(x0,t0)¤0½subscript¥0subscript¡00 0Î² ( x _ 0 , t _ 0 ) ¤ 0 and uH(y0)=0superscript»subscript¦00 u}H(y_{0})=0 ^ u H ( y _ 0 ) = 0, we arrive at It follows from the hÃ¶lder inequality that and where M1=supu(x)subscript1supremum¥M_{1}= u(x)M _ 1 = roman_sup u ( x ). Now, let q>00q>0q > 0 such that 2(1Îµ)NÎµ(Nn)s1sq¥1Îµ1+(2s1)2821 1 11superscript2 128 2 ( 1 - Îµ ) N - Îµ ( N - n ) / s - 1 s q ¥ / 1 Îµ - 1 + / ( 2 s - 1 ) ^ 2 8 . Combining with (3.37), we get Hence we achieve the global gradient estimates on compact Finsler manifolds. Theorem 1.1 follows by taking Îµ=1212 = / 1 2 , s=2 2s=2s = 2, q=227(N+n)227q= = / 2 27 ( N + n ) in Theorem 3.1 . In this section, we prove the local gradient estimates on forward complete Finsler metric measure spaces with the assistance of Lemma 3.34 and the Comparison theorem (cf. Theorem 2.1). Let (M,F,Î¼)¹(M,F, M , F , Î¼ ) be a complete noncompact Finsler metric measure space. Denote by B(p,2R)µ2…B(p,2R)B ( p , 2 R ) the forward geodesic ball centered at ppp with forward radius 2R2…2R2 R. Suppose the mixed weighted Ricci curvature RmicN¥K(2R)superscript…superscript¾2…{}^{m}Ric^{N} m end_FLOATSUPERSCRIPT R i c ^ N ¥ - K ( 2 R ) in B(p,2R)µ2…B(p,2R)B ( p , 2 R ) with K(2R)¥0¾2…0K(2R) 0K ( 2 R ) ¥ 0, and the misalignment Î±¼ satisfies Î±¤A(2R)¼2… A(2R)Î± ¤ A ( 2 R ) in B(p,2R)µ2…B(p,2R)B ( p , 2 R ). Moreover, the non-Riemannian tensors satisfy F(U)+F*(¯)+F(divC(V))¤K0¹superscript¹¯¹£¶subscript¾0F(U)+F^{*}( K_{0}F ( U ) + F ^ * ( caligraphic_T ) + F ( d i v C ( V ) ) ¤ K _ 0 . Assume u(x,t)¥¡u(x,t)u ( x , t ) is a bounded positive smooth solution of the Fisher-KPP parabolic equation (1.1) on MÃ[0,)0M Ã [ 0 , ), then we have on B(p,R)µ…B(p,R)B ( p , R ) for 0<Îµ<1010< < Îµ < 1, s>1 1s>1s > 1, q>00q>0q > 0, such that 2(1Îµ)NÎµ(Nn)s1sq¥1Îµ1+(2s1)2821 1 11superscript2 128 2 ( 1 - Îµ ) N - Îµ ( N - n ) / s - 1 s q ¥ / 1 Îµ - 1 + / ( 2 s - 1 ) ^ 2 8 , where C1subscript¶1C_{1}C _ 1 , C2subscript¶2C_{2}C _ 2 are positive constants. In noncompact situation, we choose the cut-off function †~C2[0,+)~superscript¶20 C^{2}[0,+ † C ^ 2 [ 0 , + ), such that So †~(r)[0,1]~01 † ( r ) [ 0 , 1 ]. In addition, we supposed that where C1,C2subscript¶1subscript¶2C_{1},C_{2}C _ 1 , C _ 2 are positive constants. For a fixed point ppp, denote by r(x)¥r(x)r ( x ) the forward distance function from ppp to any point x¥xx. We define the cut-off function by †(x)=†~(r(x)R)¥~¥… ( x ) = over~ † ( / r ( x ) R ). So that According to the curvature conditions and the Laplacian comparison theorem on forward complete Finsler manifolds, it satisfies the Laplacian estimation [32] that where C3=C3(N,A,K0)subscript¶3subscript¶3subscript¾0C_{3}=C_{3}(N,A,K_{0})C _ 3 = C _ 3 ( N , A , K _ 0 ) is a constant depending on K0subscript¾0K_{0}K _ 0 , NNN and AAA. Defining Z=tH(x,t)¡»¥¡Z=tH(x,t)Z = t H ( x , t ), we suppose that the support of function †(x)Z(x,t)¥¥¡ ( x ) Z ( x , t ) is contained in Bp(2R)subscriptµ2…B_{p}(2R)B _ p ( 2 R ). For any fixed T>00T>0T > 0, let (x0,t0)subscript¥0subscript¡0(x_{0},t_{0})( x _ 0 , t _ 0 ) be the point where †(x)Z(x,t)¥¥¡ ( x ) Z ( x , t ) achieves its positive maximum, at which it satisfies that (4.6) implies that and at the maximum point (x0,t0)subscript¥0subscript¡0(x_{0},t_{0})( x _ 0 , t _ 0 ). Let Î²=aq½ = / a q , a=sq2 superscript2a=sq^{2}a = s q ^ 2 , employing Lemma 3.34, we have It follows from the hÃ¶lder inequality that and Substituting (4.10) and (4.11) into (4.9), and choosing s>1 1s>1s > 1, q>00q>0q > 0, such that 2(1Îµ)ns1sq¥1Îµ1+(2s1)2821 1 11superscript2 128 1)^{2}}{8}/ 2 ( 1 - Îµ ) n / s - 1 s q ¥ / 1 Îµ - 1 + / ( 2 s - 1 ) ^ 2 8 , one may find that Substituting (4.12) into (4.8) and noticing (4.7), we have It™s easy to know that Multiplying through by t†¡t † at (4.13) and utilizing (4.14), we get that which implies that is satisfied at (x0,t0)subscript¥0subscript¡0(x_{0},t_{0})( x _ 0 , t _ 0 ). Clearly, corresponding to the assumption of (x0,t0)subscript¥0subscript¡0(x_{0},t_{0})( x _ 0 , t _ 0 ), one can deduce that Recall a result that, if a1,a2,a3¥0subscript1subscript2subscript30a_{1},a_{2},a_{3} 0a _ 1 , a _ 2 , a _ 3 ¥ 0 then a1a2+a3¤a1a2+a1a3subscript1subscript2subscript3subscript1subscript2subscript1subscript3 a _ 1 square-root a _ 2 + a _ 3 ¤ square-root a _ 1 a _ 2 + square-root a _ 1 a _ 3 . Thus, on B(p,2R)µ2…B(p,2R)B ( p , 2 R ), which is the desired inequality. Theorem 1.2 follows from Theorem 4.1 directly.",
        "keywords": "keywords: gradient estimate, Fisher-KPP equation, Finsler metric measure space, weighted Ricci curvature, mixed weighted Ricci curvature"
    },
    {
        "id": 6,
        "title": "Radical Pair Mechanism and the Role of Chirality-Induced Spin Selectivity during Planaria Regeneration: Effect of Weak Magnetic Field on ROS levels",
        "abstract": "AbstractPlanarian is an intriguing model system wherein the effect of electric and magnetic fields can be studied on various biochemical pathways during cell morphogenesis. Recent experimental observations have demonstrated the non-trivial modulation of reactive oxygen species (ROS) levels by weak magnetic field during planaria regeneration. However, the underlying biophysical mechanism behind this remains elusive. In this paper, we study the radical pair mechanism to explain the effect of weak magnetic fields on ROS modulation during planaria regeneration to explain the experimental results. We also investigate the effect of chirality-induced spin selectivity (CISS) on ROS levels by including it in the framework of the radical pair mechanism. We conclude that the inclusion of CISS not only explains the experimental results better but also allows for the radical pair model to have more parametric space to satisfy the experimental constraints. This study explains the crucial process of ROS modulation by the weak magnetic field with and without CISS thereby paving the way to unravel the vast domain of ROS modulation for desired outcomes.",
        "corpus": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on. Authors: achieve the best HTML results from your LaTeX submissions by following these best practices. Planarian is an intriguing model system wherein the effect of electric and magnetic fields can be studied on various biochemical pathways during cell morphogenesis. Recent experimental observations have demonstrated the non-trivial modulation of reactive oxygen species (ROS) levels by weak magnetic field during planaria regeneration. However, the underlying biophysical mechanism behind this remains elusive. In this paper, we study the radical pair mechanism to explain the effect of weak magnetic fields on ROS modulation during planaria regeneration to explain the experimental results. We also investigate the effect of chirality-induced spin selectivity (CISS) on ROS levels by including it in the framework of the radical pair mechanism. We conclude that the inclusion of CISS not only explains the experimental results better but also allows for the radical pair model to have more parametric space to satisfy the experimental constraints. This study explains the crucial process of ROS modulation by the weak magnetic field with and without CISS thereby paving the way to unravel the vast domain of ROS modulation for desired outcomes. Several research studies emphasize the significant impact of electromagnetic fields on diverse biological processes and systems [1, 2, 3]. Morphogenesis, a biological process, is affected by electromagnetic fields, with planarians serving as a notable model for comprehending morphogenesis and cell communication. Planarians, recognized for their exceptional regenerative abilities, are often studied in this context [4, 5, 6, 7, 8, 9]. Planaria™s impressive regenerative ability is due to the presence of a set of robust adult stem cells known as neoblasts. Once injured, these cells migrate to the injured site and form a specialized structure known as a blastema. The blastema subsequently undergoes multiplication and transformation, regenerating the lost body parts like the head, trunk, or tail [10, 11, 12, 13, 14]. The formation of the blastema is influenced by specific signaling molecules known as reactive oxygen species (ROS). Blocking and activation of these molecules contribute to the change in the formation of blastema. Consequently, this impacts the growth of new tissues and affects the process of planarian regeneration [15, 16, 17]. Recent insights into how living organisms interact with electromagnetic radiation indicate the possibility of discovering novel techniques for controlling ROS within the body using weak magnetic fields (WMFs) [17]. However, the exact biophysical mechanism behind the effect of weak magnetic fields on ROS levels is not fully known. Radical pair mechanism has been proposed to elucidate the impact of weak magnetic fields on modulation of ROS levels [18, 19, 20, 21, 22, 23, 24, 25]. More generally, the radical pair mechanism (RPM) has emerged as a prominent theory explaining the effect of magnetic field on various biological and chemical systems [23, 26, 27, 28, 29] In particular, it has been studied in detail for the cryptochrome protein present in bird™s retina and is considered a potential explanation for the avian magnetoreception [30, 31, 32, 33, 34]. Moreover, it has recently been reported that CISS might play a role in conjunction with radical pairs in birds for the navigational compass [35, 36, 37]. The radical pair mechanism involves electron transport steps during the formation and recombination [32, 38, 39]. This involves the movement of electrons through protein molecules. Owing to the chirality of protein molecules, the chiral-induced spin selectivity (CISS) effect could play an essential role in the electron transport part of the reaction. Although the exact role of CISS in the avian magnetoreception has been debated, there are strong evidence in support of its presence in various biochemical reactions involving electron transfer or rearrangement in chiral molecules [40, 41, 42, 43, 44]. The origin of CISS is attributed to the spin-orbit interaction and the electrostatic potential provided by the chiral molecules [45, 46, 47, 48, 49, 50, 51, 52]. In this work, we examine the radical pair mechanism in light of recent experimental work pertaining to ROS level modulation by the weak magnetic field [53]. Furthermore, we also investigate the effect of chirality-induced spin selectivity (CISS) in the modulation of Reactive Oxygen Species (ROS) in the planarian system. To achieve this, we establish a theoretical model of the CISS-assisted radical pair mechanism, aligning our simulated results with experimental data [17]. Our investigation of the CISS-assisted radical pair mechanism is structured around three key considerations: 1) the absolute yield of the reaction, 2) the ratio of yield values at the experimental point of interest, and 3) the number of nuclei configurations that adhere to the experimental trend. Additionally, we explore two scenarios: 1) CISS presence exclusively during the formation of radicals and 2) CISS presence during both the formation and recombination of radical pairs. The manuscript has been organized as follows: Section II discusses the simulation methodology followed for analysis. Section III discusses the results, where subsection III.1 discusses the RP model with no CISS subsection III.2 discusses the RP model with CISS, subsection III.3 explores the impact of singlet and triplet recombination rate on planaria regeneration and III.4 discuss a system with higher number of nuclei. Finally, we discuss the shortcomings and conclusions of the study. In the radical pair model of the planaria regeneration, an electron is excited in the acceptor molecule, creating a vacancy in the ground state. Another electron from a neighboring donor molecule travels in the chiral medium to fill this vacancy. It results in the formation of a radical pair. The spin operator of the electron on the donor molecule is S^Dzsubscript^†·§ S _ D z and on the acceptor molecule is S^Azsubscript^†§ S _ A z . Therefore, the spin state of the above-formed radical pair is governed by the following Hamiltonian [54, 31, 37] where =gÎ¼B¯B¯””¯subscriptµ¯µ = g over¯ Î¼ _ B over¯ B , B¯=B0z¯¯µsubscriptµ0¯§ B = B _ 0 over¯ z where B0subscriptµ0B_{0}B _ 0 corresponds to the applied magnetic field. J½JJ and D·DD are the exchange and dipolar interactions. AAA is the hyperfine tensor depicting interactions between electrons and neighboring nuclear spins. The spin state of the radical pair evolves under the zeeman and hyperfine interactions. Along with this evolution, the radical pair also recombines back to the singlet yield and triplet yield shown in Fig. 1. The singlet yield corresponds to the ground state while the triplet state corresponds to reactive oxygen species(ROS) concentrationFig. 1). The radical pair formation begins with the donor molecule (D) and the acceptor molecule (A) in their ground states. The reaction initiates with the excitation of the acceptor molecule, causing an electron to transition to a higher energy state and leaving a vacancy in the ground state of the acceptor molecule. Electron transfer occurs from the ground state of the donor molecule to fill the vacancy in the ground state of the acceptor molecule. This leads to the formation of the radical pair on donor ion D.+D^{.+}D ^ . + and acceptor ion A.A^{.-}A ^ . - . Fig,2 illustrates the formation of the radical pair in a chiral medium where the spin state of the electron |†“©ket†“ †“ © is allowed in the forward propagation of the electron. In a non-chiral case, both |†“©ket†“ †“ © and |†©ket† † © will be allowed in the formation of the radical pair. The recombination of the radical pair involves the transition from the donor ion D.+D^{.+}D ^ . + and acceptor ion A.A^{.-}A ^ . - to their respective ground states, as well as the generation of regenerative oxygen species. This process occurs in three stages. Stage 1 is the initial stage where the radical pair is formed. The chirality of the medium allows only the |†“©ket†“ †“ © state to move from the donor ion D.+D^{.+}D ^ . + to the acceptor ion A.A^{.-}A ^ . - as illustrated in Fig.2. In Stage 2, the system exists in a superposition of singlet and triplet states. The spins of the isolated electrons on D.+D^{.+}D ^ . + and A.A^{.-}A ^ . - change due to their interaction with the nuclei (hyperfine interaction). This stage is affected by the system Hamiltonian (Eq.1). In stage 3, because the forward movement allowed the |†“©ket†“ †“ © state, the |†©ket† † © state will move in the opposite (backward) direction for recombination. This recombination yield leads to the formation of the respective singlet and triplet yield. The triplet yield would contribute towards the ROS signaling, whereas the singlet yield would contribute towards the ground state of the donor and acceptor molecule. In Fig.3, we have illustrated it for a fully chiral medium and how chirality affects the formation of the yield. Due to chirality, since only |†©ket† † © is allowed, we observe that yield formation of the yield of triplet state |†“†“©ket†“absent†“ †“ †“ © is inhibited. The CISS effect plays a role in forming (Fig.2) and recombining (Fig.3) the radical pair as it involves electron transport through the chiral protein molecule. Therefore, the action of CISS is captured by the initial state PIsubscriptƒ¼P_{I}P _ I and recombination state PSsubscriptƒ†P_{S}P _ S and PTsubscriptƒP_{T}P _ T , shown with red arrows in Fig. 1. Then the initial state density matrix is given as: PInitial=PInŠIZsubscriptƒ¼¡™tensor-productsubscriptƒ¼¼P_{Initial}=P_{In} _ I n i t i a l = P _ I n Š / I Z , where IZ¼ I Z corresponds to the mixed state of nuclei, and ZZZ is the size of the nuclear Hilbert space. The singlet recombination operator PS=|S©¨S|subscriptƒ†ketsubscript“†brasubscript“†P_{S}={ _ S = | _ S © ¨ _ S | accounts for recombination to the ground state where: The triplet recombination operator PT=PT++PT+PT0subscriptƒsubscriptƒlimit-fromsubscriptƒlimit-fromsubscriptƒ0P_{T}=P_{T+}+P_{T-}+P_{T0}P _ T = P _ T + + P _ T - + P _ T 0 accounts for recombination to the reactive oxygen species concentration. PT+subscriptƒlimit-fromP_{T+}P _ T + correspond to triplet state when net magnetic moment mS=1subscript†1m_{S}=1m _ S = 1. PTsubscriptƒlimit-fromP_{T-}P _ T - correspond to triplet state when net magnetic moment mS=1subscript†1m_{S}=-1m _ S = - 1 and PT0subscriptƒ0P_{T0}P _ T 0 correspond to triplet state when net magnetic moment mS=0subscript†0m_{S}=0m _ S = 0 The CISS parameter [0,2]0‹2 [ 0 , / 2 ] depends on the spin selectivity of the protein medium; =00 = 0 corresponding to no CISS and =/2‹2 = / 2 corresponding to the full CISS. The master equation governing the state evolution of the system is given as: Where kSsubscript†k_{S}k _ S is the singlet recombination rate, and kTsubscriptk_{T}k _ T is the triplet recombination rate. [A,B]=ABBAµµµ[A,B]=AB-BA[ A , B ] = A B - B A correspond to the commutator whereas, {A,B}=AB+BAµµµ A , B } = A B + B A is the anti-commutator. As the literature states, the triplet yield is directly related to reactive oxygen species (ROS) concentration [17, 53]. Consequently, it is crucial to determine how the distribution of triplet yield varies concerning the external magnetic field. Defining the yield product of the triplet state (•Tsubscriptitalic-• _ T ) is imperative; therefore, it is defined according to Eq.10. (t)^^¡ ( t ) is the solution of the master equation Eq. 9, TrTrT r is the trace over the state density matrix As reported in [17, 53], the ROS yield follows the following distribution Hence, if RPM dictates the generation of reactive oxygen species (ROS), a plot of triplet yield (•Tsubscriptitalic-• _ T ) against the external magnetic field (B0subscriptµ0B_{0}B _ 0 ) should exhibit a strong correlation with the aforementioned experimental findings. In [17, 53], it is documented that the two nuclei involved may be an oxide (O2Ë™superscriptsubscript‚2Ë™O_{2}^{ _ 2 ^ overË™ - ) and tryptophan (Trp+Ë™superscriptË™Trp^{ r p ^ overË™ + ). The oxygen nucleus in O2Ë™superscriptsubscript‚2Ë™O_{2}^{ _ 2 ^ overË™ - is nonmagnetic and, thus, does not contribute to the hyperfine interaction. Conversely, tryptophan contains multiple magnetic nuclei of nitrogen and hydrogen. Considering the biological context where distances between nuclei and electrons are variable, a parametric approach has been adopted, specifically addressing the hyperfine interaction of tryptophan with an electron. In our initial investigation into the feasibility of the radical pair mechanism, we have defined three quantities that will assist us in our investigation. Based on the experimental results reported in [17, 53], we expect parameters L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 to have values at least greater than 1. We initially perform analysis considering there is no CISS (=00 = 0) in the system and try to ascertain the planaria response to the external magnetic field. In Figure 4, the triplet yield (•Tsubscriptitalic-• _ T ) is plotted against the external magnetic field (B0subscriptµ0B_{0}B _ 0 ) for =00 = 0 (no CISS) at kS=108subscript†superscript108k_{S}=10^{8}k _ S = 10 ^ 8 and kT=106subscriptsuperscript106k_{T}=10^{6}k _ T = 10 ^ 6 , with D=0·0D=0D = 0 and J=0½0J=0J = 0, under the hyperfine configuration [axx,ayy,azz]=[0.54,0.06,0.24]mTsubscript¥¥subscript¦¦subscript§§0.540.060.24[a_{xx},a_{yy},a_{zz}]=[0.54,0.06,0.24]mT[ a _ x x , a _ y y , a _ z z ] = [ 0.54 , 0.06 , 0.24 ] m T. The figure highlights points of interest corresponding to experimental conditions in planaria: AAA for •Tsubscriptitalic-• _ T at 45Î¼T4545 T45 Î¼ T, BµBB for •Tsubscriptitalic-• _ T at 200Î¼T200200 T200 Î¼ T, and C¶CC for •Tsubscriptitalic-• _ T at 500Î¼T500500 T500 Î¼ T. The experimental plot from references [17, 53] is displayed at the bottom of Figure 4, demonstrating the concentration of reactive oxygen species (ROC) in the experimental outcome. A parametric analysis was conducted for all values of [axx,ayy,azz]subscript¥¥subscript¦¦subscript§§[a_{xx},a_{yy},a_{zz}][ a _ x x , a _ y y , a _ z z ], with each component ranging from 0 to 3 mTmTm T (step size 0.06 mTmTm T), ensuring that all three parameters (L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 ) yield values greater than unity. Out of 132,651 possible hyperfine tensors, approximately 39 values met the desired condition. From Figure 4, the observed values were L1=1.132,L2=1.131,L3=1.001formulae-sequencesubscript¿11.132formulae-sequencesubscript¿21.131subscript¿31.001L_{1}=1.132,L_{2}=1.131,L_{3}=1.001L _ 1 = 1.132 , L _ 2 = 1.131 , L _ 3 = 1.001. Although ideally, a higher value of these ratios is preferred, the range of these ratios for all 39 possible hyperfine tensors was between 1-1.25. The maximum values of hyperfine tensor [axx,ayy,azz]subscript¥¥subscript¦¦subscript§§[a_{xx},a_{yy},a_{zz}][ a _ x x , a _ y y , a _ z z ], where L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 are greater than unity, were found to be [0.54,0.06,0.24]mT0.540.060.24[0.54,0.06,0.24]mT[ 0.54 , 0.06 , 0.24 ] m T . Consequently, the desired trend was only observed for low hyperfine values. Additionally, it was noted that the absolute value of •Tsubscriptitalic-• _ T was relatively low. In the next section, chirality will be introduced to investigate whether spin selectivity provides a more accurate model that closely aligns with the experimental outcome. This section introduces spin selectivity arising from chirality, taking into account the nonzero value of Initially, we focus on spin selectivity during the formation of the radical pair from the acceptor and donor molecules. Consequently, the initial density matrix, denoted as P^Insubscript^ƒ¼ P _ I n , is influenced by variations in the CISS parameter However, the recombination operators are observed under the no CISS case (=00 = 0), representing the standard singlet and triplet projection operators. In the subsequent part, we explore the role of CISS in both the formation and recombination of the radical. Thus, will impact not only the initial density matrix P^Insubscript^ƒ¼ P _ I n but also recombination operators P^Ssubscript^ƒ† P _ S and P^Tsubscript^ƒ P _ T . This work calculates the result for the fixed orientation of the CISS axis to the hyperfine z-axis [37]. In Figure 5, the triplet yield (•Tsubscriptitalic-• _ T ) is graphed against the external magnetic field for five different values of =0,6,4,3,20‹6‹4‹3‹2 = 0 , / 6 , / 4 , / 3 , / 2 . This analysis is conducted at kS=108subscript†superscript108k_{S}=10^{8}k _ S = 10 ^ 8 and kT=106subscriptsuperscript106k_{T}=10^{6}k _ T = 10 ^ 6 , with D=0·0D=0D = 0 and J=0½0J=0J = 0, and under the hyperfine configuration [axx,ayy,azz]=[0,0.78,0.84]mTsubscript¥¥subscript¦¦subscript§§00.780.84[a_{xx},a_{yy},a_{zz}]=[0,0.78,0.84]mT[ a _ x x , a _ y y , a _ z z ] = [ 0 , 0.78 , 0.84 ] m T. Notably, CISS is present only during the formation of the radical in this scenario. We list the values of L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 in Tab.1 for Fig 5. It is noteworthy that when CISS exclusively influences the formation of the radical, the observed trend aligns with the experimental findings. A distinctive peak is evident at 500Î¼T500500 T500 Î¼ T when CISS is considered. Specifically, when =2‹2 = / 2 , there is a notable increase in the ratio L1subscript¿1L_{1}L _ 1 by at least four times. Tab. 2 provides the count of hyperfine tensors corresponding to the five values that meet the criteria 11 at kS=108subscript†superscript108k_{S}=10^{8}k _ S = 10 ^ 8 and kT=106subscriptsuperscript106k_{T}=10^{6}k _ T = 10 ^ 6 , considering CISS solely in the formation of the radical pair. In Fig.6, we plotted triplet yield •Tsubscriptitalic-• _ T with respect to external magnetic field for five values of =0,6,4,3,20‹6‹4‹3‹2 = 0 , / 6 , / 4 , / 3 , / 2 at kS=108subscript†superscript108k_{S}=10^{8}k _ S = 10 ^ 8 and kT=106subscriptsuperscript106k_{T}=10^{6}k _ T = 10 ^ 6 with D=0·0D=0D = 0 and J=0½0J=0J = 0 at two different hyperfine configuration. (a) [axx,ayy,azz]=[0.12,0.12,0]mTsubscript¥¥subscript¦¦subscript§§0.120.120[a_{xx},a_{yy},a_{zz}]=[0.12,0.12,0]mT[ a _ x x , a _ y y , a _ z z ] = [ 0.12 , 0.12 , 0 ] m T and (b) [axx,ayy,azz]=[0,0.72,0.9]mTsubscript¥¥subscript¦¦subscript§§00.720.9[a_{xx},a_{yy},a_{zz}]=[0,0.72,0.9]mT[ a _ x x , a _ y y , a _ z z ] = [ 0 , 0.72 , 0.9 ] m T. We list the values of L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 in Tab.3 and Tab.4 for all five values of Our parametric analysis found no single combination of hyperfine values for which L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 are greater than one for all five values of at kS=108subscript†superscript108k_{S}=10^{8}k _ S = 10 ^ 8 and kT=106subscriptsuperscript106k_{T}=10^{6}k _ T = 10 ^ 6 . The closest result observed was when [axx,ayy,azz]=[0.12,0.12,0]mTsubscript¥¥subscript¦¦subscript§§0.120.120[a_{xx},a_{yy},a_{zz}]=[0.12,0.12,0]mT[ a _ x x , a _ y y , a _ z z ] = [ 0.12 , 0.12 , 0 ] m T when =6,4,3,2‹6‹4‹3‹2 = / 6 , / 4 , / 3 , / 2 all reported a value of L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 greater than 1 however this was not observed for =00 = 0. It was observed that the values of •Tsubscriptitalic-• _ T at =2‹2 = / 2 are at least ten times larger than that for the case when =00 = 0. However, the values of the ratio L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 remain almost the same. Moreover, the number of hyperfine tensors corresponding to five values of which satisfy criteria 11 at kS=108subscript†superscript108k_{S}=10^{8}k _ S = 10 ^ 8 and kT=106subscriptsuperscript106k_{T}=10^{6}k _ T = 10 ^ 6 is given in 5 when CISS acts both in formation and recombination of radical pair. Hence, we summarize the difference in the cases when CISS is only present at the formation of the radical (Case A) and when CISS is present both in the formation and recombination of radicals (Case B). Case A has a lower value of triplet yield at 45,200,500Î¼T4520050045,200,500 T45 , 200 , 500 Î¼ T compared to Case B; hence, case B seems more favorable. Case A shows an improved increase in the ratio of L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 due to CISS compared to Case B at around the same hyperfine tensor; hence, case A seems more favorable. Case A has more number of hyperfine tensor for 00 0 0 compared to case B. Since this reaction occurs in a biological system, this property gives the system more degree of freedom and reaction and is not bounded by strict values of hyperfine tensors. Therefore, case A is more promising if we consider this property. In this section, our aim was to investigate the impact of singlet and triplet recombination rates on the yield of the triplet product, •Tsubscriptitalic-• _ T . The goal is to discern a trend that aligns with experimental findings, as articulated in Eq.11, across the maximum number of hyperfine tensors. Recognizing that the planaria reaction transpires in a biological medium, we have exercised the discretion to consider hyperfine tensors. Tabulated in Tab6 are the counts of hyperfine configurations, out of the 132,651 possible configurations, that adhere to the observed experimental trend when =00 = 0. Our observation indicates that when =00 = 0, rate combinations with kS=108subscript†superscript108k_{S}=10^{8}k _ S = 10 ^ 8 and kT=105subscriptsuperscript105k_{T}=10^{5}k _ T = 10 ^ 5 exhibit the maximum number of hyperfine configurations satisfying the experimental conditions. However, the absolute value of triplet yield is low due to the diminished triplet recombination rate. Consequently, a higher triplet rate leads to an increased triplet yield value; nevertheless, the rapid recombination rate renders the yield less sensitive to the external magnetic field. This results in a limited number of hyperfine tensor values that meet the specified criteria. Additionally, it was observed that no rate combination resulted in L1,L2,L3¥2subscript¿1subscript¿2subscript¿32L_{1},L_{2},L_{3} 2L _ 1 , L _ 2 , L _ 3 ¥ 2 (table not shown). When =2‹2 = / 2 , we have calculated the number of combinations of hyperfine tensors for which L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 is greater than unity. This has been done for case A (Tab.7) and case B (Tab.8) at =2‹2 = / 2 . We generally observe an increase in the number of hyperfine tensors for either case when compared to when =00 = 0. However, in case A, the number of hyperfine tensors for which L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 is greater than unity is much greater compared to case B for most values of kSsubscript†k_{S}k _ S and kTsubscriptk_{T}k _ T . We have also computed the maximum values of L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 when each component of the hyperfine tensor [axx,ayy,azz]subscript¥¥subscript¦¦subscript§§[a_{xx},a_{yy},a_{zz}][ a _ x x , a _ y y , a _ z z ] varies from 0 to 3 mT, covering a total of 132,651 tensor values at =2‹2 = / 2 for case A (Tab.13) and case B (Tab.14). Notably, significant values of L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 are observed when kTsubscriptk_{T}k _ T is in the range of 104superscript10410^{4}10 ^ 4 and 105superscript10510^{5}10 ^ 5 , with the singlet recombination rate being at least a hundred times (107superscript10710^{7}10 ^ 7 and 108superscript10810^{8}10 ^ 8 ) for both the cases.We also observe that L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 values are higher for case A compared to case B. We have also calculated the maximum absolute value of triplet yield (ROS level) at 45Î¼T,200Î¼T,500Î¼T4520050045 T,~{}200 T,~{}500 T45 Î¼ T , 200 Î¼ T , 500 Î¼ T for case A (Tab.16) and case B (Tab.15) It is essential to acknowledge that the rate combination resulting in the maximum value of L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 tends to yield lower values of triplet yield compared to yields observed at other rates. This implies that at =2‹2 = / 2 , one can either achieve the maximum value of the ratio L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 or attain the absolute yield value by adjusting the recombination rates. This section examines whether the intended experimental outcomes are observed for higher nuclei or if the trend diverges. Given in [17, 53], indicating that the second nucleus might be tryptophan, we assume the second nucleus to be spin-half, akin to hydrogen. We vary it similarly to a single-nuclei system, but with two nuclei, the number of parameters increases to six. We sampled 51 points from 0 to 3 mT for each component of the hyperfine tensor for a single nuclei system, resulting in 132,651 combinations. This number is 516superscript51651^{6}51 ^ 6 tensors for two nuclei systems a very large value. Consequently, we selected values for the hyperfine tensor of the first nucleus that satisfy the conditions outlined in the criteria 11 as given in the preceding section of one nucleus study and varied the hyperfine tensor for the second nucleus. We observe in Fig.7 that for a two nuclei system, we observe the desired trend for =2‹2 = / 2 . The L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 are listed in Tab.9 for five values of We observe that compared to single nucleus case with hyperfine tensor [axx,ayy,azz]=[0.12,0.12,0]mTsubscript¥¥subscript¦¦subscript§§0.120.120[a_{xx},a_{yy},a_{zz}]=[0.12,0.12,0]mT[ a _ x x , a _ y y , a _ z z ] = [ 0.12 , 0.12 , 0 ] m T Fig.(6.a), we observe that L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 is greater for =2‹2 = / 2 for two nuclei case. Hence, we also observe an experimental trend for two nuclei cases. However, due to scaling issues, it might not be possible to give maximum values of L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 . Keeping the value of hyperfine tensor fixed [axx,ayy,azz]=[0.12,0.12,0]mTsubscript¥¥subscript¦¦subscript§§0.120.120[a_{xx},a_{yy},a_{zz}]=[0.12,0.12,0]mT[ a _ x x , a _ y y , a _ z z ] = [ 0.12 , 0.12 , 0 ] m T, we have done a parametric analysis for the second nucleus and found a number of hyperfine tensors satisfying the criteria for various value of (Tab.10). A similar analysis for hyperfine tensor is[axx,ayy,azz]=[0,0.72,0.9]mTsubscript¥¥subscript¦¦subscript§§00.720.9[a_{xx},a_{yy},a_{zz}]=[0,0.72,0.9]mT[ a _ x x , a _ y y , a _ z z ] = [ 0 , 0.72 , 0.9 ] m T of spin 1 nucleus give us very different results in Tab.11. Here, we obtain no hyperfine tensor for second nuclei, which might satisfy the criteria for =2‹2 = / 2 . Hence, we cannot reach any conclusive result by this brute-force method. We also simulated the case when CISS is only present in the formation of the radical pair when the spin one nucleus has hyperfine tensor [axx,ayy,azz]=[0,0.78,0.84]mTsubscript¥¥subscript¦¦subscript§§00.780.84[a_{xx},a_{yy},a_{zz}]=[0,0.78,0.84]mT[ a _ x x , a _ y y , a _ z z ] = [ 0 , 0.78 , 0.84 ] m T. We found, in general, that more hyperfine tensors satisfied the criteria (Tab.12). This hints more towards the involvement of CISS only in forming the radical pair. However, a more rigorous approach is required to achieve a more conclusive outcome. The process of blastema formation, crucial for planarian regeneration, seems to be correlated with the level of Reactive Oxygen Species (ROS). Interestingly, the experiments suggest that exposure to Weak Magnetic Fields (WMF) should be continued until the blastema is fully formed. It™s worth noting that the radical pair mechanism, which involves the spin dynamics of electrons and is influenced by magnetic fields, typically has a very short lifetime, on the order of microseconds. This short lifetime raises questions about whether the radical pair mechanism directly participates in the planarian regeneration process or if there™s another mechanism at play. The observed correlation between WMF exposure duration and blastema formation could imply a more complex interplay between magnetic field effects and biological processes. It™s possible that the radical pair mechanism, despite its short-lived nature, triggers or influences other biochemical pathways or mechanisms that lead to blastema formation over a longer timescale. Further research is needed to unravel the specifics of these interactions and mechanisms in planarian regeneration. The issue of radical pair formation in the context of planarian regeneration, especially in the absence of an external excitation source, raises intriguing questions. In avian magnetoreception, radical pairs are typically formed through the photoexcitation of neutral acceptor molecules. In the experiments conducted on planaria regeneration, where Weak Magnetic Fields (WMF) are applied [17, 53], there is no apparent external excitation source. The absence of a known external stimulus for radical pair formation prompts the question of what initiates this reaction in the absence of an external excitation source. One plausible explanation could be related to the cut or segment of the planaria body. The external cut might release some reactive chemical or signaling molecule, which could act as an internal stimulus, leading to the generation of excitation and the subsequent formation of radical pairs in the system. However, the specific identity and nature of this chemical or excitation remain unclear and would require further investigation. Understanding the underlying biochemical processes triggered by the external cut in planaria regeneration, especially in the context of magnetic field effects, could provide valuable insights into the role of radical pair mechanisms and other potential signaling pathways in this intriguing biological phenomenoa. In summary, our analysis suggests that the Radical Pair Mechanism (RPM) provides an explanation for the influence of a weak magnetic field on planaria regeneration. However, our findings indicate that the effect is more robustly explained when considering the Chirality-Induced Spin Selectivity (CISS) in conjunction with the radical pair mechanism. Specifically, when chirality is involved solely in the formation of the radical pair, it yields even more consistent and fitting results based on the experimental findings. This underscores the potential significance of CISS in understanding the magnetic field effects on planaria regeneration with radical pair mechanism. In this section, we have listed the L1,L2,L3subscript¿1subscript¿2subscript¿3L_{1},L_{2},L_{3}L _ 1 , L _ 2 , L _ 3 at all possible recombination rates of kS,kTsubscript†subscriptk_{S},k_{T}k _ S , k _ T when CISS is involved in the formation and recombination of radical pair as well as when it is involved only in the formation of the radical. We have also listed the absolute values of the yield at 45Î¼T,200Î¼T,500Î¼T4520050045 T,200 T,500 T45 Î¼ T , 200 Î¼ T , 500 Î¼ T for both of the mentioned cases. All the results are for the case when =2‹2 = / 2 .",
        "keywords": ""
    },
    {
        "id": 7,
        "title": "Uso de herramientas digitales matemÃ¡ticas en la EducaciÃ³n Secundaria",
        "abstract": "ResumenLas TecnologÃ­as de la InformaciÃ³n y la ComunicaciÃ³n (TIC) estÃ¡n cada dÃ­a mÃ¡s presentes en nuestra sociedad y por tanto en el Ã¡mbito educativo. En apenas dos dÃ©cadas hemos pasado de una enseÃ±anza basada, en muchos casos, en las clases magistrales a una enseÃ±anza en la que metodologÃ­as como el aula invertida o la gamificaciÃ³n tienen mÃ¡s fuerza que nunca.A lo largo del trabajo hemos realizado una encuesta a docentes y alumnado con el objetivo de comparar los conocimientos en herramientas digitales, su uso y su aceptaciÃ³n. Hemos utilizado WxMaxima y Geogebra como herramientas didÃ¡cticas. Para ello propondremos un ejercicio de laEvaluaciÃ³n de Bachillerato para el Acceso a la Universidad(EBAU) relacionado con la geometrÃ­a, analizando sus puntos fuertes y dÃ©biles en comparaciÃ³n con la resoluciÃ³n manual. Por Ãºtlimo, expondremos algunas conclusiones y posibles lÃ­neas de investigaciÃ³n acerca de las herramientas digitales, asÃ­ como una propuesta de curso introductorio a WxMaxima y Geogebra con el que formar al docente de secundaria.",
        "corpus": "Las TecnologÃas de la InformaciÃ³n y la ComunicaciÃ³n (TIC) estÃ¡n cada dÃa mÃ¡s presentes en nuestra sociedad y por tanto en el Ã¡mbito educativo. En apenas dos dÃ©cadas hemos pasado de una enseÃ±anza basada, en muchos casos, en las clases magistrales a una enseÃ±anza en la que metodologÃas como el aula invertida o la gamificaciÃ³n tienen mÃ¡s fuerza que nunca. A lo largo del trabajo hemos realizado una encuesta a docentes y alumnado con el objetivo de comparar los conocimientos en herramientas digitales, su uso y su aceptaciÃ³n. Hemos utilizado WxMaxima y Geogebra como herramientas didÃ¡cticas. Para ello propondremos un ejercicio de la EvaluaciÃ³n de Bachillerato para el Acceso a la Universidad (EBAU) relacionado con la geometrÃa, analizando sus puntos fuertes y dÃ©biles en comparaciÃ³n con la resoluciÃ³n manual. Por Ãºtlimo, expondremos algunas conclusiones y posibles lÃneas de investigaciÃ³n acerca de las herramientas digitales, asÃ como una propuesta de curso introductorio a WxMaxima y Geogebra con el que formar al docente de secundaria. Abstract Information and Community Technologies (ICT) are very present in our society nowadays and particularly in the educative field. In just two decades, we have passed from a learning based, in many cases, on the master lessons to one such that methodologies like the flipped classroom or the gamification are stronger than ever. Along this work, we have done a study to teachers and students with the main objective to compare the knowledge on digital tools, their use and their acceptation. We use WxMaxima and Geogebra in order to solve an exercise of EvaluaciÃ³n de Bachillerato para el Acceso a la Universidad (EBAU) related with Geometry, comparing their ins and outs with the manual solution. Finally, we expose some conclusions and some possible research lines about digital tools, as well as a proposition of an introductory course on WxMaxima and Geogebra in order to teach the teachers. Keywords: TIC; Geogebra; WxMaxima; Herramientas digitales; EducaciÃ³n MatemÃ¡tica; Competencia Digital. MSC2020: 97D10; 97N80; 97U10. La Ley OrgÃ¡nica 2/2006, de 3 de mayo, de EducaciÃ³n, LOE en adelante, define las competencias como las capacidades para aplicar de forma integrada los contenidos propios de cada enseÃ±anza y etapa educativa, con el fin de lograr la realizaciÃ³n adecuada de actividades y la resoluciÃ³n eficaz de problemas complejos. Todas las leyes que han sucedido a esta, han apostado por un carÃ¡cter competencial y cada vez tienen mÃ¡s fuerza dentro de la enseÃ±anza. En la vigente Ley OrgÃ¡nica 3/2020, de 29 de diciembre, por la que se modifica la Ley OrgÃ¡nica 2/2006, de 3 de mayo, de EducaciÃ³n (LOMLOE en adelante), podemos encontrar la Competencia MatemÃ¡tica y de ciencias, tecnologÃa e ingenierÃa (STEM) y la Competencia digital. SegÃºn el Ministerio de EducaciÃ³n y FormaciÃ³n Profesional, la competencia matemÃ¡tica implica la capacidad de aplicar el razonamiento matemÃ¡tico y sus herramientas para describir, interpretar y predecir distintos fenÃ³menos en su contexto, mientras que la competencia digital implica el uso creativo y crÃtico de las tecnologÃas de la informaciÃ³n y la comunicaciÃ³n, siempre de manera segura y controlada, por lo que no sÃ³lo implicarÃ¡ destrezas en el acceso o procesamiento de la informaciÃ³n, sino tambiÃ©n del filtrado y comprobaciÃ³n de la veracidad de dicha informaciÃ³n. Es evidente la relaciÃ³n directa entre las matemÃ¡ticas y el uso de las tecnologÃas, ya que los nuevos avances cientÃficos permiten, por ejemplo, el mejor cifrado y encriptado de contraseÃ±as en internet, pero la relaciÃ³n entre informÃ¡tica y matemÃ¡ticas parece mÃ¡s difusa. Esta relaciÃ³n se propicia cuando, para descubrir nuevos resultados matemÃ¡ticos, se hace uso de las herramientas informÃ¡ticas para realizar cÃ¡lculos que hacerlos manualmente nos llevarÃan una eternidad. Si echamos la vista atrÃ¡s, no hace mucho los logaritmos se calculaban mediante interpolaciones a partir de valores ya calculados en unas tablas. Estas tablas quedaron obsoletas cuando aparecieron las calculadoras. Estamos quizÃ¡s ante la siguiente brecha digital, donde los ordenadores han venido a sustituir a las calculadoras, como estas hicieron con las tablas de logaritmos y otros muchos cÃ¡lculos manuales. No podemos dar el siguiente paso tecnolÃ³gico a los ordenadores, tablets o pizarras digitales si el personal docente no tiene suficientes conocimientos sobre su utilizaciÃ³n. Es evidente entonces, que la incorporaciÃ³n de nuevas herramientas digitales a la docencia de las matemÃ¡ticas es inevitable y se intensificarÃ¡ en los prÃ³ximos aÃ±os. El uso de los ordenadores para resolver problemas no implica la desapariciÃ³n del razonamiento lÃ³gico, ademÃ¡s, ambas formas de trabajar se complementan y pueden, incluso, potenciarse entre sÃ. Por tanto, debemos formar a los alumnos en el uso de las nuevas tecnologÃas para resolver, visualizar o interactuar con un problema matemÃ¡tico y su aplicaciÃ³n en la vida diaria. Un libro que aborda estas y otras inquietudes relacionadas con las matemÃ¡ticas y la educaciÃ³n es [2]. En Ã©l se hace un repaso de distintos Ã¡mbitos relacionados con la educaciÃ³n matemÃ¡tica tanto en las etapas obligatorias como en la post-obligatoria, asÃ como cuestiones que deben ser transversales a lo largo de toda la enseÃ±anza de las matemÃ¡ticas. TambiÃ©n se detallan algunos aspectos sobre la formaciÃ³n profesional del profesorado de matemÃ¡ticas. Por tanto, ha sido una obra de referencia en las dos Ãºltimas dÃ©cadas y puede ser un complemento a la lectura de este artÃculo. A lo largo del artÃculo trataremos los siguientes aspectos: En primer lugar, sentaremos el marco normativo, mÃ¡s concretamente, hablaremos sobre las competencias STEM y digital. Analizaremos el manejo que tienen los profesores y los alumnos en las herramientas digitales relacionadas con las matemÃ¡ticas. Para ello, se ha llevado a cabo una encuesta a docentes y alumnos sobre el uso de diferentes herramientas como LaTeX, Wolfram Alpha o Geogebra. Utilizaremos los programas de libre acceso WxMaxima y Geogebra para resolver un ejercicio de la prueba de acceso a la Universidad de Extremaudra y analizar cÃ³mo podrÃan ayudar al alumnado. Dada la necesidad de formaciÃ³n en herramientas digitales especÃficas de matemÃ¡ticas, propondremos un curso para iniciar a docentes y alumnos en el manejo de WxMaxima y Geogebra. Finalmente, cerraremos el documento con una secciÃ³n dedicada a conclusiones obtenidas y posibles lÃneas de investigaciÃ³n que se podrÃan explorar a partir de este trabajo. La educaciÃ³n de nuestros dÃas se basa en el desarrollo del currÃculo mediante las competencias bÃ¡sicas o clave (segÃºn la ley a la que hagamos referencia). Aunque en el panorama internacional ya se hablaba sobre la educaciÃ³n por competencias desde que Philippe Perrenoud las introdujese en 1998 [16] y, posteriormente, la RecomendaciÃ³n del Parlamento Europeo y del Consejo 2006/962/CE, de 18 de diciembre de 2006, sobre las competencias clave para el aprendizaje es la primera vez que un organismo europeo recomendaba la implantaciÃ³n de este tipo de educaciÃ³n. En EspaÃ±a hicieron su apariciÃ³n en la Ley OrgÃ¡nica 2/2006, de 3 de mayo, de EducaciÃ³n, aunque no se mencionaban las mismas de forma expresa, ya dejaba ver el carÃ¡cter europeÃsta del que se querÃa dotar a la eduaciÃ³n. Las competencias se fueron asentando con el paso de los aÃ±os, hasta que a finales de 2013 la Ley OrgÃ¡nica 8/2013, de 9 de diciembre, para la mejora de la calidad educativa, LOMCE en adelante, recoge por primera vez en EspaÃ±a las siete competencias claves del sistema educativo espaÃ±ol y sienta las relaciones entre las competencias, los contenidos y los criterios de evaluaciÃ³n de los distintos niveles educativos a travÃ©s de la Orden ECD/65/2015, de 21 de enero, por la que se describen las relaciones entre las competencias, los contenidos y los criterios de evaluaciÃ³n de la educaciÃ³n primaria, la eduaciÃ³n secundaria obligatoria y el bachillerato. El caracter competencial en la educaciÃ³n secundaria sigue vigente con la nueva legislaciÃ³n, la Ley OrgÃ¡nica 3/2020, de 29 de diciembre, por la que se modifica la Ley OrgÃ¡nica 2/2006, de 3 de mayo, de EducaciÃ³n (LOMLOE). En ella se describen las competencias clave, entre las que se encuentran la competencia MatemÃ¡tica y de ciencias, tecnologÃa e ingenierÃa (STEM) y la competencia digital. Se establece que la competencia matemÃ¡tica es aquella que implica la capacidad de aplicar el razonamiento matemÃ¡tico y sus herramientas para describir, interpretar y predecir distintos fenÃ³menos [4, Anexo I,2.a]. Por tanto, el estudio de las asignaturas de matemÃ¡ticas no debe buscar la memorizaciÃ³n de fÃ³rmulas, problemas tÃpicos o definiciones concretas, sino que, a partir de ciertos conocimientos teÃ³ricos, ha de permitir interpretar el mundo que nos rodea para resolver ploblemas con herramientas matemÃ¡ticas. No se trata entonces de aprender cuÃ¡les son las herramientas matemÃ¡ticas, sino cÃ³mo aplicar estas herramientas en el mundo exterior. Por ejemplo, no hay razÃ³n ninguna para memorizar la famosa fÃ³rmula del Teorema de PitÃ¡goras si no sabemos en quÃ© situaciones es pertinente el uso de este conocimiento matemÃ¡tico. AsÃ, esta fÃ³rmula puede servir como mero ejercicio mental de memorizaciÃ³n, o puede servirnos para analizar una rampa de un acceso para minusvÃ¡lidos que nos encontremos en cualquier acera de nuestra ciudad. Por ejemplo, si el triÃ¡ngulo de la Figura 1 representa una de dichas rampas entonces podemos calcular la longitud de dicha rampa gracias a que a2=b2+c2superscript2superscript2superscript2a^{2}=b^{2}+c^{2}a ^ 2 = b ^ 2 + c ^ 2 y tambiÃ©n podemos ver el Ã¡ngulo de inclinaciÃ³n de la rampa con las relaciones trigonomÃ©tricas y establecer si la rampa cumple o no con las medidas para la accesibilidad del moviliario urbano. Se trata por tanto de relacionar las fÃ³rmulas y herramientas matemÃ¡ticas con el mundo que nos rodea, de forma que lo hagamos entendible desde un punto de vista matemÃ¡tico. De todo ello trata el libro Mirar la ciudad con ojos matemÃ¡ticos, [1], L. Blanco establece distintos itinerarios por la ciudad de Badajoz en los que iremos paseando a la vez que realizamos pequeÃ±as actividades de reflexiÃ³n o de cÃ¡lculo matemÃ¡tico en las que emplearemos algunas de las herramientas que aprendemos en la escuela y que muchos alumnos piensan que no tienen utilidad. En el informe PISA [15, p. 7] se introduce el concepto de alfabetizaciÃ³n matemÃ¡tica que, a su vez es recogido en castellano en [8, p. 10]: La alfabetizaciÃ³n matemÃ¡tica es la capacidad de un individuo de razonar matemÃ¡ticamente y de formular, emplear e interpretar las matemÃ¡ticas para resolver problemas en una amplia variedad de contextos de la vida real. Esto incluye conceptos, procedimientos, datos y herramientas para describir, explicar y predecir fenÃ³menos. Ayuda a los individuos a conocer el papel que cumplen las matemÃ¡ticas en el mundo y hacer los juicios y tomar las decisiones bien fundamentadas que necesitan los ciudadanos reflexivos, constructivos y comprometidos del siglo XXI. Continuando con el mismo documento, el CEMat aporta algunas indicaciones que deben dejar de realizarse en la EducaciÃ³n Secundaria Obligatoria y el Bachillerato, por ejemplo utilizar fÃ³rmulas dadas como modelos de problemas del mundo real, sin analizar de dÃ³nde surge el modelo o expresar ecuaciones de una funciÃ³n de forma analÃtica, con el exclusivo objeto de representarlas grÃ¡ficamente. Todo ello sugiere que las matemÃ¡ticas y el mundo que conocemos deben estar relacionados y el objeto de estudio de las matemÃ¡ticas debe ser precisamente entender dicha relaciÃ³n. Respecto a la competencia digital [4, Anexo I, 3] es aquella que implica el uso creativo, crÃtico y seguro de las tecnologÃas de la informaciÃ³n y la comunicaciÃ³n. De forma que Esta competencia supone, ademÃ¡s de la adecuaciÃ³n a los cambios que introducen las nuevas tecnologÃas [¦] para ser competente en un entorno digital. Requiere de conocimientos relacionados con el lenguaje especÃfico bÃ¡sico: textual, numÃ©rico, icÃ³nico, visual, grÃ¡fico y sonoro, asÃ como sus pautas de decodificaciÃ³n y transferencia. Esto conlleva el conocimiento de las principales aplicaciones informÃ¡ticas. Por tanto, con ojos matemÃ¡ticos, el uso de las nuevas tecnologÃas puede servirnos para complementar las explicaciones teÃ³ricas. Al igual que el uso de la calculadora no implica que la suma de fracciones no tenga que ser estudiada, por ejemplo, el aprendizaje en el uso de herramientas informÃ¡ticas para el cÃ¡lculo matemÃ¡tico no implica que esos contenidos teÃ³ricos no deban ser explicados, sino que el alumno tendrÃ¡ una forma mÃ¡s de comprobar si los resultados que va obteniendo son correctos. En el [17] se pone este mismo ejemplo de la calculadora como herramienta plenamente instaurada en el aula, pero que como toda herramienta, es indispensable aprender a utilizarla para que el alumnado no la considere un sustituto del razonamiento matemÃ¡tico, es decir, no deben verse las herramientas digitales como un sutitutivo del razonamiento, sino como herramientas que exploten las posibilidades que ofrece para agilizar procesos de cÃ¡lculo en las aulas y para que los alumnos puedan centrarse en la comprensiÃ³n de los nuevos conceptos. SegÃºn [14], La tecnologÃa como recurso de aprendizaje tiene que estar integrada en el currÃculum, es decir, el maestro formado tiene que saber a la hora de diseÃ±ar una actividad si decide emplear un recurso tecnolÃ³gico, cuÃ¡ndo y cÃ³mo hacerlo y asÃ se establece en la Orden ECD/65/2015. En ella podemos apreciar que se indica textualmente que se debe conocer el lenguaje bÃ¡sico de las herramientas digitales en relaciÃ³n con conocimientos numÃ©ricos y grÃ¡ficos, lo que implica el conocimiento de las principales aplicaciones informÃ¡ticas. Parece, por tanto, necesario relacionar las dos competencias con las que estamos tratanto, la matemÃ¡tica y la digital, de manera que se complementen mutuamente. A pesar de que el uso de herramientas digitales estÃ¡ expresamente recogido en el currÃculo de la educaciÃ³n secundaria de las asignaturas de matemÃ¡ticas, el informe realizado por la Real Sociedad MatemÃ¡tica EspaÃ³la y la Sociedad CientÃfica InformÃ¡tica de EspaÃ±a en 2020 [18] reconoce que la gran mayorÃa de los libros sigue incorporando la tecnologÃa de modo anecdÃ³tico y muy ligada a la calculadora, ademÃ¡s de reflejar la gran diferencia que hay en los docentes, desde profesorado muy tecnologizado que incorpora geometrÃa dinÃ¡mica, hojas de cÃ¡lculo, software de representaciÃ³n grÃ¡fica y hasta lenguajes de programaciÃ³n en su docencia de matemÃ¡tica, hasta profesorado que prohÃbe el uso de la calculadora en ESO. En [10], se propuso una serie de actividades con Geogebra basados en rompecabezas, llegando a la conclusiÃ³n de que aprender GeometrÃa utilizando rompecabezas fue percibido por la mayorÃa de los estudiantes como una experiencia agradable que les permite descubrir nuevas cosas y que al docente le sirve no sÃ³lo para estimular el pensamiento geomÃ©trico, sino tambiÃ©n fomentar el interÃ©s y motivaciÃ³n hacia aprendizaje de la GeometrÃa. AdemÃ¡s, el uso de las nuevas tecnologÃas en el aula tiene importantes implicaciones pedagÃ³gicas, como pueden ser, segÃºn [14]: la creaciÃ³n un entorno interactivo de enseÃ±anza / aprendizaje en el cual los aprendices pueden ser indistintamente emisores y receptores de informaciÃ³n, lo que provoca alta motivaciÃ³n en el aprendiz, que repercutirÃ¡ directamente en el clima de la clase; la enseÃ±anza se adapta a las necesidades especÃficas del alumno, lo que favorece una de las obligaciones de todo docente, la atenciÃ³n a la diversidad; se facilita la enseÃ±anza cooperativa, favoreciendo asÃ las relaciones sociales y la empatÃa entre discentes; y por Ãºltimo se fomenta el autoaprendizaje, es decir, estamos repercutiendo positivamente en la adquisiciÃ³n de otra de las competencias bÃ¡sicas de especial importancia en la educaciÃ³n integral de los alumnos, aprender a aprender. Una vez que hemos visto las dos competencias fundamentales que se buscan estudiar en este trabajo, cabe destacar que la Real Sociedad MatemÃ¡tica EspaÃ±ola estÃ¡ haciendo importantes esfuerzos por estudiar cÃ³mo aunar estas dos competencias durante todo el perÃodo de escolaridad. MÃ¡s concretamente, en [17] se establece que la educaciÃ³n y, especÃficamente, el aprendizaje de las matemÃ¡ticas, no pueden ser ajenas a los cambios que nos conducen hacia una sociedad mÃ¡s tecnificada y digitalizada. Analizaremos en la prÃ³xima secciÃ³n, la necesidad de formaciÃ³n del profesorado en el uso y utilidad de las herramientas digitales. SegÃºn Carioca et. al. [7], la necesidad de formaciÃ³n del profesorado en el uso de las herramientas digitales es una necesidad prioritaria de nuestros dÃas, no tanto en el uso de las propias herramientas, sino en la utilidad didÃ¡ctica que tienen y en cÃ³mo llevarlas al aula. AdemÃ¡s, MartÃn destaca en [14, p. 44] que debido a la rapidez de los avances tecnolÃ³gicos se hace mÃ¡s patente que nunca la asunciÃ³n del profesorado en general de la necesidad de un reciclaje en su formaciÃ³n tecnolÃ³gica y en el uso pedagÃ³gico de la tecnologÃa. Necesidad que se subraya en el Libro Blanco de las MatemÃ¡ticas [17, p.33] mÃ¡s de una dÃ©cada despuÃ©s: el uso de tecnologÃas en las aulas requiere de un conocimiento didÃ¡ctico que va mÃ¡s allÃ¡ del conocimiento de su simple uso. Ya hemos seÃ±alado en las secciones anteriores, el papel de las nuevas tecnologÃas en la enseÃ±anza de las matemÃ¡ticas. Sin embargo, aÃºn hay docentes que se limitan al uso de las clases magistrales para la enseÃ±anza de esta materia. El objetivo principal de este artÃculo es realizar un estudio en docentes para comprobar el uso que le dan a distintas herramientas digitales, asÃ como el grado de conocimiento que poseen de ellas. MÃ¡s aÃºn, nos interesa ver cÃ³mo percibe el alumnado la implementaciÃ³n de las herramientas digitales en la enseÃ±anza de las matemÃ¡ticas. Todo esto nos lleva a realizar una encuesta a docentes y alumnado, en la que se les pregunte por distintos aspectos relacionados con las herramientas digitales y, mÃ¡s especÃficamente, por aquellas relacionadas con las matemÃ¡ticas. A raÃz de estas encuestas, podremos establecer distintas comparativas en cuanto al uso de las herramientas digitales por parte de los docentes en el aula o por parte de los alumnos en sus casas, asÃ como intentar entender los beneficios y los perjuicios que declaran ambos colectivos en el uso de las herramientas digitales en la enseÃ±anza de las matemÃ¡ticas. En esta secciÃ³n veremos las principales respuestas a las encuestas realizadas a docentes y alumnos durante el curso 2020-2021. La muestra tomada para el estudio en los docentes ha sido el conjunto de profesores de Ciencias (MatemÃ¡ticas, FÃsica, QuÃmica y BiologÃa) del centro en el que el autor realizÃ³ las prÃ¡cticas del MÃ¡ster de FormaciÃ³n del Profesorado, durante el curso 2020-2021, el Colegio Santa Teresa de JesÃºs en Badajoz, cuya titularidad es privada“concertada para los niveles de la ESO y privada“no concertada para los niveles de Bachillerato. AdemÃ¡s, para obtener mÃ¡s informaciÃ³n, se ha ampliado la muestra con el conjunto de los profesores de Ciencias del Instituto de EducaciÃ³n Secundaria El Sur, de la ciudad de Lepe, cuya titularidad es pÃºblica. La muestra tomada en los alumnos ha sido exclusivamente del Colegio Santa Teresa de Badajoz, mÃ¡s concretamente los alumnos de los grupos donde mi tutora de prÃ¡cticas imparte clases, es decir, 4.º de la ESO, 1.º y 2.º de Bachillerato, tanto la rama de MatemÃ¡ticas de Ciencias Sociales como de la de MatemÃ¡ticas para Ciencias. Por supuesto, la encuesta ha sido anÃ³nima y voluntaria. Las preguntas que se hicieron a ambos colectivos pueden encontrarse en los siguientes enlaces: Docentes: https://drive.google.com/file/d/1r-_YReEOhFgaTqtXX2TqPu94N4EQoDFW/view?usp=share_link Alumnado: https://drive.google.com/file/d/1460CqUUTl63VheDltHetF_r2OuMZmJeC/view?usp=share_link Se pueden observar que las preguntas son parecidas, de forma que podamos establecer algunas relaciones. Analizaremos primero las respuestas de los docentes, despuÃ©s la del alumnado, y finalmente comparemos ambos colectivos. Entre los docentes se han registrado un total de 12 respuestas. Empezando por la formaciÃ³n, observamos que la mayorÃa (58,3%percent58.358{,}3 %) de docentes provienen de licenciaturas, por lo que han pasado al menos diez aÃ±os desde que egregaron y, posiblemente, no tengan toda la formaciÃ³n en el uso de las TIC con fines didÃ¡cticos como serÃa deseable. Sin embargo, todos los docentes reconocen su utilidad y sÃ³lo el 8,3%percent8.38{,}3 % admite no saber usarlas. Algunas de las respuestas mÃ¡s interesantes a la pregunta ¿CÃ³mo crees que las herramientas digitales (en general) pueden ayudar a los alumnos en el aula o fuera de ella?, han sido: Pueden visualizar contenidos complejos de reproducir sin digitalizaciÃ³n. La motivaciÃ³n hacia la asignatura es mayor utilizando una herramienta digital. Pueden ayudar a comprender y aprender mÃ¡s y mejor que con el mÃ©todo tradicional. Les ayuda a que los estudios sean mÃ¡s cercanos. AdemÃ¡s, dos tercios del profesorado encuestado afirma haber realizado cursos de formaciÃ³n en herramientas digitales en el Ãºltimo aÃ±o, quizÃ¡ debido a que han tenido que actualizarse rÃ¡pidamente por los confinamientos y la situaciÃ³n del COVID“19, mientras que un 16,7%percent16.716{,}7 % afirma haber realizado cursos en los Ãºltimos cinco aÃ±os y el otro 16,7%percent16.716{,}7 % afirma no haber realizado cursos desde hace mÃ¡s de diez aÃ±os. Muchos de los cursos ofertados por el Centro de Profesores y Recursos (CPR) son de manejo elemental y, precisamente por esto, algunos de los docentes que usan estas herramientas terminan no haciendo los cursos. Se preguntÃ³ tambiÃ©n sobre el nivel de uso de distintas herramientas (ofimÃ¡tica, Kahoot!, generador de nubes de palabras, mapas conceptuales, Canva, Genially y Powtoon) para la elaboraciÃ³n de material didÃ¡ctico. Las respuestas se recogen en la Figura 2. Cabe destacar el uso casi a diario de las herramientas de ofimÃ¡tica, y un uso algo menor para Kahoot! y herramientas de creaciÃ³n de mapas conceptuales. El resto de aplicaciones tiene un uso prÃ¡cticamente nulo. En el Ã¡mbito especÃfico de las herramientas digitales relacionadas con las matemÃ¡ticas, sobre el conocimiento en general de ciertas herramientas se han obtenido las respuestas recogidas en la Figura 3. Donde apreciamos que todos los docentes conocen el editor de fÃ³rmulas de Word, pero que la mayorÃa (siete de los doce encuestados) no conocen LaTeX para la ediciÃ³n de texto cientÃfico. Sobre las herramientas de cÃ¡lculo destaca que casi todos conocen Geogebra, pero la mayorÃa desconocen programas de cÃ¡lculo simbÃ³lico como WxMaxima, Wolfram Alpha o Wolfram Mathematica. CentrÃ¡ndonos, por tanto, en el uso del editor de fÃ³rmulas de Word, cinco docentes afirman usarlo con mucha frecuencia y dos de ellos a diario, indicando, en la siguiente pregunta, que han aprendido a usarlo, mayoritariamente de manera autodidacta. En cuanto a Geogebra, la mayorÃa de respuesta se ubica en que su uso es esporÃ¡dico (alguna vez) y que la mayorÃa, de nuevo, han sido autodidactas en su aprendizaje. Algunas opiniones de los docentes sobre cÃ³mo ayudan estas herramientas especÃficamente matemÃ¡ticas a los alumnos son: Permite obtener multitud de ejemplos y generalizar contenidos trabajados. Muy Ãºtil en geometrÃa, estadÃstica y anÃ¡lisis. Pueden hacer mÃ¡s atractiva la asignatura y les puede motivar mÃ¡s su estudio. Facilitar los cÃ¡lculos, comprobar sus resultados, visualizar. Les hacen desarrollar la ™imaginaciÃ³n™ y capacidad de deducciÃ³n y resoluciÃ³n de problemas y teorÃas. Esto se corresponde con la idea generalizada que ya puntualizamos en la secciÃ³n anterior y que en [8] transmite el CEMAT, y es que es necesario usar utilidades informÃ¡ticas para desarrollar estructuras conceptuales en detrimento del cÃ¡lculo con lÃ¡piz y papel ya que solucionar problemas con ayuda del ordenador es un ejercicio que permite adquirir la costumbre de enfrentarse a problemas predefinidos de una forma rigurosa y sistemÃ¡tica. Como Ãºltimo apunte, algunas de las respuestas mÃ¡s interesantes de los docentes a si les gustarÃa recibir algÃºn tipo de formaciÃ³n sobre el uso de estas herramientas fueron: Geogebra, sobre la realizaciÃ³n de recursos interactivos. Geogebra y WxMaxima. SÃ, serÃa muy bueno aprender todas las funcionalidades de todas estas herramientas. SÃ, sobre Geogebra, con un curso prÃ¡ctico. SÃ. Me gustarÃa saber emplear en mis clases mÃ¡s de las herramientas digitales que ya uso. De estas respuestas se deduce el interÃ©s del profesorado en la formaciÃ³n en herramientas digitales especÃficas de las matemÃ¡ticas para su utilizaciÃ³n en el aula y que, ademÃ¡s opinan que pueden tener un importante beneficio en el alumnado. AsÃ, en la secciÃ³n 5 se propondrÃ¡ un curso formativo para docentes en las herramientas WxMaxima y Geogebra. En este caso, hemos obtenido 40 respuestas por parte del alumnado. Empezaremos localizando en los cursos a estos alumnos, el 20%percent2020 % corresponde a alumnos de 4.º de la ESO, el 52,5%percent52.552{,}5 % a alumnos de 1.º de Bachillerato y el 27,5%percent27.527{,}5 % de 2.º de Bachillerato. Para empezar, cabe destacar que todos afirmaron que las herramientas digitales son Ãºtiles, y sÃ³lo el 5%percent55 % de ellos admitiÃ³ no saber utilizarlas. Algunas de las respuestas mÃ¡s interesantes a la pregunta ¿CÃ³mo crees que las herramientas digitales (en general) pueden ayudarte en el aula o fuera de ella?, han sido: Sobre todo en la dinÃ¡mica que estableces con el profesor, teniendo en cuenta que es mucho mÃ¡s entretenida la clase y se te pasa mucho mÃ¡s rÃ¡pido. A la hora de poder hacer trabajos, estudiar o incluso informarme de cosas que no sabÃa antes que pueden ser interesantes. Creo que la comunicaciÃ³n mejorarÃa mucho entre alumnos y profesores. TambiÃ©n harÃa que las clases fueran mÃ¡s dinÃ¡micas. Ayudan a organizarte mejor, y a veces te hacen las cosas mÃ¡s fÃ¡ciles, mÃ¡s visuales. AdemÃ¡s te permite preguntar al profesor aunque no estÃ© en horario de clases. Cuando se pregunta al alumnado sobre el uso de distintas herramientas generales (ofimÃ¡tica, Kahoot!, generador de nubes de palabras, mapas conceptuales, Canva, Genially y Powtoon), se obtienen las respuestas recogidas en la Figura 4. Donde destaca especialmente el uso bastante extendido de las aplicaciones de ofimÃ¡tica y algo menor el uso de Kahoot!, ademÃ¡s de un uso prÃ¡cticamente nulo del resto de herramientas. CentrÃ¡ndonos en las herramientas digitales matemÃ¡ticas, se preguntÃ³ a los alumnos por las mismas herramientas que a los docentes, obteniendo las respuestas recogidas en la Figura 5. Siguen destacando el editor de fÃ³rmulas de Word y Geogebra, aunque sus usos sean prÃ¡cticamente nulos tal y como se recoge en la Figura 6. AsÃ, algunas de las respuestas mÃ¡s interesantes a la pregunta ¿Te gustarÃa recibir algÃºn tipo de formaciÃ³n sobre alguna de las herramientas matemÃ¡ticas?, fueron: SÃ, sinceramente de todas. Porque recientemente he visto como era la de Geogebra y, me ha parecido que estÃ¡ muy avanzada y tiene muy buenas cosas. No tengo ninguna preferencia y la formaciÃ³n deberÃa ser una breve introducciÃ³n o algo por el estilo. SÃ, como WxMaxima u otra para saber mÃ¡s sobre ellas. A travÃ©s de algÃºn seminario o vÃdeos explicativos. No me interesa en este momento. Observamos que, a pesar de haber algunas respuestas negativas a recibir algÃºn tipo de formaciÃ³n, la mayorÃa de alumnos reconociÃ³ querer formarse en este tipo de herramientas. Y, finalmente, sobre por quÃ© creen que estas les pueden ayudar en casa o en el aula, algunas de las respuestas mÃ¡s representativas fueron: Una mayor precisiÃ³n en casos de grÃ¡ficas, exactitud en cÃ¡lculos y control y organizaciÃ³n cuando se necesario manejar mucho cÃ¡lculos diferentes. A la hora de las correcciones son mÃ¡s precisas que a lo mejor hacer una representaciÃ³n grÃ¡fica en la pizarra. Haciendo ejercicios que no entiendo, para poder prepararme un examen y asÃ saber si hago bien los ejercicios. No sÃ© a los demÃ¡s, pero a mi durante la cuarentena me salvaron la evaluaciÃ³n, porque ponÃa los ejercicios y ya estÃ¡ ja ja (Pero era porque no me enteraba, lo prometo). Por tanto, a pesar de que en la respuesta anterior hubo alumnos que no querÃan formarse, en esta pregunta se aprecia una opiniÃ³n generalizada en torno a que el uso de este tipo de herramientas puede ayudar en casa y en el aula. Incluso la Ãºltima respuesta la he incluido porque hay alumnos para los que el uso de las herramientas informÃ¡ticas es circunstancial y anecdÃ³tico, y no estÃ¡ relacionado con el trabajo o el aprendizaje matemÃ¡tico. Teniendo en cuenta lo expuesto en las secciones de anÃ¡lisis de las respuestas de docentes y alumnos, podemos destacar las siguientes conclusiones importantes: Ambos colectivos consideran Ãºtil el uso de las herramientas digitales. De manera general, docentes y discentes tienen conocimientos sobre herramientas poco creativas y efectivas, como pueden ser las de ofimÃ¡tica; empieza a haber discrepancias en herramientas que necesitan un mayor grado de creatividad como pueden ser Kahoot!, Canva o Genially. El uso de dichas herramientas ofimÃ¡ticas estÃ¡ muy extendido, prÃ¡cticamente a diario por parte de docentes y alumnos, pero el resto de herramientas son usadas de manera esporÃ¡dica. Ambos grupos consideran que las redes sociales pueden ayudar a la comunicaciÃ³n docente“discente y por tanto resaltan la importancia de una conversaciÃ³n fluida y, en cierto modo, distendida. Sobre las herramientas de carÃ¡cter matemÃ¡tico, el desconocimiento es prÃ¡cticamente absoluto, salvando quizÃ¡ los casos del editor de fÃ³rmulas de Word y Geogebra. El conocimiento del primero de ellos quizÃ¡ sea debido al amplio uso que han reconocido de las herramientas de ofimÃ¡tica. El conocimiento y uso de Geogebra quizÃ¡ merezca un estudio mÃ¡s en profundidad, pero en cualquier caso es una de las herramientas mÃ¡s sencillas e intuitivas de utilizar y que tiene una dilatada experiencia en el sector educativo, por lo que muchos docentes, cada vez mÃ¡s, la utilizan en clase con distintos fines. Por ejemplo [11], estudia el uso de Geogebra como facilitador del aprendizaje en el AnÃ¡lisis, concretamente establece que Geogebra favorece la utilizaciÃ³n de las funciones de variable real como instrumento de modelizaciÃ³n y herramienta de soluciÃ³n de situaciones problemÃ¡ticas intra y extra matemÃ¡ticas, asÃ como la visualizaciÃ³n dinÃ¡mica de los comportamientos funcionales. Aunque algunas herramientas, tanto generales como matemÃ¡ticas, son conocidas, la gran mayorÃa de herramientas son, o bien desconocidas, o bien de poca utilizaciÃ³n. Esto nos lleva a preguntarnos si quizÃ¡ se deba a que no han recibido ningÃºn tipo de formaciÃ³n sobre sus diferentes usos. Ya que, en cierta medida no podemos utilizar algo que no conocemos. En esta lÃnea se investigÃ³ en el artÃculo [7] la formaciÃ³n continuada en herramientas informÃ¡ticas del profesorado de Extremadura y del Alentejo portuguÃ©s, determinando que en Extremadura [¦] la formaciÃ³n permanente del profesorado no necesita incidir tanto en la preparaciÃ³n tÃ©cnica en el campo de la informÃ¡tica cuanto en la utilizaciÃ³n didÃ¡ctica del ordenador en el contexto escolar. Es decir, tenemos profesores que entienden el uso del ordenador y de herramientas informÃ¡ticas, pero no saben cÃ³mo implementarlas en el aula. AsÃ, a modo de conclusiÃ³n, observamos que hay consenso en que las nuevas tencnologÃas estÃ¡n aquÃ para quedarse, por lo que, en nuestro caso particular, docentes y discentes entienden la importancia y el amplio abanico que pueden ofrecer herramientas informÃ¡ticas a la mejora de la calidad educativa. El software WxMaxima es un programa de software libre, esto es, gratuito, lo cuÃ¡l facilita su difusiÃ³n y utilizaciÃ³n, a diferencia de otros programas que requieren una licencia de pago. Puede descargarse para distintos sistemas operativos como Windows, Mac OS, Linux o Ubuntu, por lo que prÃ¡cticamente podremos instalarlo en cualquier ordenador e incluso tiene su versiÃ³n en Android, por lo que algunos alumnos podrÃ¡n incluso trabajar con sus dispositivos mÃ³viles. Se trata de un programa que, mÃ¡s allÃ¡ del propio cÃ¡lculo, tiene grandes implicaciones pedagÃ³gicas, como apunta Jorquera en [13, p.11-12]: Permite realizar cÃ¡lculos reales, de mayor dificultad matemÃ¡tica evitando perder tiempo en el cÃ¡lculo rutinario, con lo que se puede dedicar mÃ¡s tiempo a la explicaciÃ³n de los conceptos que a las habilidades de cÃ¡lculo. SerÃ¡ Ãºtil en algunos casos, e inÃºtil en otros. No hay que forzar su uso en todas las ocasiones ya que esto serÃa un error, pero hay contextos en los cuales el error serÃa no usarlo. Se fomenta mÃ¡s el trabajo creativo en detrimento del rutinario. Por su parte, Geogebra tiene una versiÃ³n de escritorio y una versiÃ³n en lÃnea, ambas gratuitas. AdemÃ¡s, en la pÃ¡gina de inicio de Geogebra existen un gran abanico de actividades en lÃnea y descargables para realizar en el aula con esta aplicaciÃ³n. EstÃ¡ en espaÃ±ol, lo cual es una gran ventaja para muchos docentes y alumnos. Y tiene una interfaz sencilla e intuitiva, que cualquiera puede aprender a manejarla simplemente probando sus distintas herramientas. A pesar de ello, tiene un gran nÃºmero de funciones mÃ¡s complejas para crear actividades muy interesantes. En este sentido, Geogebra mejora significativamente el aprendizaje de la capacidad de razonamiento y demostraciÃ³n, de comunicaciÃ³n matemÃ¡tica asÃ como la capacidad de resoluciÃ³n de problemas [12], asÃ como contribuyÃ³ a que mejorasen sus actitudes hacia las matemÃ¡ticas durante su uso, exhibiendo gusto, implicaciÃ³n y autoconfianza en matemÃ¡ticas [12]. Veamos, mediante un ejercicio de la prueba de acceso a la Universidad de Extremadura correspondiente al bloque de GeometrÃa de la asignatura MatemÃ¡ticas II, cÃ³mo WxMaxima y Geogebra se complementan para una resoluciÃ³n sencilla. Para ello, resolveremos el ejercicio manualmente y con ambos programas, argumentando cuÃ¡l es mÃ¡s idoneo en cada situaciÃ³n. Este ejercicio apareciÃ³ en la convocatoria ordinaria del curso 2020“2021 y su encunciado fue el siguiente: Sea el plano Î Î de ecuaciÃ³n 2x+yz2=02¥¦§202x+y-z-2=02 x + y - z - 2 = 0 y la recta rrr dada por x3=y23=z13¥3¦23§13 x 3 = / y - 2 - 3 = / z - 1 3 . Estudie la posiciÃ³n relativa de la recta respecto al plano. Calcule la distancia de la recta al plano. Debemos tener en cuenta que un punto de la recta es P=(0,2,1)ƒ0.2.1P=(0,2,1)P = ( 0,2,1 ) y su vector director es u†=(3,3,3)†33.3 u = ( 3 , - 3,3 ). Por otro lado el vector normal del plano es n†=(2,1,1)†2.11 n = ( 2,1 , - 1 ). AsÃ, como y comprobando si PƒPP pertenece, o no, a Î Î , Luego el plano y la recta son paralelos. La distancia vendrÃ¡ dada por La idea que tenemos que seguir es la misma que la resoluciÃ³n manual. En primer lugar, definimos los vectores director de la recta y normal del plano, y realizamos su producto escalar (Figura 7): Se define la funciÃ³n que nos indica si un punto pertenece o no al plano Î Î y sustituimos el punto P=(0,2,1)ƒ0.2.1P=(0,2,1)P = ( 0,2,1 ) (Figura 8): Como u†‹…n†=0‹…††0 u ‹… over† n = 0 y PÎ ƒÎ P roman_Î , se deduce que la recta rrr y el plano Î Î son paralelos, por lo que su distancia puede ser calculada como sigue (Figura 9) Y el ejercicio quedarÃa resuelto. Primero definimos el plano y la recta dados, asÃ como el punto P=(0,2,1)ƒ0.2.1P=(0,2,1)P = ( 0,2,1 ) (Figura 10). A continuaciÃ³n, trazamos la recta perpendicular al plano que pasa por PƒPP y seÃ±alamos como P²superscriptƒ²P^{ ^ ² el punto donde intersecta esta perpendicular con el plano Î Î , es decir, la proyecciÃ³n de PƒPP en Î Î (Figura 11) Para finalizar, sÃ³lo tendremos que utilizar la herramienta de distancia seÃ±alando los puntos PƒPP y P²superscriptƒ²P^{ ^ ² (Figura 12). Obteniendo el resultado aproximado de 0²4116superscript0²41160^{ ^ ² 41 / 1 square-root 6 , concluyendo asÃ el ejercicio. En este caso es clara la diferencia entre las tres resoluciones: La resoluciÃ³n manual es tediosa y falta de visualizaciÃ³n, algo clave en la parte de GeometrÃa, ya que es la parte de las matemÃ¡ticas mÃ¡s visual. La resoluciÃ³n con WxMaxima hace sencillos los cÃ¡lculos, pero de nuevo carece de visualizaciÃ³n. Bien es cierto que, ademÃ¡s, tiene un lenguaje particular y es necesario saber escribir las operaciones y funciones, ya que, de lo contrario, podemos tardar mucho en resolver un ejercicio. La resoluciÃ³n con Geogebra es eminentemente visual, de hecho, para resolver el primer apartado basta con representar ambos objetos para darnos cuenta de su posiciÃ³n relativa. MÃ¡s complicado es, sin embargo, el segundo apartado, ya que debemos conocer que la distancia entre una recta y un plano paralelos es la distancia entre un punto cualquiera de la recta hasta su proyecciÃ³n en el plano, lo cual no es tan sencillo de realizar en Geogebra ya que tendremos que calcular la perpendicular, la intersecciÃ³n y despuÃ©s la distancia. AdemÃ¡s, observamos que la distancia obtenida es una aproximaciÃ³n y no el valor exacto. Por tanto, mi conclusiÃ³n en este ejercicio es que la soluciÃ³n Ã³ptima vendrÃa dada por una mezcla de los dos programas, el primer apartado se resolverÃa fÃ¡cilmente con Geogebra mientras que para el segundo apartado emplearÃa WxMaxima. Teniendo en cuenta que a lo largo del trabajo se ha pretendido inculcar una visiÃ³n de las herramientas digitales como facilitadoras del aprendizaje, pero que tanto docentes como alumnos no conocen su funcionamiento, el autor propone un curso/seminario formativo orientado a docentes. EstarÃ¡ compuesto por cuatro sesiones y se trabajarÃ¡n los aspectos y utilidades fundamentales de WxMaxima y Geogebra recogidos en las Tablas 1, 2, 3 y 4. En total, este curso/seminario consta de 21 vÃdeotutoriales con una duraciÃ³n de dos horas y media aproximadamente. Trece de ellos estÃ¡n dedicados, entre las dos primeras sesiones, a WxMaxima, con una duraciÃ³n de 1 hora y 25 minutos aproximadamente; y ocho vÃdeos, los correspondientes a las sesiones tercera y cuarta, dedicados a Geogebra, con una duraciÃ³n aproximada de una hora y cinco minutos. Como hemos venido haciendo hincapiÃ©, su objetivo es brindar a los docentes las herramientas y utilidades bÃ¡sicas de estos dos programas para que puedan adaptar los ejercicios del dÃa a dÃa en el aula e implementar el uso de las herramientas digitales para la resoluciÃ³n y visualizaciÃ³n de contenidos matemÃ¡ticos, asÃ como de la mejora de la asimilaciÃ³n de los conceptos y una mejora de la predisposiciÃ³na a aprender matemÃ¡ticas. A modo de conclusiÃ³n, podemos extraer algunas de las ideas fundamentales que se han desarrollado a lo largo del trabajo: En primer lugar, el aprendizaje del contenido matemÃ¡tico y de procesos de resoluciÃ³n de problemas no estÃ¡ reÃ±ido con el hecho de aprender el uso de herramientas digitales. Esto se debe a que ambos mÃ©todos de resoluciÃ³n se complementan. Hemos visto, dentro de los ejercicios resueltos, que un programa puede servir mejor que el otro para hacer cuentas o para representar, pero en cualquiera de los dos programas, se ha de tener una visiÃ³n general de la parte teÃ³rica para poder seguir el hilo argumental del problema y obtener la soluciÃ³n. Esto puede ayudar a la autocorrecciÃ³n de los ejercicios que, habitualmente, los alumnos tienen como tarea para casa; tambiÃ©n puede ayudar a que alumnos con altas capacidades indaguen e investiguen en mayor profundidad, buscando nuevos retos y problemas mÃ¡s complicados; e incluso, puede ayudar, al aprendizaje por descubrimiento o al just in time teaching, es decir, darle respuestas al alumno en el momento en que se haga las cuestiones, por lo que los programas pueden servir para dejarles probar y, como docentes, les ayudaremos en sus procesos de descubrimiento. En segundo lugar, creo que hay mucho camino por delante en cuanto a la implantaciÃ³n de las nuevas tecnologÃas en los centros educativos. Aunque es verdad que cada vez se invierte mÃ¡s en adaptar las aulas a las nuevas tecnologÃas (pizarras digitales, proyectores o aulas virtuales), no podemos decir lo mismo sobre la inversiÃ³n en la formaciÃ³n de los docentes en el uso de estas herramientas. Esto implica que las nuevas tecnologÃas pueden ser en ciertos casos, vistas como una fuente adicional de estrÃ©s en el profesorado, por ejemplo ante la posibilidad de que el docente tenga preparada una clase y, el dÃa que pretenda impartirla, no funcione internet; o que las herramientas sean vistas mÃ¡s como un elemento extraÃ±o que como un medio educativo. Tampoco los docentes conocen todas las funcionalidades que estas les ofrecen, por tanto, muchos de ellos seguirÃ¡n con sus rutinas sin saber los beneficios de dichas herramientas. Como hemos dicho, hay mucho camino por recorrer, lo cual es bueno, prÃ¡cticamente cualquier iniciativa formativa serÃ¡ bien acogida por la comunidad educativa, tanto de formaciÃ³n al profesorado como de formaciÃ³n al alumnado, ya que ambos colectivos observan que las herramientas digitales tienen un gran potencial, pero no saben cÃ³mo aprovecharlo. Por Ãºltimo, teniendo en cuenta estas conclusiones, podemos proponer las siguientes lÃneas de investigaciÃ³n para las que serÃ¡ necesario un anÃ¡lisis mÃ¡s detallado de las diferencias en el uso y en la concepciÃ³n de las herramientas digitales en la educaciÃ³n secundaria: Lo primero en lo que deberÃamos pensar es en realizar una encuesta a un espacio muestral mÃ¡s amplio y elegido considerando ciertas variables (formaciÃ³n inicial del profesorado, titulaciÃ³n de acceso al cuerpo de profesores de secundaria, aÃ±os de experiencia como docente, contexto socioeconÃ³mico del centro, etc.), con el objetivo de poder obtener conclusiones mÃ¡s significativas para una poblaciÃ³n mayor. Una vez realizado el curso introductorio de las herramientas digitales WxMaxima y Geogebra, realizar un seguimiento, por ejemplo, tomar dos grupos de alumnos que cursen la misma asignatura de matemÃ¡ticas en el mismo curso. A uno de estos grupos se le asignarÃ¡ un profesor que no haya realizado el curso introductorio y seguirÃ¡ una metodologÃa sin TIC. Al otro grupo, se le asignarÃ¡ un profesor que sÃ haya realizado el curso, que haya intentado introducir estas herramientas a lo largo de las explicaciones. Finalmente, se compararÃ¡ si el uso de las herramientas digitales ha influido no sÃ³lo en los resultados, sino en la aceptaciÃ³n de la asignatura de matemÃ¡ticas, en la motivaciÃ³n con que van a clase y en la satisfacciÃ³n y la percepciÃ³n de aprendizaje de los alumnos. Por Ãºltimo, en caso de que lo anterior refleje un aumento significativo de los resultados acadÃ©micos y la motivaciÃ³n del alumno por la asignatura, cabrÃa plantear un curso de material mÃ¡s avanzado con el que puedan seguir profundizando en el uso de WxMaxima y Geogebra los docentes que hicieron el primer curso. AsÃ, de nuevo, tras un dominio mÃ¡s avanzado de las herramientas podrÃamos hacer un estudio comparativo entre ambos grupos, uno con un docente que tenga un menor dominio, que solo haya realizado el primer curso, y otro con un profesor que haya realizado el curso avanzado.",
        "keywords": ""
    },
    {
        "id": 8,
        "title": "Algorithms for constrained optimal transport",
        "abstract": "AbstractWe derive iterative scaling algorithms of the Sinkhorn-Knopp (SK) type for constrained optimal transport. The constraints are in the form of prior-imposed zeroes in the transport plan. Based on classical Bregman arguments, we prove asymptotic convergence of our algorithms to a unique optimal solution. New insights obtained from the convergence proof are highlighted. An example from electrical vehicle charging in a smart city context is outlined, in which the prior zero-constraints prevent energy from being transported from some providers to some vehicles.",
        "corpus": "1 We derive iterative scaling algorithms of the Sinkhorn-Knopp (SK) type for constrained optimal transport. The constraints are in the form of prior-imposed zeroes in the transport plan. Based on classical Bregman arguments, we prove asymptotic convergence of our algorithms to a unique optimal solution. New insights obtained from the convergence proof are highlighted. An example from electrical vehicle charging in a smart city context is outlined, in which the prior zero-constraints prevent energy from being transported from some providers to some vehicles. Optimal Transport (OT) has emerged as a key enabling mathematical technology which is driving a growing number of contemporary engineering applications in fields such as machine learning, image processing, and in optimization and control [1], [2], [3]. The field is an old one, dating back to Monge in the 18th century. Nevertheless, OT is now attracting accelerated attention from both theoreticians and practitioners alike, with widespread application in engineering practice. OT refers to a class of problems in which we seek to transport a finite resource (e.g. mass) from a distributed source to a distributed target, in some optimal manner. For example, the classical problems of Monge and Kantorovitch relate to the transportation of a distributed pile of sand [4] from one location to another. The contemporary analogues of these formulations (for example, in economics) arise in resource (re)allocation problems, and in supply-demand matching problems [5]. Closely related problems arise in key machine learning (ML) settings, such as domain adaptation. Here, source data are optimally adapted (i.e. transported) for use in a related, but distinct, target domain [6]. An example is data repair, in which OT is used to de-bias ML tasks in order to improve their fairness properties. Classifiers and recommender systems”trained on data with fairness deficiencies such as representation bias, dependence on protected attributes, etc.”can be fairness-repaired using methods of OT [7]. In this paper, we confine ourselves to the discrete case, in which the resource (mass) is distributed across a finite (though typically large) number of states in both the source and target. For every pair of source-target states, the designer specifies a cost per unit mass of transporting the resource from this source state to this target state. The OT problem then involves the design of a transport plan that specifies the amount of resource to be transported for every one of these source-target state pairs, in a way that minimizes the total cost of transport. A feature of our setting of the OT problem in this paper is that we impose constraints in which some source states are not allowed to transport to some target states. These prior zero-transport constraints may arise for a variety of reasons. For example, in economics, certain producers may not be allowed to supply a product to certain consumers. In fairness-aware domain adaptation for ML (above), data for a particular demographic group may not be mapped to a different demographic group. These OT problems give rise to linear programs of very large scale [8]. Linear programming solvers are known to scale poorly, and much of the current OT work focuses on regularizing the classical OT problem to ensure tractability at scale. The best known of these approaches is the entropic regularisation of the OT problem [8]. In essence, this amounts to the addition of an extra term in the (total cost) objective, which penalizes a specified divergence of the transport plan from a prior-defined ideal plan [9]. The resulting optimization problem is then strongly convex on a compact convex set, and can be solved efficiently using a Sinkhorn-Knopp-type (SK) iterative scaling algorithm, with its favourable large-scale computational properties [10], [11]. These computational OT results and guarantees”based on the SK algorithm” have been central to the widespread adoption of the OT paradigm in applications, particularly in ML [4], [6], [12], [13]. In contrast to previous results in the literature, our purpose in this paper is to consider OT problems in which some elements of the transport plan are forced to be zero a priori, and to derive iterative scaling algorithms of the SK-type to solve these problems. We use Bregman-type arguments [14] to prove convergence of our algorithms to the (unique) solution of the OT problems under consideration. We outline an application of our results in a smart cities context, where prior zeroes in the transport plan are an important constraint. OT problems are often cast in the language of probability, where the resource (mass) to be transported is a probability measure, and so the net resource (mass) is normalized to one. The transport plan is thus a bivariate probability mass function (pmf), and the source and target distributions are its two marginals. We speak of a transport plan that moves the source distribution (marginal) to the target distribution (marginal). In this way, OT can be viewed as a mathematical framework for measuring the distance between (source and target marginal) probability distributions, this distance being the minimal expected cost associated with the optimal transport plan [4]. This Kantorovich-Rubinstein (also called Wasserstein or earth-mover™s) norm [4] for classical entropy-regularized OT gives rise to a unique solution of a strongly convex optimization problem on a compact convex support [15], [16], with linear convergence of the SK iterative scaling algorithm to this unique solution [17]. If, as in this paper, we impose prior zero-constraints on some elements of the transport plan, there may not exist a plan that satisfies exact marginal constraints at the source and target, as already noted. Another situation in which a transport plan satisfying exact marginal constraints does not exist is unbalanced OT (UOT) [16], in which the prescribed net masses of the source and target are not the same. Earlier, in [18], an SK-type iterative scaling algorithm was derived for unconstrained UOT (i.e. without prior zero-constraints on the transport plan), by relaxing the source and target marginal constraints via extra terms in the cost that penalize noncompliance with the marginal constraints. This was shown to converge linearly to a unique solution. In [16], the authors study the UOT case with prior zero-constraints on the transport plan. They prove that the SK algorithm has two convergent subsequences in this case and they propose a transport plan which is the component-wise geometric mean of the limits plans of the two convergent subsequences The paper is organized as follows: in Section III, our OT problem is specified with prior constraints on the pattern of the transport plan. After developing background material, two iterative scaling algorithms are presented in Section IV for OT problems with prior zero-constraints. Section V presents some properties of the resulting OT plans. Section VI provides the proof of convergence or our algorithms. Finally, a use-case from the domain of smart mobility is outlined in Section VII, illustrating the efficacy of our new algorithms. Consider an OT problem in which we have a finite number, mmm, of source agents (i.e. source states), i=1,2,¦,m12¦i=1,2, = 1 , 2 , ¦ , m, and a finite number, nnn, of target agents (i.e. target states), j=1,2,¦,n12¦j=1,2, = 1 , 2 , ¦ , n. Suppose each source agent, iii, has capacity or mass, u~i>0subscript~0 u _ i > 0, and each target agent, jjj, desires a capacity or mass, v~j>0subscript~£0 v _ j > 0. We wish to transport the masses of the source agents to the target agents. For the time being, we impose mass balance (conservation), i.e. that is, ²u~=²v~superscript1²~superscript1²~£{ ^ ² over~ u = 1 ^ ² over~ v where 1{ is a vector of ones, of appropriate length. Let tij¥0subscript¡0t_{ij} 0t _ i j ¥ 0 be the mass or capacity that source agent, iii, transports to target agent, jjj. We will refer to the mÃnm nm Ã n matrix, T={tij}subscript¡T= = { t _ i j }, as the transport plan. As a consequence of the capacity constraints of the source and target agents, we must have or, T=u~1~T{ 1 = over~ u , and or, T²=v~superscript²1~£T^{ ^ ² 1 = over~ v . Note that constraints (1), (2) and (3) imply that Next, suppose that a specific source agent, iii, never transports to a specific target agent, jjj, so that tij=0subscript¡0t_{ij}=0t _ i j = 0. We let µµ be the set of these prior non-transporting index pairs: The set of allowable transport plans is given by We assume that for every iii, that there is at least one (i,j)µµ(i,j) i , j ) caligraphic_Z and for every jjj, there is also at least one (i,j)µµ(i,j) i , j ) caligraphic_Z; i.e. if tij>0subscript¡0t_{ij}>0t _ i j > 0 for all (i,j)µµ(i,j) i , j ) caligraphic_Z then, TTT has no zero-rows or zero-columns. If µµ is empty, i.e. if every source agent can transport to every target agent, then the marginal constraints (2) and (3) are feasible (i.e. there is at least one solution in ¯¯ for example, T¡u~v~²/²v~~superscript~£²superscript1²~£T ¡ over~ u over~ v ^ ² / 1 ^ ² over~ v ). However, if µµ is non-empty, then (2) and (3) may not be feasible (i.e. there may not be any solution in ¯¯ An example is given by m=n=22m=n=2m = n = 2 and µ¡{(2,2)}µ22 ¡ { ( 2 , 2 ) }: Suppose that there is a cost, cijsubscriptc_{ij} _ i j R, of moving a unit mass from source, iii, to target, jjj. Then, the total cost of transport is (i,j)µcijtijsubscriptµsubscriptsubscript¡ _ ( i , j ) caligraphic_Z c _ i j t _ i j This gives rise to the constrained OT problem: Next, suppose that it is also desirable for TTT to be close to some ideal (desired) transport plan, T~¯~¯ T caligraphic_T (8) with t~ij>0subscript~¡0 t _ i j > 0 for (i,j)µµ(i,j) i , j ) caligraphic_Z. We therefore modify the cost to where Î³0>0subscript¾00 _ 0 > 0 is a pre-assigned regularization constant, and the Kullback-Leibler (KL¾¿KLK L) divergence [9] of TTT from T~~ T is defined as where, for any two scalar, t¥0¡0t 0t ¥ 0 and t~>0~¡0 t > 0, we define One may readily show that, for all t¥0¡0t 0t ¥ 0 and t~>0~¡0 t > 0, kl(t|t~)¥0™conditional¡~¡0kl(t| 0k l ( t | over~ t ) ¥ 0; also kl(t|t~)=0™conditional¡~¡0kl(t| l ( t | over~ t ) = 0 iff t=t~¡~¡t= = over~ t . Hence, for all T¯¯T caligraphic_T, KL(T|T~)¥0¾¿conditional~0KL(T| 0K L ( T | over~ T ) ¥ 0 and KL(T|T~)=0¾¿conditional~0KL(T| L ( T | over~ T ) = 0 iff T=T~~T= = over~ T . Note again that original optimization problem (9) is a linear programming problem and may be computationally burdensome to solve. So, a common solution approach is approximately to solve the original problem (9) by minimizing the regularized cost in (11) with Î³0subscript¾0 _ 0 small but postive. This problem can be solved more efficiently than the original problem. Next, note that cijtij=Î³0tijlog¡(1exp¡(cij/Î³0))subscriptsubscript¡subscript¾0subscript¡1subscriptsubscript¾0c_{ij}t_{ij}= _ i j t _ i j = Î³ _ 0 t _ i j roman_log ( / 1 roman_exp ( - c _ i j / Î³ _ 0 ) ) and define Then, whenever tij 0subscript¡0t_{ij} 0t _ i j 0, (16) also holds for tij=0subscript¡0t_{ij}=0t _ i j = 0 , (i,j)µµ(i,j) i , j ) caligraphic_Z. Hence, cost (11) can be expressed as Î³0KL(T|K)+(i,j)µÎ³0(t~ijkij)subscript¾0¾¿conditional¾subscriptµsubscript¾0subscript~¡subscript _ 0 K L ( T | K ) + _ ( i , j ) caligraphic_Z Î³ _ 0 ( over~ t _ i j - k _ i j ). Therefore, we consider the new optimization problem: where °(u~,v~)°~~£ ( over~ u , over~ v ) is given by (10). When the optimization problem (17) is feasible (i.e. °(u~,v~)°~~£ ( over~ u , over~ v ) is non-empty), its solution can be obtained using the Sinkhorn-Knopp (SK) Algorithm [17]. Sinkhorn-Knopp (SK) Algorithm. Initialize d2j(0)=1subscript201d_{2j}(0)=1d _ 2 j ( 0 ) = 1. Iterate for l=0,1,¦™01¦l=0,1, = 0 , 1 , ¦ ¡¡{ 220.50885pt} If (17) is feasible, the sequence, {T(l)}™ T ( l ) } converges to a limit, TsuperscriptT^{*}T ^ , which is the unique minimizer for (17) [16]. If tij=0subscriptsuperscript¡0t^{*}_{ij}=0t ^ _ i j = 0 for some (i,j)µµ(i,j) i , j ) caligraphic_Z, then either {dii(l)}subscript™ d _ i i ( l ) } or {d2j(l)}subscript2™ d _ 2 j ( l ) } do not converge. If (17) is not feasible, the sequence, {T(l)}™ T ( l ) } has two convergent subsequences with different limits [16]. If µµ is non-empty, it may not be possible to simultaneously satisfy the marginal constraints, (2) and (3). If”as already imposed in (8)”TTT has no enforced rows of zeroes, the row constraints can always be achieved, as explained in Remark 2, below. Therefore, consider the situation in which row constraints (2) are satisfied but column constraints (3) are not necessarily satisfied. Instead of requiring satisfaction of the column constraints, we add a KL-based penalty term to the cost function, which is a measure of non-compliance with the column constraints, v~~£ v . We therefore consider a new optimization problem given by where KL(vT|v~)=j=1nkl(vTj|v~j)¾¿conditionalsubscript£~£superscriptsubscript1™conditionalsubscript£subscriptsubscript~£KL(v_{T}| L ( v _ T | over~ v ) = _ j = 1 ^ n k l ( v _ T _ j | over~ v _ j ), vTj¡i=1mtijsubscript£subscriptsuperscriptsubscript1subscript¡v_{T_{j}}{ _ T _ j ¡ _ i = 1 ^ m t _ i j , Î³>0¾0 > 0, For any admissible u~~ u , one can always obtain a TTT in ±(u~)±~ ( over~ u ) with tij>0subscript¡0t_{ij}>0t _ i j > 0 for all (i,j)µµ(i,j) i , j ) caligraphic_Z. Simply choose any T¯¯T caligraphic_T with tij>0subscript¡0t_{ij}>0t _ i j > 0 for all (i,j)µµ(i,j) i , j ) caligraphic_Z, and scale each of its rows to yield a new matrix in ±(u~)±~ ( over~ u ). Therefore, ±(u~)±~ ( over~ u ) is non-empty, and since we are minimizing a continuous strictly convex function (21) over a compact convex set (22), a unique minimizer exists. ¡¡{ 220.50885pt} [19] considers a generalization of the problem considered here. To highlight the novelty of our work we make the following comments. It is true that our problem is a special case of the initial general problem considered in [19] by, among other things, specializing to the bivariate (so, bi-marginal) case, and taking cij=subscriptc_{ij}= _ i j = for all entries in the transport plan that are to be zero. However, our algorithms”which have been designed, among other things, to be relevant in important applications of OT, such as the sharing economy application of our Section VII”are not presented in [19]. More substantially, we note the following difference. In order to prove convergence of their algorithm to solve their general problem, it is assumed that all the elements of their cost tensor ‚‚ are finite (Assumption C on page 5, left column, of [19]). Under this assumption, our problem is not a special case of the problem solved by the algorithm in [19], because for our problem to be a special case, some of the elements of their cost tensor ‚‚ must be infinite. This assumption is used in the proof of Lemma III.8 in [19]. Note also that prior zeros are not imposed on their transport tensor, MMM (corresponding to our transport matrix, TTT). We now present the main results of the paper. These results yield iterative scaling algorithms that produce a sequence, {T(l} T ( l }, converging to the optimal transport plan, TsuperscriptT^{*}T ^ , for (21). In particular, two algorithms are presented. Algorithm 1 is obtained using results in [14]. Algorithm 2 is equivalent to Algorithm I and is presented for comparison to other related algorithms such as the SK Algorithm. A third algorithm, the Chizat Algorithm is a related algorithm from the literature that is included to provide context for our contributions. Algorithm 1. Initialize T(0)=K,v(0)=v~formulae-sequence0¾£0~£T(0)=K,v(0)= ( 0 ) = K , v ( 0 ) = over~ v . Iterate for l=0,1,¦™01¦l=0,1, = 0 , 1 , ¦ ¡¡{ 220.50885pt} The following theorem provides the main result of this paper. A proof is provided in Section VI. Consider the sequence, {T(l),v(l)}™£™ T ( l ) , v ( l ) }, generated by Algorithm 1. This sequence converges to the (unique) limit, (T,v)superscriptsuperscript£(T^{*},v^{*})( T ^ , v ^ ), and TsuperscriptT^{*}T ^ is the minimizer for the optimization problem given by (21). Since from (23), liml†c1i(l)=1subscript†™subscript1™1 _ l † c _ 1 i ( l ) = 1 for all iii. In Lemma 2 (see Section V), it will be shown that tij>0subscriptsuperscript¡0t^{*}_{ij}>0t ^ _ i j > 0 for all (i,j)µµ(i,j) i , j ) caligraphic_Z. Hence, TsuperscriptT^{*}T ^ is guaranteed not to contain any row of zeroes, and it follows from (25) that liml†c2j(l)=1subscript†™subscript2™1 _ l † c _ 2 j ( l ) = 1 for all jjj. From (24), we see that and, since TsuperscriptT^{*}T ^ does not contain any column of zeroes, we must have vj>0subscriptsuperscript£0v^{*}_{j}>0v ^ _ j > 0, jfor-all j j. It now follows, from (27) and (28), that j=1nvj=i=1mu~isuperscriptsubscript1superscriptsubscript£superscriptsubscript1subscript~ _ j = 1 ^ n v _ j ^ = _ i = 1 ^ m over~ u _ i and so vsuperscript£v^{*}v ^ and u~~ u are balanced (i.e. mass-conserving). The above result holds even in the unbalanced (i.e. non-mass-conserving) problem, i=1mu~i j=1nv~jsuperscriptsubscript1subscript~superscriptsubscript1subscript~£ _ i = 1 ^ m over~ u _ i _ j = 1 ^ n over~ v _ j . In this case, application of the SK algorithm produces a sequence, {T(l)}™ T ( l ) }, which has two convergent subsequences with different limits [16]. ¡¡{ 220.50885pt} In order to obtain an algorithm for comparison with the SK algorithm, we introduce scaling parameters, d1i(l)subscript1™d_{1i}(l)d _ 1 i ( l ) and d2j(l)subscript2™d_{2j}(l)d _ 2 j ( l ), defined by Then The sequence, {T(l)}™ T ( l ) }, obtained from Algorithm 1, can then also be obtained from the following algorithm. Algorithm 2. Initialize d2j(0)=1.subscript201d_{2j}(0)=1.d _ 2 j ( 0 ) = 1 . Iterate for l=0,1,¦™01¦l=0,1, = 0 , 1 , ¦ ¡¡{ 220.50885pt} Since v~j,vj>0subscript~£subscriptsuperscript£0 v _ j , v ^ _ j > 0 for all jjj, it follows from (32) that the sequence {d2j(l)}subscript2™ d _ 2 j ( l ) } has a limit d2jsubscriptsuperscript2d^{*}_{2j}d ^ _ 2 j which is non-zero for all jjj. Now (31) implies that that the sequence {d1i(l)}subscript1™ d _ 1 i ( l ) } has a limit d1isubscriptsuperscript1d^{*}_{1i}d ^ _ 1 i for all iii. Moreover (33) and (34) show that these limits satisfy and (35) results in ¡¡{ 220.50885pt} If one considers the limit, as Î³††¾ † , in Algorithm 2, one obtains the SK algorithm [17], whose sequence, {T(l)}™ T ( l ) }, converges to the minimizer for optimization problem (17), provided this problem is feasible. ¡¡{ 220.50885pt} Before proceeding we note that [18] considers a problem which is related (but different from) to the problem considered here. Namely, to solve where uTi=j=1ntijsubscriptsubscriptsuperscriptsubscript1subscript¡u_{T_{i}}= _ T _ i = _ j = 1 ^ n t _ i j for all iii, kij=exp¡(cij/Î³0) for all (i,j)subscriptsubscriptsubscript¾0 for all k_{ij}= for all }(i,j)k _ i j = roman_exp ( - c _ i j / Î³ _ 0 ) for all ( i , j ) and Î³1,Î³2>0subscript¾1subscript¾20 _ 1 , Î³ _ 2 > 0. In [18], neither marginal constraint is enforced and none of the elements of TTT are constrained to be zero. This is in contrast to the current paper, in which the source marginal is constrained to be u~~ u , and some of the elements of TTT are enforced to be zero, specifically tij=0subscript¡0t_{ij}=0t _ i j = 0 for (i,j)µµ(i,j) i , j ) caligraphic_Z. [18] shows that the sequence, {T(l)}™ T ( l ) }, generated by the following algorithm converges to a limit which is the minimizer for optimization problem (38). Chizat Algorithm. Initialize d2j(0)=1.subscript201d_{2j}(0)=1.d _ 2 j ( 0 ) = 1 . Iterate for l=0,1,¦™01¦l=0,1, = 0 , 1 , ¦ ¡¡{ 220.50885pt} Remarkably, if one considers the limit as Î³1††subscript¾1 _ 1 † , then the Chizat algorithm reduces to our Algorithm 2 in the case where there are no constraints on TTT and all the elements of T~~ T equal one. If TsuperscriptT^{*}T ^ is a minimizer for (21), then tij>0subscriptsuperscript¡0t^{*}_{ij}>0t ^ _ i j > 0 for all (i,j)µµ(i,j) i , j ) caligraphic_Z. Suppose that TsuperscriptT^{*}T ^ is a minimizer for (21). From Remark 2, there is a T^±(u~)^±~ T caligraphic_V ( over~ u ) with t^ij>0subscript^¡0 t _ i j > 0 for all (i,j)µµ(i,j) i , j ) caligraphic_Z. Since ±(u~)±~ ( over~ u ) is convex, (1Î»)T+Î»T^1†superscript†^(1- 1 - Î» ) T ^ + Î» over^ T is in ±(u~)±~ ( over~ u ) for all Î»[0,1]†01 [ 0 , 1 ]. Also, there are bounds, Î²1subscript½1 _ 1 and Î²2subscript½2 _ 2 , such that, if tij>0subscriptsuperscript¡0t^{*}_{ij}>0t ^ _ i j > 0 then for any (i,j)µµ(i,j) i , j ) caligraphic_Z, 0<Î²1¤tij¤Î²20subscript½1subscript¡subscript½20< t_{ij} < Î² _ 1 ¤ t _ i j ¤ Î² _ 2 for T=(1Î»)T+Î»T^ and Î»[0,1]1†superscript†^ and †01T=(1- and } = ( 1 - Î» ) T ^ + Î» over^ T and Î» [ 0 , 1 ]. The function to be minimized in (21) can be expressed as Consider any Î»(0,1]†01 ( 0 , 1 ] and T=(1Î»)T+Î»T^1†superscript†^T=(1- = ( 1 - Î» ) T ^ + Î» over^ T . By the mean value theorem, ƒ Î»¯(0,Î»)¯†0† Î» ( 0 , Î» ) such that where T¯=(1Î»¯)T+Î»¯T^¯1¯†superscript¯†^ T = ( 1 - under¯ Î» ) T ^ + under¯ Î» over^ T and If tij>0subscriptsuperscript¡0t^{*}_{ij}>0t ^ _ i j > 0 then, for all Î»(0,1]†01 ( 0 , 1 ], we have 0<Î²1¤t¯ij¤Î²20subscript½1subscript¯¡subscript½20< < Î² _ 1 ¤ under¯ t _ i j ¤ Î² _ 2 , and 0<Î²1¤l=1mt¯lj¤Î²30subscript½1superscriptsubscript™1subscript¯¡™subscript½30< < Î² _ 1 ¤ _ l = 1 ^ m under¯ t _ l j ¤ Î² _ 3 for some Î²3subscript½3 _ 3 . Hence ‚f‚tij(T¯)(t^ijtij)¤Î³ij“subscript¡¯subscript^¡subscriptsuperscript¡subscript¾ f}{ t_{ij}}( ‚ f ‚ t _ i j ( under¯ T ) ( over^ t _ i j - t ^ _ i j ) ¤ Î³ _ i j for some Î³ijsubscript¾ _ i j . Suppose that tij=0subscriptsuperscript¡0t^{*}_{ij}=0t ^ _ i j = 0 for some (i,j)µµ(i,j) i , j ) caligraphic_Z. Then limÎ»†0t¯ij=tij=0subscript††0subscript¯¡subscriptsuperscript¡0 0} _ Î» † 0 under¯ t _ i j = t ^ _ i j = 0; and limÎ»†0‚f‚tij(T¯)(t^ijtij)=subscript††0“subscript¡¯subscript^¡subscriptsuperscript¡ 0} f}{ t_{ij}}( _ Î» † 0 / ‚ f ‚ t _ i j ( under¯ T ) ( over^ t _ i j - t ^ _ i j ) = - . This implies that, for Î»>0†0 > 0 sufficiently small, (i,j)µ‚f‚tij(T¯)(t^ijtij)<0subscriptµ“subscript¡¯subscript^¡subscriptsuperscript¡0 f}{ t_{ij}}( ( _ ( i , j ) caligraphic_Z / ‚ f ‚ t _ i j ( under¯ T ) ( over^ t _ i j - t ^ _ i j ) < 0 which along with (42) yields the contradiction, f(T)<f(T).““superscriptf(T)<f(T^{*}).f ( T ) < f ( T ^ ) . The following result can be obtained from the discussion of Algorithm 2; see Remark 5. However we wish to provide a proof which is independent of any algorithm. A matrix, TsuperscriptT^{*}T ^ , solves the OT problem given by (21) iff ƒ positive scalars, d11,¦,d1msubscript11¦subscript1d_{11}, _ 11 , ¦ , d _ 1 m and d21,¦,d2nsubscript21¦subscript2d_{21}, _ 21 , ¦ , d _ 2 n , such that, for all (i,j)(i,j)( i , j ) (37) and (36) hold. If (i,j)µµ(i,j) i , j ) caligraphic_Z, we have tij=kij=0subscriptsuperscript¡subscript0t^{*}_{ij}=k_{ij}=0t ^ _ i j = k _ i j = 0 and the expression for tijsubscriptsuperscript¡t^{*}_{ij}t ^ _ i j in (37) holds. The Lagrangian associated with this optimization problem is For (i,j)µµ(i,j) i , j ) caligraphic_Z and tij 0subscript¡0t_{ij} 0t _ i j 0, From Lemma 1, we know that tij>0subscriptsuperscript¡0t^{*}_{ij}>0t ^ _ i j > 0 for all (i,j)µµ(i,j) i , j ) caligraphic_Z. Hence TsuperscriptT^{*}T ^ is a minimizer iff ƒ scalars, Î±1,¦,Î±msubscript¼1¦subscript¼ _ 1 , ¦ , Î± _ m , such that ‚L‚tij(T,Î±)=0¿subscript¡superscript¼0 L}{ t_{ij}}(T^{*}, ‚ L ‚ t _ i j ( T ^ , Î± ) = 0 for all (i,j)µµ(i,j) i , j ) caligraphic_Z, that is, log¡(tijkij)=Î±iÎ²jsubscriptsuperscript¡subscriptsubscript¼subscript½ ( / t ^ _ i j k _ i j ) = - Î± _ i - Î² _ j , where Hence where d1i¡exp¡(Î±i)subscript1subscript¼d_{1i} _ 1 i ¡ roman_exp ( - Î± _ i ) and d2j¡exp¡(Î²j)subscript2subscript½d_{2j} _ 2 j ¡ roman_exp ( - Î² _ j ). Using the row constraints, we have u~i=j=1mtij=j=1md1ikijd2jsubscript~superscriptsubscript1subscriptsuperscript¡superscriptsubscript1subscript1subscriptsubscript2 u _ i = _ j = 1 ^ m t ^ _ i j = _ j = 1 ^ m d _ 1 i k _ i j d _ 2 j . Hence, d1i=u~ij=1mkijd2jsubscript1subscript~superscriptsubscript1subscriptsubscript2d_{1i}= _ 1 i = / over~ u _ i _ j = 1 ^ m k _ i j d _ 2 j . It follows, from (44) and (45), that that is, d2j1+Î³Î³i=1md1ikij=v~jsuperscriptsubscript21¾¾superscriptsubscript1subscript1subscriptsubscript~£d_{2j}^{ _ 2 j ^ / 1 + Î³ Î³ _ i = 1 ^ m d _ 1 i k _ i j = over~ v _ j , or To prove Theorem 1, we need a result from [14]. Let and, for a fixed x~³~¥³ x caligraphic_X, consider the strictly convex function, f:³¯†:“†¯³f: : over¯ caligraphic_X † R, given by where ³¯¯³ caligraphic_X is the closure of ³³ that is Also, let AimiÃqsubscriptsuperscriptsubscriptA_{i} q}A _ i R ^ m _ i Ã q and bimisubscriptsuperscriptsubscriptb_{i} _ i R ^ m _ i , for i=1,2,¦,N12¦i=1,2, = 1 , 2 , ¦ , N, for some positive integers, NNN and misubscriptm_{i}m _ i , and consider the following optimization problem: Let isubscript _ i be the closed convex set, {xq:Aix=bi}conditional-set¥superscriptsubscript¥subscript x R ^ q : A _ i x = b _ i }, and assume that ¡‹‚i=1Nisuperscriptsubscript1subscript ¡ ‹‚ _ i = 1 ^ N caligraphic_C _ i is non-empty. We also require the following assumption. For each i=1,2,¦,N12¦i=1,2, = 1 , 2 , ¦ , N and x³¥³x caligraphic_X, ƒ y³©isuperscript¦³subscripty^{*} ^ caligraphic_X © caligraphic_C _ i such that Note that, in this assumption, optimization is over ³©i³subscript © caligraphic_C _ i and not over ³¯©i¯³subscript caligraphic_X © caligraphic_C _ i . We will denote the point ysuperscript¦y^{*}y ^ above by Pi(x)subscriptƒ¥P_{i}(x)P _ i ( x ) and refer to it as the KL¾¿KLK L-projection of x¥xx onto isubscript _ i . Let ppp be a permutation on {1,2,¦,N}12¦ 1 , 2 , ¦ , N }, that is where pksuperscriptp^{k}p ^ k is the application of ppp, kkk times. The following result may be gleaned from [14]. Suppose ©³¯¯³ © over¯ caligraphic_X is non-empty, that Assumption 1 holds, and where il+1=p(il)subscript™1subscript™i_{l+1}=p(i_{l})i _ l + 1 = p ( i _ l ). Then, liml†x(l)=x©³¯subscript†™¥™superscript¥¯³ _ l † x ( l ) = x ^ caligraphic_C © over¯ caligraphic_X . Moreover, if where ² denotes transpose, then The algorithm in Theorem 2 initially chooses some point, x(0)³¥0³x(0) ( 0 ) caligraphic_X. It then cycles indefinitely through each index, iii, and projects onto isubscript _ i . It can be viewed as an alternating projection algorithm. The resulting sequence, {x(l)}l=0superscriptsubscript¥™™0 x ( l ) } _ l = 0 ^ , converges to a point which is common to all of the sets, isubscript _ i and ³¯¯³ caligraphic_X . In addition, if f(x(0))“¥0 f(x(0)) f ( x ( 0 ) ) satisfies (50), then xsuperscript¥x^{*}x ^ is a minimizer for optimization problem (47). This algorithm is very useful when one can readily solve the optimization problems in (48). The optimization problem of this paper (21) can be rewritten as min(T,v)¯±¯¡J(T,v)subscript£¯¯±½£ _ ( T , v ) over¯ caligraphic_T caligraphic_V J ( T , v ), subject to where Î³>0¾0 > 0 and Let q=nT+nsubscriptq=n_{T}+nq = n _ T + n, where nTsubscriptn_{T}n _ T is the number of index pairs (i,j)(i,j)( i , j ) not in µµ Then, by appropriate definition of x¥xx in qsuperscript ^ q , one can associate each element of x¥xx to an element of TTT or Î³v¾£ vÎ³ v. We denote this by x=vec(T,Î³v)¥vec¾£x= v)x = vec ( T , Î³ v ) and the objective function in (53) can be written as f(x)=KL(x|x~)“¥¾¿conditional¥~¥f(x)=KL(x| ( x ) = K L ( x | over~ x ), where x~=vec(K,Î³v~)~¥vec¾¾~£ x = vec ( K , Î³ over~ v ). Also, the constraints in (52) can be expressed as A1x=b1,A2x=b2formulae-sequencesubscript1¥subscript1subscript2¥subscript2A_{1}x=b_{1}, _ 1 x = b _ 1 , A _ 2 x = b _ 2 , where If (T,v),(S,w)¯±£†¤¯±(T,v),(S,w) T , v ) , ( S , w ) caligraphic_T caligraphic_V, y=vec(T,Î³v)¦vec¾£y= v)y = vec ( T , Î³ v ), x=vec(S,Î³w)¥vec†¾¤x= w)x = vec ( S , Î³ w ): If (S,w)¯±†¤¯±(S,w) S , w ) caligraphic_T caligraphic_V then iff tij=c1isij,vj=wjformulae-sequencesubscriptsuperscript¡subscript1subscript subscriptsuperscript£subscript¤t^{*}_{ij}=c_{1i}s_{ij}, ^ _ i j = c _ 1 i s _ i j , v ^ _ j = w _ j , where Since tij=0subscript¡0t_{ij}=0t _ i j = 0 for (i,j)µµ(i,j) i , j ) caligraphic_Z, the Lagrangian associated with this optimization problem is When (i,j)µµ(i,j) i , j ) caligraphic_Z, Setting these to zero yields Hence, tij=sijexp¡(Î±i)=c1isijsubscriptsuperscript¡subscript subscript¼subscript1subscript t^{*}_{ij}=s_{ij} ^ _ i j = s _ i j roman_exp ( - Î± _ i ) = c _ 1 i s _ i j , where c1i=exp¡(Î±i)subscript1subscript¼c_{1i}= _ 1 i = roman_exp ( - Î± _ i ) and vj=wjsubscriptsuperscript£subscript¤v^{*}_{j}=w_{j}v ^ _ j = w _ j . Also, j=1ntij=u~isuperscriptsubscript1subscriptsuperscript¡subscript~ _ j = 1 ^ n t ^ _ i j = over~ u _ i implies (54). If (S,w)¯±†¤¯±(S,w) S , w ) caligraphic_T caligraphic_V then iff tij=c2jsij,vj=c2j1Î³wjformulae-sequencesubscriptsuperscript¡subscript2subscript subscriptsuperscript£superscriptsubscript21¾subscript¤t^{*}_{ij}=c_{2j}s_{ij}, ^ _ i j = c _ 2 j s _ i j , v ^ _ j = c _ 2 j ^ - / 1 Î³ w _ j , where Since tij=0subscript¡0t_{ij}=0t _ i j = 0 for (i,j)µµ(i,j) i , j ) caligraphic_Z, the Lagrangian associated with this optimization problem is given by When (i,j)µµ(i,j) i , j ) caligraphic_Z, Setting these to zero results in Hence, tij=sijc2jsubscriptsuperscript¡subscript subscript2t^{*}_{ij}=s_{ij}c_{2j}t ^ _ i j = s _ i j c _ 2 j where c2j=exp¡(Î²j)subscript2subscript½c_{2j}= _ 2 j = roman_exp ( - Î² _ j ). Also, vj=wjexp¡(Î²j/Î³)=c2j1/Î³wjsubscriptsuperscript£subscript¤subscript½¾superscriptsubscript21¾subscript¤v^{*}_{j}=w_{j} ^ _ j = w _ j roman_exp ( Î² _ j / Î³ ) = c _ 2 j ^ - 1 / Î³ w _ j . Due to the column sum constraints, we must have c2ji=1msij=i=1mtij=vj=c2j1/Î³wjsubscript2superscriptsubscript1subscript superscriptsubscript1subscriptsuperscript¡subscriptsuperscript£superscriptsubscript21¾subscript¤c_{2j} _ 2 j _ i = 1 ^ m s _ i j = _ i = 1 ^ m t ^ _ i j = v ^ _ j = c _ 2 j ^ - 1 / Î³ w _ j , that is, c2j1+Î³Î³i=1msij=wjsuperscriptsubscript21¾¾superscriptsubscript1subscript subscript¤c_{2j}^{ _ 2 j ^ / 1 + Î³ Î³ _ i = 1 ^ m s _ i j = w _ j , which implies (55). It follows, from Theorem 2 and Lemmas 3 and 4, that the sequence, {(T(l),v(l)} ( T ( l ) , v ( l ) }, converges to a limit, (T,v)superscriptsuperscript£(T^{*},v^{*})( T ^ , v ^ ), with T¯superscript¯T^{*} ^ caligraphic_T, v+nsuperscript£subscriptsuperscriptv^{*} ^ R ^ n _ + , and this limit satisfies the constraints in (52). We now only need to prove that f(x(0))“¥0 f(x(0)) f ( x ( 0 ) ) satisfies (50), where x(0)=vec(K,Î³v~)¥0vec¾¾~£x(0)= ( 0 ) = vec ( K , Î³ over~ v ). With J½JJ given by (53), we have, for tij>0subscript¡0t_{ij}>0t _ i j > 0 and vj>0subscript£0v_{j}>0v _ j > 0, Hence, ‚J‚tij(K,v~)=0,‚J‚Î³vj(K,v~)=0formulae-sequence½subscript¡¾~£0½¾subscript£¾~£0 J}{ t_{ij}}(K, J}{% v_{j}}(K, ‚ J ‚ t _ i j ( K , over~ v ) = 0 , / ‚ J ‚ Î³ v _ j ( K , over~ v ) = 0, and f(x(0))i=0“subscript¥00 f(x(0))_{i}=0 f ( x ( 0 ) ) _ i = 0 for i=1,2,¦,q12¦i=1,2, = 1 , 2 , ¦ , q. Therefore, f(x(0))“¥0 f(x(0)) f ( x ( 0 ) ) satisfies (50). { 220.50885pt} SK-type iterations make sense for large-scale problems. Smart cities are a natural place to look for such problems, and, in particular, sharing economy [20] applications, since these are precisely the problem domains where allocation of resources at scale emerge, and where the scale of the problem is subject to temporal variations. One such problem arises in the context of charging mmm electric vehicles (EVs) overnight in a city such as London. With the advent of vehicle-to-grid (V2G), vehicle-to-vehicle (V2V) and widespread availability of solar, it is likely that many entities that currently consume energy will become prosumers in the near future; i.e. most homes, and even cars, will consume energy and also make energy available, depending on the circumstance. Such a scenario is clearly very large scale, with a potentially very large number, mmm, of EVs (there are currently more than 2.5M cars registered in London), and a large number, nnn, of energy providers (consisting of conventional utilities, energy brokerages, households and even other cars, and constituting the target agents of our OT setup in Section III). In the setting of this paper (Section III), each EV specifies the energy it requires in KWhrs, and then this demand for energy is communicated (i.e. transported™) to a set of providers. Each provider may set a cost based on their type of energy generation, their proximity to the car being charged, and the type of vehicle being charged. In addition, some providers may prohibit certain types of vehicles, for example plug-in hybrid vehicles (PHEVs), or vehicles that are very large in size. Our OT formulation (21) captures the realistic scenario in which the nnn energy providers make available flexible amounts of energy, nominally v~~£ v , to mmm EVs whose demands are exactly u~~ u . Furthermore, energy transfer between specified provider-EV pairs, (i,j)µµ(i,j) i , j ) caligraphic_Z (5), are not allowed a priori. To simulate this scenario, consider m=10,00010000m=10,000m = 10 , 000 EVs requiring charging and n=1010n=10n = 10 energy providers. We simulate the specified energy charging requirement, u~isubscript~ u _ i , i=1,¦,m1¦i=1, = 1 , ¦ , m, of each EV via independent and uniform draws in the range (0,1)01(0,1)( 0 , 1 ) KWhrs. We simulate the nominal available energy of each supplier, v~jsubscript~£ v _ j , j=1,¦,n1¦j=1, = 1 , ¦ , n, in the same way. Therefore, (1) may not hold, i.e. the transport problem may be unbalanced, in that the total energy required by the EVs may differ from the total nominal energy made available by the providers. This UOT problem is also solved by our algorithms. To effect prior zero constraints, we assume that even-indexed cars are PHEVs, and that even-indexed providers will not supply these PHEVs. This defines a transport plan with mn44 m n 4 pre-specified zeroes. The non-zero elements of K¾KK in (15) are obtained with t~ij=1subscript~¡1 t _ i j = 1 and Î³0=1.99subscript¾01.99 _ 0 = 1.99, and the transport cost, cijsubscriptc_{ij}c _ i j , (i,j)µfor-allµ ( i , j ) caligraphic_Z are again iid uniformly drawn from (0,1)01(0,1)( 0 , 1 ). We use Algorithm 1, with Î³=1.005¾1.005 = 1.005 (21), to obtain the OT plan, TsuperscriptT^{*}T ^ . Figure 1 illustrates the convergence of the algorithm for 10 simulation runs. Let T(l1)™1T(l-1)T ( l - 1 ) and T(l)™T(l)T ( l ) be the transport plans obtained from two consecutive iterations of the algorithm in any one simulation, and let Î”(l)Î”™ ( l ) be the matrix with (i,j)(i,j)( i , j )th entry defined as Îij(l)=|tij(l)tij(l1)|subscript¿™subscript¡™subscript¡™1 _ i j ( l ) = | t _ i j ( l ) - t _ i j ( l - 1 ) |. Figure 1 plots the log of the sum of the Îijsubscript¿ _ i j s, normalised by the first log-sum, for each of the 10 simulations, i.e. log¡((i,j)µ|tij(l)tij(l1)|)log¡((i,j)µ|tij(1)tij(0)|)subscriptµsubscript¡™subscript¡™1subscriptµsubscript¡1subscript¡0 { roman_log ( _ ( i , j ) caligraphic_Z | t _ i j ( l ) - t _ i j ( l - 1 ) | ) roman_log ( _ ( i , j ) caligraphic_Z | t _ i j ( 1 ) - t _ i j ( 0 ) | ) . We have normalized in order to facilitate comparison of the different simulations. In this paper, we have presented SK-type algorithms for constrained optimal transport. Specifically, our algorithms allow for transport plans that force some entries to be zero a priori. The convergence proof is based on Bregman-type ideas. An example in resource allocation for the sharing economy is provided, pointing to situations in which our algorithm is relevant. Future work will investigate extension of this work to problems that incorporate other forms of regularisation terms [21], and other potential use-cases.",
        "keywords": ""
    },
    {
        "id": 9,
        "title": "A contribution to the theory of Ïƒğ�œ�\\sigmaitalic_Ïƒ-properties of a finite groupâ€ â€ thanks: Research was supported by the National\nNatural Science Foundation of China\n(No. 12171126, 12101165).\nResearch of the third author and the fourth author was\nsupported by the Ministry of Education of the\nRepublic of Belarus (No.Â 20211328, 20211778).\n\nIn memory of Professor Francesco de Giovanni",
        "abstract": "AbstractLetƒ={ƒi£iI}conditional-setsubscript¼\\sigma=\\{\\sigma_{i}\\mid i\\in I\\}ƒ = { ƒ _ i £ i I }be some partition of the set of all primes. A subgroupAAAof a finite groupGºGGis said to be: (i)ƒ\\sigmaƒ-subnormalinGºGGif there is a subgroup chainA=A0¤A1¤‹¯¤An=Gsubscript0subscript1‹¯subscriptºA=A_{0}\\leq A_{1}\\leq\\cdots\\leq A_{n}=GA = A _ 0 ¤ A _ 1 ¤ ‹¯ ¤ A _ n = Gsuch that eitherAi1ŠAisubscript1ŠsubscriptA_{i-1}\\trianglelefteq A_{i}A _ i - 1 Š A _ i orAi/(Ai1)Aisubscriptsubscriptsubscript1subscriptA_{i}/(A_{i-1})_{A_{i}}A _ i / ( A _ i - 1 ) _ A _ i is aƒjsubscript{\\sigma}_{j}ƒ _ j -group,j=j(i)j=j(i)j = j ( i ), for alli=1,¦,n1¦i=1,\\ldots,ni = 1 , ¦ , n; (ii)modularinGºGGif the following conditions are held: (1)¨X,A©Z©=¨X,A©©Z‹‹\\langle X,A\\cap Z\\rangle=\\langle X,A\\rangle\\cap Z¨ X , A © Z © = ¨ X , A © © Zfor allX¤G,Z¤Gformulae-sequence‹ººX\\leq G,Z\\leq GX ¤ G , Z ¤ Gsuch thatX¤Z‹X\\leq ZX ¤ Z, and (2)¨A,Y©Z©=¨A,Y©©Z\\langle A,Y\\cap Z\\rangle=\\langle A,Y\\rangle\\cap Z¨ A , Y © Z © = ¨ A , Y © © Zfor allY¤G,Z¤Gformulae-sequenceººY\\leq G,Z\\leq GY ¤ G , Z ¤ Gsuch thatA¤ZA\\leq ZA ¤ Z; (iii)ƒ\\sigmaƒ-quasinormal inGºGGifAAAisƒ\\sigmaƒ-subnormal and modular inGºGG.We obtain a description of finite groups in whichƒ\\sigmaƒ-quasinormality (respectively, modularity) is a transitive relation. Some known results are extended.00footnotetext:Keywords: finite group, modular subgroup,ƒ\\sigmaƒ-subnormal subgroup,ƒ\\sigmaƒ-quasinormal subgroup,QƒTQ\\sigma TQ ƒ T-group.00footnotetext:Mathematics Subject Classification (2010): 20D10, 20D15, 20D30.",
        "corpus": "Let ƒ={ƒi£iI}conditional-setsubscript¼ i I = { ƒ _ i £ i I } be some partition of the set of all primes. A subgroup AAA of a finite group GºGG is said to be: (i) ƒ in GºGG if there is a subgroup chain A=A0¤A1¤‹¯¤An=Gsubscript0subscript1‹¯subscriptºA=A_{0} A_{1} A_{n}=GA = A _ 0 ¤ A _ 1 ¤ ‹¯ ¤ A _ n = G such that either Ai1ŠAisubscript1ŠsubscriptA_{i-1} A_{i}A _ i - 1 Š A _ i or Ai/(Ai1)Aisubscriptsubscriptsubscript1subscriptA_{i}/(A_{i-1})_{A_{i}}A _ i / ( A _ i - 1 ) _ A _ i is a ƒjsubscript{ _ j -group, j=j(i)j=j(i)j = j ( i ), for all i=1,¦,n1¦i=1, = 1 , ¦ , n; (ii) modular in GºGG if the following conditions are held: (1) ¨X,A©Z©=¨X,A©©Z‹‹ X,A Z X,A Z¨ X , A © Z © = ¨ X , A © © Z for all X¤G,Z¤Gformulae-sequence‹ººX G,Z GX ¤ G , Z ¤ G such that X¤Z‹X ZX ¤ Z, and (2) ¨A,Y©Z©=¨A,Y©©Z A,Y Z A,Y Z¨ A , Y © Z © = ¨ A , Y © © Z for all Y¤G,Z¤Gformulae-sequenceººY G,Z GY ¤ G , Z ¤ G such that A¤ZA ZA ¤ Z; (iii) ƒ in GºGG if AAA is ƒ and modular in GºGG. We obtain a description of finite groups in which ƒ (respectively, modularity) is a transitive relation. Some known results are extended. Throughout this paper, all groups are finite and GºGG always denotes a finite group; (G)º{ L}(G)caligraphic_L ( G ) is the lattice of all subgroups of GºGG; GºGG is said to be an MMM-group [1] if the lattice (G)º{ L}(G)caligraphic_L ( G ) is modular. Moreover, ™™ is the set of all primes, Š†™‹™ Š† P, ²=™superscript‹²™‹ ^ ² = P , and ƒ={ƒi£iI}conditional-setsubscript¼ i I = { ƒ _ i £ i I } is some partition of ™™ If nnn is an integer, the symbol (n)‹ ( n ) denotes the set of all primes dividing nnn; as usual, (G)=(|G|)‹º‹º ( G ) = ( | G | ), the set of all primes dividing the order of GºGG; ƒ(n)={ƒi£ƒi©(n) …}conditional-setsubscriptsubscript‹ ( n ) = { ƒ _ i £ ƒ _ i © ( n ) … } and ƒ(G)=ƒ(|G|)ºº ( G ) = ƒ ( | G | ) [2, 3]. A group GºGG is said to be [2, 3]: ƒ if GºGG is a ƒisubscript _ i -group for some iii; ƒ if GºGG is a direct product of ƒ groups. A subgroup AAA of GºGG is said to be quasinormal (Ore) or permutable (Stonehewer) in GºGG if AAA permutes with every subgroup H»HH of GºGG, that is, AH=HA»»AH=HAA H = H A. The quasinormal subgroups have many interesting and useful for applications properties. For instance, if AAA is quasinormal in GºGG, then: AAA is subnormal in GºGG (Ore [4]), A/AGsubscriptºA/A_{G}A / A _ G is nilpotent (Ito and Szep [5]), every chief factor H/K»¾H/KH / K of GºGG between AGsubscriptºA_{G}A _ G and AGsuperscriptºA^{G}A ^ G is central, that is, CG(H/K)=Gsubscript¶º»¾ºC_{G}(H/K)=GC _ G ( H / K ) = G (Maier and Schmid [6]), and, in general, the section A/AGsubscriptºA/A_{G}A / A _ G is not necessarily abelian (Thomson [7]). Quasinormal subgroups have a close connection with the so-called modular subgroups. Recall that a subgroup MMM of GºGG is said to be: (i) modular in GºGG [1] if MMM is a modular element (in the sense of Kurosh [1, p. 43]) of the lattice (G)º{ L}(G)caligraphic_L ( G ), that is, (1) ¨X,M©Z©=¨X,M©©Z‹‹ X,M Z X,M Z¨ X , M © Z © = ¨ X , M © © Z for all X¤G,Z¤Gformulae-sequence‹ººX G,Z GX ¤ G , Z ¤ G such that X¤Z‹X ZX ¤ Z, and (2) ¨M,Y©Z©=¨M,Y©©Z M,Y Z M,Y Z¨ M , Y © Z © = ¨ M , Y © © Z for all Y¤G,Z¤Gformulae-sequenceººY G,Z GY ¤ G , Z ¤ G such that M¤ZM ZM ¤ Z; (ii) submodular in GºGG if there is a subgroup chain A=A0¤A1¤‹¯¤An=Gsubscript0subscript1‹¯subscriptºA=A_{0} A_{1} A_{n}=GA = A _ 0 ¤ A _ 1 ¤ ‹¯ ¤ A _ n = G such that Ai1subscript1A_{i-1}A _ i - 1 is modular in AisubscriptA_{i}A _ i for all i=1,¦,n1¦i=1, = 1 , ¦ , n. Every quasinormal is clearly modular in the group. Moreover, the following remarkable result is well-known. Theorem A (Schmidt [1, Theorem 5.1.1]) A subgroup AAA of GºGG is quasinormal in GºGG if and only if AAA is subnormal and modular in GºGG. This result made it possible to find an analogue of quasinormality in the theory of the ƒ of a group [8]. A subgroup AAA of GºGG is said to be ƒ in GºGG [2, 3] if there is a subgroup chain A=A0¤A1¤‹¯¤An=Gsubscript0subscript1‹¯subscriptºA=A_{0} A_{1} A_{n}=GA = A _ 0 ¤ A _ 1 ¤ ‹¯ ¤ A _ n = G such that either Ai1ŠAisubscript1ŠsubscriptA_{i-1} A_{i}A _ i - 1 Š A _ i or Ai/(Ai1)Aisubscriptsubscriptsubscript1subscriptA_{i}/(A_{i-1})_{A_{i}}A _ i / ( A _ i - 1 ) _ A _ i is ƒ{ for all i=1,¦,n1¦i=1, = 1 , ¦ , n; ƒ in GºGG (J.C. Beidleman) if xNG(A)¥subscriptºx N_{G}(A)x N _ G ( A ) for all xG¥ºx Gx G such that ƒ(|x|)©ƒ(A)=…¥ ( | x | ) © ƒ ( A ) = …. Definition 1.1. We say that a subgroup AAA of GºGG is ƒ in GºGG if AAA is ƒ and modular in GºGG. Before continuing, consider some examples. Example 1.2. (i) In the first limiting case, when ƒ={™}™ = { P }, every group is ƒ and every subgroup of any group is ƒ Therefore in this case a subgroup AAA of GºGG is ƒ if and only if it is modular in GºGG. (ii) In the second limiting case, when ƒ=ƒ1={{2},{3},{5}¦}superscript1235¦ = ƒ ^ 1 = { { 2 } , { 3 } , { 5 } ¦ }, a subgroup AAA of GºGG is ƒ in GºGG if and only if it is subnormal in GºGG. Therefore in this case, in view of Theorem A, a subgroup AAA of GºGG is ƒ if and only if it is quasinormal in GºGG. (iii) In the case ƒ=ƒ1={{p1},¦,{pn},²}superscript1‹subscript1¦subscriptsuperscript‹² = ƒ ^ 1 = { { p _ 1 } , ¦ , { p _ n } , ^ ² }, where ={p1,¦,pn}‹subscript1¦subscript = { p _ 1 , ¦ , p _ n }, a subgroup AAA of GºGG is ƒ1superscript1‹ ^ 1 -subnormal in GºGG if and only if GºGG has a subgroup chain A=A0¤A1¤‹¯¤An=Gsubscript0subscript1‹¯subscriptºA=A_{0} A_{1} A_{n}=GA = A _ 0 ¤ A _ 1 ¤ ‹¯ ¤ A _ n = G such that either Ai1ŠAisubscript1ŠsubscriptA_{i-1} A_{i}A _ i - 1 Š A _ i or Ai/(Ai1)Aisubscriptsubscriptsubscript1subscriptA_{i}/(A_{i-1})_{A_{i}}A _ i / ( A _ i - 1 ) _ A _ i is a ²superscript‹²{ ^ ² -group for all i=1,¦,n1¦i=1, = 1 , ¦ , n. In this case we say, following [9, 10, 11], that AAA is 11‹1 -subnormal in GºGG, and we say that AAA is 11‹1 -quasinormal in GºGG if AAA is 11‹1 -subnormal and modular in GºGG. Note, in passing, that AAA is 11‹1 -subnormal in GºGG if and only if AAA is ”” in GºGG in the sence of Kegel [12], where ”” is the class of all ²superscript‹² ^ ² -groups. (iv) In the other classical case ƒ=ƒ={,²}superscript‹‹superscript‹² = ƒ ^ = { , ^ ² } a subgroup AAA of GºGG is ƒsuperscript‹{ ^ -subnormal in GºGG if and only if GºGG has a subgroup chain A=A0¤A1¤‹¯¤An=Gsubscript0subscript1‹¯subscriptºA=A_{0} A_{1} A_{n}=GA = A _ 0 ¤ A _ 1 ¤ ‹¯ ¤ A _ n = G such that either Ai1ŠAisubscript1ŠsubscriptA_{i-1} A_{i}A _ i - 1 Š A _ i , or Ai/(Ai1)Aisubscriptsubscriptsubscript1subscriptA_{i}/(A_{i-1})_{A_{i}}A _ i / ( A _ i - 1 ) _ A _ i is a ‹{ or Ai/(Ai1)Aisubscriptsubscriptsubscript1subscriptA_{i}/(A_{i-1})_{A_{i}}A _ i / ( A _ i - 1 ) _ A _ i is a ²superscript‹²{ ^ ² -group for all i=1,¦,n1¦i=1, = 1 , ¦ , n. In this case we say that AAA is ,²‹superscript‹² , ^ ² -subnormal in GºGG [9, 10, 11], and we say that AAA is ,²‹superscript‹² , ^ ² -quasinormal in GºGG if AAA is ,²‹superscript‹² , ^ ² -subnormal and modular in GºGG. The theory of ƒ subgroups was constructed in the paper [13]. In particular, it was proven the following result covering in the case ƒ=ƒ1={{2},{3},{5}¦}superscript1235¦ = ƒ ^ 1 = { { 2 } , { 3 } , { 5 } ¦ } the above mentioned results in [4, 5, 6]. Theorem B (See Theorem C in [13]). Let AAA be a ƒ subgroup of GºGG. Then the following statements hold: (i) AAA permutes with all Hall ƒisubscript _ i -subgroups of GºGG for all iii. (ii) The quotients AG/AGsuperscriptºsubscriptºA^{G}/A_{G}A ^ G / A _ G and G/CG(AG/AG)ºsubscript¶ºsuperscriptºsubscriptºG/C_{G}(A^{G}/A_{G})G / C _ G ( A ^ G / A _ G ) are ƒ and (iii) Every chief factor H/K»¾H/KH / K of GºGG between AGsuperscriptºA^{G}A ^ G and AGsubscriptºA_{G}A _ G is ƒ in GºGG , that is, (H/K)‹Š(G/CG(H/K))right-normal-factor-semidirect-product»¾ºsubscript¶º»¾(H/K) H / K ) ‹Š ( G / C _ G ( H / K ) ) is ƒ (iv) For every iii such that ƒiƒ(G/CG(AG/AG))subscriptºsubscript¶ºsuperscriptºsubscriptº _ i ƒ ( G / C _ G ( A ^ G / A _ G ) ) we have ƒiƒ(AG/AG).subscriptsuperscriptºsubscriptº _ i ƒ ( A ^ G / A _ G ) . (v) AAA is ƒ in GºGG. A group GºGG is said to be a PTƒPTP T-group [14, 2.0.2] if quasinormality is a transitive relation on GºGG, that is, if H»HH is a quasinormal subgroup of K¾KK and K¾KK is a quasinormal subgroup of GºGG, then H»HH is a quasinormal subgroup of GºGG. The description of PTƒPTP T-groups was first obtained by Zacher [15], for the soluble case, and by Robinson in [16], for the general case. Bearing in mind the results in [15, 16] and many other known results on PTƒPTP T-groups (see, in particular, Chapter 2 in [14]), it seems to be natural to ask: Question 1.3. What is the structure of GºGG provided ƒ is a transitive relation in GºGG? Question 1.4. What is the structure of GºGG provided modularity is a transitive relation in GºGG? Note that in view of Example 1.2(i), Question 1.4 is a special case of Question 1.3, where ƒ={™}™ = { P }. Note also that for the case when GºGG is a soluble group, the answers to both of these questions are known. Frigerio proved [18] (see also [19]) that modularity is a transitive relation in a soluble group GºGG if and only if GºGG is an MMM-group. An important step in solving the general Problem 1.3 was made in the paper [17], where it was proven the following theorem turn into Frigerion result in the case where ƒ={™}™ = { P }. Theorem C (X.-F. Zhang, W. Guo, I.N. Safonova, A.N. Skiba [17]). Let GºGG be a soluble group and D=G”ƒ·superscriptºsubscript”D=G^{ = G ^ fraktur_N _ ƒ . Then ƒ is a transitive relation in GºGG if and only if the following conditions hold: (i) G=D‹ŠMºright-normal-factor-semidirect-product·G=D MG = D ‹Š M, where D·DD is an abelian Hall subgroup of GºGG of odd order, MMM is a ƒ MMM-group. (ii) every element of GºGG induces a power automorphism on D·DD, (iii) Oƒi(D)subscript‚subscript·O_{ _ ƒ _ i ( D ) has a normal complement in a Hall ƒisubscript _ i -subgroup of GºGG for all iii. Conversely, if Conditions (i), (ii) and (iii) hold for some subgroups D·DD and MMM of GºGG, then ƒ is a transitive relation in GºGG. In this theorem, G”ƒsuperscriptºsubscript”G^{ ^ fraktur_N _ ƒ denotes the ƒ residual of GºGG, that is, the intersection of all normal subgroups NNN of GºGG with ƒ quotient G/NºG/NG / N. Definition 1.5. We say that GºGG is: (i) a QƒTQ TQ ƒ T-group if the ƒ is a transitive relation on GºGG, that is, if H»HH is a ƒ subgroup of K¾KK and K¾KK is a ƒ subgroup of GºGG, then H»HH is a ƒ subgroup of GºGG; (ii) an MTMTM T-group if the modularity is a transitive relation in GºGG. It is clear that an MTMTM T-group is exactly a QƒTQ TQ ƒ T-group where ƒ={™}™ = { P }. In this article, expanding the corresponding results of the papers [16, 17, 21], we answer Questions 1.3 and 1.4 in the general case. Definition 1.6. We say that (D,Z(D);U1,¦,Uk)··subscript1¦subscript(D,Z(D);U_{1}, D , Z ( D ) ; U _ 1 , ¦ , U _ k ) is a Robinson complex if the following fold: (i) D 1·1D 1D 1 is a perfect normal subgroup of GºGG, (ii) D/Z(D)=U1/Z(D)Ã‹¯ÃUk/Z(D)··subscript1·‹¯subscript·D/Z(D)=U_{1}/Z(D) U_{k}/Z(D)D / Z ( D ) = U _ 1 / Z ( D ) Ã ‹¯ Ã U _ k / Z ( D ), where Ui/Z(D)subscript·U_{i}/Z(D)U _ i / Z ( D ) is a simple non-abelian chief factor of GºGG, Z(D)=Î¦(D)·Î¦·Z(D)= ( D ) = roman_Î¦ ( D ), and (iii) every chief factor of GºGG below Z(D)·Z(D)Z ( D ) is cyclic. Example 1.7. Let G=SL(2,7)ÃA7ÃA5ÃBº†¿27subscript7subscript5µG=SL(2,7) A_{7} A_{5} BG = S L ( 2 , 7 ) Ã A _ 7 Ã A _ 5 Ã B, where B=C43‹ŠC7µright-normal-factor-semidirect-productsubscript¶43subscript¶7B=C_{43} C_{7}B = C _ 43 ‹Š C _ 7 is a non-abelian group of order 301. Then is a Robinson complex of GºGG. Now let G=AnCp=K‹ŠCpºsubscriptsubscript¶right-normal-factor-semidirect-product¾subscript¶G=A_{n} C_{p}=K C_{p}G = A _ n C _ p = K ‹Š C _ p , where K¾KK is the base group of the regular wreath product of the alternating group AnsubscriptA_{n}A _ n of degree n>44n>4n > 4 with a group Cpsubscript¶C_{p}C _ p of prime order ppp. Then K¾KK is a minimal normal subgroup of GºGG by [20, Chapter A, 18.5(a) ]. Hence GºGG has no a Robinson complex. We say, following Robinson [16], that GºGG satisfies: (1) psubscript{ N}_{p}N _ p if whenever NNN is a soluble normal subgroup of GºGG, p²superscript²p^{ ^ ² -elements of GºGG induce power automorphism in Op(G/N)subscript‚ºO_{p}(G/N)O _ p ( G / N ); (2) psubscript{ P}_{p}P _ p if whenever NNN is a soluble normal subgroup of GºGG, every subgroup of Op(G/N)subscript‚ºO_{p}(G/N)O _ p ( G / N ) is quasinormal in every Sylow ppp-subgroup of G/NºG/NG / N. Every subnormal subgroup is both submodular and ƒ in the group. Thus the following well-known result partially describes the structure of insoluble QƒTQ TQ ƒ T-groups. Theorem D (Robinson [16]). GºGG is a PTƒPTP T-group if and only if GºGG has a normal perfect subgroup D·DD such that: (i) G/Dº·G/DG / D is a soluble PTƒPTP T-group, and (i) if D 1·1D 1D 1, GºGG has a Robinson complex (D,Z(D);U1,¦,Uk)··subscript1¦subscript(D,Z(D);U_{1}, D , Z ( D ) ; U _ 1 , ¦ , U _ k ) and (iii) for any set {i1,¦,ir}Š†{1,¦,k}subscript1¦subscript1¦ i _ 1 , ¦ , i _ r } Š† { 1 , ¦ , k }, where 1¤r<k11 r<k1 ¤ r < k, GºGG and G/Ui1²‹¯Uir²ºsuperscriptsubscriptsubscript1²‹¯superscriptsubscriptsubscript²G/U_{i_{1}}^{ U_{i_{r}}^{ / U _ i _ 1 ^ ² ‹¯ U _ i _ r ^ ² satisfy psubscript{ N}_{p}N _ p for all p(Z(D))‹·p ( Z ( D ) ) and psubscript{ P}_{p}P _ p for all p(D)‹·p ( D ). Now, recall that GºGG is a non-abelian PƒPP-group (see [1, p. 49]) if G=A‹Š¨t©ºright-normal-factor-semidirect-productdelimited-¨©¡G=A t = A ‹Š ¨ t ©, where AAA is an elementary abelian ppp-group and an element t¡tt of prime order q pq pq p induces a non-trivial power automorphism on AAA. In this case we say that GºGG is a PƒPP-group of type (p,q)(p,q)( p , q ). Definition 1.8. We say that: (i) GºGG satisfies ƒ(p,q)subscript{ Q}_{ _ ƒ ( p , q ) if whenever NNN is a soluble normal subgroup of GºGG and P/NƒP/NP / N is a normal ƒ PƒPP-subgroup of type (p,q)(p,q)( p , q ) of G/NºG/NG / N, every subgroup of P/NƒP/NP / N is modular in G/NºG/NG / N. If GºGG satisfies ƒ(p,q)subscript{ Q}_{ _ ƒ ( p , q ) and ƒ={™}™ = { P }, then say, following [21], that GºGG satisfies p,qsubscript{ M}_{p,q}M _ p , q . (ii) GºGG satisfies ƒPsubscriptƒ{ Q}_{ P}Q _ ƒ P if GºGG satisfies ƒ(p,q)subscript{ Q}_{ _ ƒ ( p , q ) for each pair p,qp,qp , q such that there is a PƒPP-group of type (p,q)(p,q)( p , q ). In this paper, based on Theorems C and D, we prove the following result. Theorem E. A group GºGG is a QƒTQ TQ ƒ T-group if and only if GºGG has a perfect normal subgroup D·DD such that: (i) G/Dº·G/DG / D is a soluble QƒTQ TQ ƒ T-group, (ii) if D 1·1D 1D 1, GºGG has a Robinson complex (D,Z(D);U1,¦,Uk)··subscript1¦subscript(D,Z(D);U_{1}, D , Z ( D ) ; U _ 1 , ¦ , U _ k ) and (iii) for any set {i1,¦,ir}Š†{1,¦,k}subscript1¦subscript1¦ i _ 1 , ¦ , i _ r } Š† { 1 , ¦ , k }, where 1¤r<k11 r<k1 ¤ r < k, the groups GºGG and G/Ui1²‹¯Uir²ºsuperscriptsubscriptsubscript1²‹¯superscriptsubscriptsubscript²G/U_{i_{1}}^{ U_{i_{r}}^{ / U _ i _ 1 ^ ² ‹¯ U _ i _ r ^ ² satisfy psubscript{ N}_{p}N _ p for all p{2,3}©(Z(D))23‹·p { 2 , 3 } © ( Z ( D ) ), psubscript{ P}_{p}P _ p for all p(D)‹·p ( D ), and ƒ(p,q){ Q}_{ _ ƒ ( p , q ) for all {p,q}©(D) …‹· p , q } © ( D ) …. Theorem E gives a solution to Question 1.3. The following special case of Theorem E gives a solution to Question 1.4. Theorem F. A group GºGG is an MTMTM T-group if and only if GºGG has a perfect normal subgroup D·DD such that: (i) G/Dº·G/DG / D is an MMM-group, (ii) if D 1·1D 1D 1, GºGG has a Robinson complex (D,Z(D);U1,¦,Uk)··subscript1¦subscript(D,Z(D);U_{1}, D , Z ( D ) ; U _ 1 , ¦ , U _ k ) and (iii) for any set {i1,¦,ir}Š†{1,¦,k}subscript1¦subscript1¦ i _ 1 , ¦ , i _ r } Š† { 1 , ¦ , k }, where 1¤r<k11 r<k1 ¤ r < k, GºGG and G/Ui1²‹¯Uir²ºsuperscriptsubscriptsubscript1²‹¯superscriptsubscriptsubscript²G/U_{i_{1}}^{ U_{i_{r}}^{ / U _ i _ 1 ^ ² ‹¯ U _ i _ r ^ ² satisfy psubscript{ N}_{p}N _ p for all p{2,3}©(Z(D))23‹·p { 2 , 3 } © ( Z ( D ) ), psubscript{ P}_{p}P _ p for all p(D)‹·p ( D ), and p,qsubscript{ M}_{p,q}M _ p , q for all pairs {p,q}©(D) ….‹· p , q } © ( D ) … . We prove Theorem E (and so Theorem F, as well) in Section 3. In Section 4 we discuss some other applications of these results. The first lemma is a corollary of general properties of modular subgroups [1, p. 201] and ƒ subgroups [3, Lemma 2.6]. Lemma 2.1. Let AAA, BµBB and NNN be subgroups of GºGG, where AAA is ƒ and NNN is normal in GºGG. (1) The subgroup A©BµA BA © B is ƒ in BµBB. (2) The subgroup AN/NAN/NA N / N is ƒ in G/NºG/NG / N. (3) If N¤BµN BN ¤ B and B/NµB/NB / N is ƒ in G/NºG/NG / N, then BµBB is ƒ in GºGG. (4) BµBB is ƒ in GºGG, then ¨A,B©µ A,B A , B © is ƒ in GºGG. Lemma 2.2 A subgroup AAA of GºGG is a maximal ƒ subgroup of GºGG if and only if either AAA is normal in GºGG and G/AºG/AG / A is a simple gropup or AG<AsubscriptºA_{G}<AA _ G < A and G/AGºsubscriptºG/A_{G}G / A _ G is a ƒ non-abelian group of order pqpqp q for primes ppp and qqq. Proof. First assume that AAA is a maximal ƒ subgroup of GºGG. If AAA is normal in GºGG, then G/A=G/AGººsubscriptºG/A=G/A_{G}G / A = G / A _ G is simple. Now assume that AAA is not normal in GºGG, so AG=GsuperscriptººA^{G}=GA ^ G = G and, in view of Theorem B(ii), G/AGºsubscriptºG/A_{G}G / A _ G is a ƒisubscript _ i -group for some iii. Hence every subgroup of GºGG containing AGsubscriptºA_{G}A _ G is ƒ in GºGG by [3, Lemma 2.6(5)]. On the other hand, U/AGsubscriptºU/A_{G}U / A _ G is modular in GºGG if and only if UUU is modular in GºGG by [1, Page 201, Properties (3)(4)]. Therefore, in fact, AAA is a maximal modular subgroup of GºGG. Hence G/AGºsubscriptºG/A_{G}G / A _ G is a non-abelian group of order pqpqp q for primes p,qƒisubscriptp,q , q ƒ _ i by [1, Lemma 5.1.2]. Now assume that AG<A<GsubscriptººA_{G}<A<GA _ G < A < G and G/AGºsubscriptºG/A_{G}G / A _ G is a ƒ non-abelian group of order pqpqp q for primes ppp and qqq. Then AAA is a maximal subgroup of GºGG and AAA is a ƒ subgroup of GºGG. Moreover, A/AGsubscriptºA/A_{G}A / A _ G is modular in G/AGºsubscriptºG/A_{G}G / A _ G by [1, Lemma 5.1.2], so AAA is a maximal modular subgroup of GºGG by [1, Page 201, Property (4)]. Hence AAA is a maximal ƒ subgroup of GºGG. Finally, assume that AAA is normal in GºGG and G/AºG/AG / A is a simple non-abelian group, then AAA is a maximal modular subgroup of GºGG by [1, Lemma 5.1.2] and AAA ƒ in GºGG. Hence AAA is a maximal ƒ subgroup of GºGG. The lemma is proved. We say that a subgroup AAA of GºGG is said to be ƒ in GºGG if there is a subgroup chain A=A0¤A1¤‹¯¤An=Gsubscript0subscript1‹¯subscriptºA=A_{0} A_{1} A_{n}=GA = A _ 0 ¤ A _ 1 ¤ ‹¯ ¤ A _ n = G such that Ai1subscript1A_{i-1}A _ i - 1 is ƒ in AisubscriptA_{i}A _ i for all i=1,¦,n1¦i=1, = 1 , ¦ , n. It is clear that GºGG is a QƒTQ TQ ƒ T-group if and only if every of its ƒ subgroups is ƒ in GºGG. The class of groups ”” is a hereditary formation if ”” is closed under taking derect products, homomorphic images and subgroups. If ” …” … is a hereditary formation, then the symbol G”superscriptº”G^{ ^ fraktur_F denotes the ”” of GºGG, that is, the intersection of all normal subgroups NNN of GºGG with G/N”º”G/N / N fraktur_F. We use ”superscript” ^ to denote the class of all abelian groups of squarefree exponent. It is clear that ”superscript” ^ is a hereditary formation. Lemma 2.3. Let AAA, BµBB and NNN be subgroups of GºGG, where AAA is ƒ GºGG and NNN is normal GºGG in GºGG. (1) A©BµA BA © B is ƒ GºGG in BµBB. (2) AN/NAN/NA N / N is ƒ GºGG in G/NºG/NG / N. (3) If N¤K¾N KN ¤ K and K/N¾K/NK / N is ƒ GºGG in G/NºG/NG / N, then K¾KK is ƒ GºGG in G.ºG.G . (4) A”superscriptsuperscript”A^{{ ^ fraktur_A ^ is subnormal in GºGG. (5) If G=U1Ã‹¯ÃUkºsubscript1‹¯subscriptG=U_{1} U_{k}G = U _ 1 Ã ‹¯ Ã U _ k , where UisubscriptU_{i}U _ i is a simple non-abelian group, then AAA is normal in GºGG. Proof. (1)“(4). These assertions follow from Lemma 2.6 in [3] and corresponding lemmas in [19]. (5) Let E=UiAsubscriptE=U_{i}AE = U _ i A, where Ui°Anot-less-than-nor-greater-thansubscriptU_{i} AU _ i ° A. We show that AŠEŠA EA Š E. The subgroup AAA is ƒ GºGG in EEE by Part (1) and A<EA<EA < E, so there is a subgroup chain A=E0<E1<‹¯<Et1<Et=Esubscript0subscript1‹¯subscript¡1subscript¡A=E_{0}<E_{1}< = E _ 0 < E _ 1 < ‹¯ < E _ t - 1 < E _ t = E such that Ei1subscript1E_{i-1}E _ i - 1 is a maximal ƒ subgroup of EisubscriptE_{i}E _ i for all i=1,¦,t1¦¡i=1, = 1 , ¦ , t and for M=Et1subscript¡1M=E_{t-1}M = E _ t - 1 we have M=A(M©Ui)subscriptM=A(M U_{i})M = A ( M © U _ i ), where M©UisubscriptM U_{i}M © U _ i is ƒ in UisubscriptU_{i}U _ i . Then M©Ui<UisubscriptsubscriptM U_{i}<U_{i}M © U _ i < U _ i since M<EM<EM < E. Therefore M©Ui=1=A©Uisubscript1subscriptM U_{i}=1=A U_{i}M © U _ i = 1 = A © U _ i by Lemma 2.2 since UisubscriptU_{i}U _ i is a simple non-abelian group, so AAA is a maximal ƒ subgroup of EEE. Assume that AAA is not normal in EEE. Then E/AE=UiA/AEsubscriptsubscriptsubscriptE/A_{E}=U_{i}A/A_{E}E / A _ E = U _ i A / A _ E is a group of order qrqrq r for primes qqq and rrr by Lemma 2.2, where UiƒUiAE/AE¤E/AEsimilar-to-or-equalssubscriptsubscriptsubscriptsubscriptsubscriptU_{i} U_{i}A_{E}/A_{E} E/A_{E}U _ i ƒ U _ i A _ E / A _ E ¤ E / A _ E . This contradiction show that Ui¤NE(A)subscriptsubscriptU_{i} N_{E}(A)U _ i ¤ N _ E ( A ), so G¤NG(A)ºsubscriptºG N_{G}(A)G ¤ N _ G ( A ). Hence we have (5). The lemma is proved. Lemma 2.4. If GºGG is a QƒTQ TQ ƒ T-group, then every quotient G/NºG/NG / N of GºGG is also a QƒTQ TQ ƒ T-group. Proof. Let L/N¿L/NL / N be a ƒ subgroup of G/NºG/NG / N. Then L¿LL is a ƒ subgroup in GºGG by Lemma 2.3(3), so L¿LL is ƒ in GºGG by hypothesis and then L/N¿L/NL / N is ƒ in G/NºG/NG / N by Lemma 2.1(2). Hence G/NºG/NG / N is a QƒTQ TQ ƒ T-group. The lemma is proved. Lemma 2.5. If GºGG is a QƒTQ TQ ƒ T-group, then G/Rº…G/RG / R satisfies ƒPsubscriptƒ{ Q}_{ P}Q _ ƒ P for every normal subgroup R…RR of GºGG. Proof. In view of Lemma 2.4, we can assume without loss of generality that R=1…1R=1R = 1. Let P/NƒP/NP / N be any normal ƒ non-abelian PƒPP-subgroup of type (p,q)(p,q)( p , q ) of G/NºG/NG / N and let L/N¤P/N¿ƒL/N P/NL / N ¤ P / N. Then L/N¿L/NL / N is modular in P/NƒP/NP / N by [1, Lemma 2.4.1], so L/N¿L/NL / N is submodular in G/NºG/NG / N. On the other hand, L/N¿L/NL / N is ƒ in G/NºG/NG / N since P/N¤Oƒi(G/N)ƒsubscript‚subscriptºP/N O_{ / N ¤ O _ ƒ _ i ( G / N ) for some iii. Therefore L/N¿L/NL / N is ƒ in G/NºG/NG / N and so L¿LL is ƒ in GºGG by Lemma 2.3(3). Hence L¿LL is ƒ in GºGG by hypothesis, so L/N¿L/NL / N is modular in G/NºG/NG / N by [1, Page 201, Property (3)]. Therefore GºGG satisfies ƒPsubscriptƒ{ Q}_{ P}Q _ ƒ P . The lemma is proved. We use G”superscriptº”G^{ ^ fraktur_S (respectively, G”superscriptº”G^{ ^ fraktur_U ) to denote the soluble (respectively, the supersoluble) residual of GºGG. Lemma 2.6. Let GºGG be a non-soluble group and suppose that GºGG has a Robinson complex (D,Z(D);U1,¦,Uk),··subscript1¦subscript(D,Z(D);U_{1}, D , Z ( D ) ; U _ 1 , ¦ , U _ k ) , where D=G”=G”·superscriptº”superscriptº”D=G^{ = G ^ fraktur_S = G ^ fraktur_U . Let UUU be a ƒ non-ƒ subgroup of GºGG of minimal order. Then: (1) If UUi²/Ui²superscriptsubscript²superscriptsubscript²UU_{i}^{ U _ i ^ ² / U _ i ^ ² is ƒ in G/Ui²ºsuperscriptsubscript²G/U_{i}^{ / U _ i ^ ² for all i=1,¦,k1¦i=1, = 1 , ¦ , k, then UUU is supersoluble. (2) If UUU is supersoluble and UL/L¿¿UL/LU L / L is ƒ in G/Lº¿G/LG / L for all non-trivial nilpotent normal subgroups L¿LL of GºGG, then UUU is a cyclic ppp-group for some prime ppp. Proof. Suppose that this lemma is false and let GºGG be a counterexample of minimal order. (1) Assume this is false. Suppose that U©D¤Z(D)··U D Z(D)U © D ¤ Z ( D ). Then every chief factor of UUU below U©Z(D)=U©D··U Z(D)=U DU © Z ( D ) = U © D is cyclic and, also, UD/DƒU/(U©D)similar-to-or-equals···UD/D U/(U D)U D / D ƒ U / ( U © D ) is supersoluble. Hence UUU is supersoluble, a contradiction. Therefore U©D°Z(D)not-less-than-nor-greater-than··U D Z(D)U © D ° Z ( D ). Moreover, Lemma 2.3(1)(2) implies that (U©D)Z(D)/Z(D)···(U D)Z(D)/Z(D)( U © D ) Z ( D ) / Z ( D ) is ƒ in D/Z(D)··D/Z(D)D / Z ( D ) and so (U©D)Z(D)/Z(D)···(U D)Z(D)/Z(D)( U © D ) Z ( D ) / Z ( D ) is a non-trivial normal subgroup of D/Z(D)··D/Z(D)D / Z ( D ) by Lemma 2.3(5). Hence for some iii we have Ui/Z(D)¤(U©D)Z(D)/Z(D),subscript····U_{i}/Z(D) D)Z(D)/Z(D),U _ i / Z ( D ) ¤ ( U © D ) Z ( D ) / Z ( D ) , so Ui¤(U©D)Z(D).subscript··U_{i} D)Z(D).U _ i ¤ ( U © D ) Z ( D ) . But then Ui²¤((U©D)Z(D))²¤U©D.superscriptsubscript²superscript··²·U_{i}^{ D)Z(D))^{ U D.U _ i ^ ² ¤ ( ( U © D ) Z ( D ) ) ^ ² ¤ U © D . By hypothesis, UUi²/Ui²=U/Ui²superscriptsubscript²superscriptsubscript²superscriptsubscript²UU_{i}^{ U _ i ^ ² / U _ i ^ ² = U / U _ i ^ ² is ƒ in G/Ui²ºsuperscriptsubscript²G/U_{i}^{ / U _ i ^ ² and so UUU is ƒ in GºGG by Lemma 2.1(3), a contradiction. Therefore Statement (1) holds. (2) Assume that this is false. Let N=U”superscript”N=U^{{ = U ^ fraktur_N be the nilpotent residual of UUU. Then N<UN<UN < U since UUU supersoluble, so NNN is ƒ in GºGG by the minimality of UUU. It is also clear that every proper subgroup S†SS of UUU with N¤S†N SN ¤ S is ƒ in GºGG, so S†SS is ƒ in GºGG. Therefore, if UUU has at least two distinct maximal subgroups S†SS and WŠWW such that N¤S©W†ŠN S WN ¤ S © W, then U=¨S,W©†ŠU= S,W = ¨ S , W © is ƒ in GºGG by Lemma 2.1(4), contrary to the choice of UUU. Hence U/NU/NU / N is a cyclic ppp-group for some prime ppp and N 11N 1N 1 since UUU is not cyclic. Now we show that UUU is a PTƒPTP T-group. Let S†SS be a proper subnormal subgroup of UUU. Then S†SS is ƒ in GºGG, so S†SS is ƒ in GºGG and hence S†SS is ƒ in UUU by Lemma 2.1(1). Therefore S†SS is quasinormal in UUU by Theorem A. Therefore UUU is a soluble PTƒPTP T-group, so N=U”superscript”N=U^{{ = U ^ fraktur_N is a Hall abelian subgroup of UUU by[14, Theorem 2.1.11]. It follows that N¤U”superscriptsuperscript”N U^{{ ¤ U ^ fraktur_A ^ and so U”=NV,superscriptsuperscript”U^{{ ^ fraktur_A ^ = N V , where VVV is a maximal subgroup of a cyclic Sylow ppp-subgroup PƒU/Nsimilar-to-or-equalsƒP U/NP ƒ U / N of UUU. Hence NVNVN V is ƒ in GºGG and NVNVN V is subnormal in GºGG by Lemma 2.3(4). Therefore NVNVN V is quasinormal in GºGG by Theorem A. Assume that for some minimal normal subgroup R…RR of GºGG we have R¤(NV)G…subscriptºR ¤ ( N V ) _ G . Then U/R…U/RU / R is ƒ in G/Rº…G/RG / R by hypothesis, so UUU is ƒ in GºGG, a contradiction. Therefore (NV)G=1subscriptº1(NV)_{G}=1( N V ) _ G = 1, so NVNVN V is nilpotent and NV¤Z(G)subscriptºNV Z_{ V ¤ Z _ ( G ) by [14, Corollary 1.5.6] and then U=NPƒU=NPU = N P is nilpotent, so N=11N=1N = 1, a contradcition. Therefore Statement (2) holds. The lemma is proved. Lemma 2.7. Suppose that a soluble group G=D‹ŠMºright-normal-factor-semidirect-product·G=D MG = D ‹Š M satisfies Conditions (i), (ii) and (iii) in Theorem C. If AAA is a ƒ ƒ subgroup of GºGG such that A¤MA MA ¤ M, then D¤CG(A)·subscript¶ºD C_{G}(A)D ¤ C _ G ( A ). Proof. Let AAA be a ƒisubscript _ i -group and x¥xx an element of prime power order pnsuperscriptp^{n}p ^ n of D·DD. Let Hksubscript»H_{k}H _ k be a Hall ƒksubscript _ k -subgroup of GºGG. Then, by hypothesis, Hk=Oƒk(D)ÃSksubscript»subscript‚subscript·subscript†H_{k}=O_{ S_{k}H _ k = O _ ƒ _ k ( D ) Ã S _ k , where Oƒk(D)subscript‚subscript·O_{ _ ƒ _ k ( D ) and Sksubscript†S_{k}S _ k are Hall subgroups of GºGG. Since AAA is ƒ in GºGG, A¤Hisubscript»A H_{i}A ¤ H _ i by Lemma 2.6(7) in [3]. On the other hand, since A¤MA MA ¤ M, A©D=1·1A D=1A © D = 1. Therefore A=(A©Oƒi(D))Ã(A©Si)=A©Sisubscript‚subscript·subscript†subscript†A=(A O_{ S_{i})=A S_{i}A = ( A © O _ ƒ _ i ( D ) ) Ã ( A © S _ i ) = A © S _ i , so A¤Sisubscript†A S_{i}A ¤ S _ i and hence Oƒi(D)¤CG(A)subscript‚subscript·subscript¶ºO_{ C_{G}(A)O _ ƒ _ i ( D ) ¤ C _ G ( A ). Now, let k ik ik i. Then AAA is a Hall ƒisubscript _ i -subgroup of V:=Oƒk(D)Aassignsubscript‚subscript·V:=O_{ := O _ ƒ _ k ( D ) A and AAA is ƒ in VVV by Lemma 2.6(1) in [3], so V=Oƒk(D)ÃAsubscript‚subscript·V=O_{ AV = O _ ƒ _ k ( D ) Ã A by Lemma 2.6(10) in [3] and hence D¤CG(A)·subscript¶ºD C_{G}(A)D ¤ C _ G ( A ). The lemma is proved. Lemma 2.8 (See Lemma 5.1.9 in [1]). Let AAA be a subgroup of prime power order of GºGG. (1) If AAA is modular but not subnormal in GºGG, then where AG/AGsuperscriptºsubscriptºA^{G}/A_{G}A ^ G / A _ G is a non-abelian PƒPP-group of order prime to |K/AG|¾subscriptº|K/A_{G}|| K / A _ G |. (2) AAA is modular in GºGG if and only if AAA is modular in ¨x,A©¥ x,A x , A © for all xG¥ºx Gx G of prime power order. Lemma 2.9. If G/ZºG/ZG / Z is ppp-closed for some prime ppp and Z¤Z(G)subscriptºZ Z_{ ¤ Z _ ( G ), then GºGG is ppp-closed. Proof. Since Z¤Z(G)subscriptºZ Z_{ ¤ Z _ ( G ), for a Sylow ppp-subgroup ZpsubscriptZ_{p}Z _ p of ZZZ we have Z=ZpÃWsubscriptŠZ=Z_{p} WZ = Z _ p Ã W, where ZpsubscriptZ_{p}Z _ p and WŠWW are characteristic in ZZZ and so normal in GºGG. Let P/ZƒP/ZP / Z be a normal Sylow ppp-subgroup of G/ZºG/ZG / Z and VVV a Sylow ppp-subgroup of PƒPP. Then Zp¤VsubscriptZ_{p} VZ _ p ¤ V and P=VZ=VÃWƒŠP=VZ=V WP = V Z = V Ã W since W¤Z(G)©P¤Z(P)ŠsubscriptºƒsubscriptƒW Z_{ P Z_{ ¤ Z _ ( G ) © P ¤ Z _ ( P ). Therefore VVV is characteristic in PƒPP and so normal in GºGG. The lemma is proved. Lemma 2.10. Let G=Q‹ŠPºright-normal-factor-semidirect-productƒG=Q PG = Q ‹Š P be a non-abelian PƒPP-group of type (q,p)(q,p)( q , p ). (1) PG=GsuperscriptƒººP^{G}=GP ^ G = G. (2) G/NºG/NG / N is a non-abelian PƒPP-group of type (q,p)(q,p)( q , p ) for every proper normal subgroup NNN of GºGG. Proof. See Lemma 2.2.2 in [1]. Lemma 2.11. If AAA and BµBB are normal subgroups of GºGG, then every chief factor H/K»¾H/KH / K of GºGG below ABµABA B is GºGG-isomorphic to either a chiew factor of GºGG below AAA or a chief factor of GºGG between B©AµB AB © A and BµBB. Proof. This assertion follows from the GºGG-isomorphism AB/AƒB/(B©A)similar-to-or-equalsµµµAB/A B/(B A)A B / A ƒ B / ( B © A ) and the Jordan-HÃ¶lder theorem for the Î©Î© seties of a group (see [20, Chapter A, 3.2]). From Proposition 2.2.8 in [14] we get the following useful lemma. Lemma 2.12. Let ”” be a non-empty hereditary formation. (1) If NNN is a normal subgroup of GºGG, then (G/N)”=G”N/N.superscriptº”superscriptº”(G/N)^{ G / N ) ^ fraktur_F = G ^ fraktur_F N / N . (2) If EEE is a subgroup of GºGG, then E”¤G”superscript”superscriptº”E^{ G^{ ^ fraktur_F ¤ G ^ fraktur_F and N(NE)”=NE”superscript”superscript”N(NE)^{ ( N E ) ^ fraktur_F = N E ^ fraktur_F . Lemma 2.13. Let (D,Z(D);U1,¦,Uk)··subscript1¦subscript(D,Z(D);U_{1}, D , Z ( D ) ; U _ 1 , ¦ , U _ k ) be a Robinson complex of GºGG and NNN a normal subgroup of GºGG. (1) Ui²/(Ui²©Z(D))superscriptsubscript²superscriptsubscript²·U_{i}^{ Z(D))U _ i ^ ² / ( U _ i ^ ² © Z ( D ) ) is a simple non-abelian group and Ui²©Z(D)=Î¦(Ui²)=Z(Ui²)superscriptsubscript²·Î¦superscriptsubscript²superscriptsubscript²U_{i}^{ Z(D)= _ i ^ ² © Z ( D ) = roman_Î¦ ( U _ i ^ ² ) = Z ( U _ i ^ ² ). In particular, Ui²superscriptsubscript²U_{i}^{ _ i ^ ² is a quasi-simple group. (2) If N=Ui²superscriptsubscript²N=U_{i}^{ = U _ i ^ ² and k 11k 1k 1, then (D/N,Z(D/N);U1N/N,¦,Ui1N/N,Ui+1N/N,‹¯,NUk/N)··subscript1¦subscript1subscript1‹¯subscript(D/N,Z(D/N);U_{1}N/N, D / N , Z ( D / N ) ; U _ 1 N / N , ¦ , U _ i - 1 N / N , U _ i + 1 N / N , ‹¯ , N U _ k / N ) is a Robinson complex of G/NºG/NG / N and Ui/N=Z(D)N/N=Z(D/N)subscript··U_{i}/N=Z(D)N/N=Z(D/N)U _ i / N = Z ( D ) N / N = Z ( D / N ). (3) If NNN is nilpotent, then (DN/N,Z(D)N/N;U1N/N,¦,NUk/N)··subscript1¦subscript(DN/N,Z(D)N/N;U_{1}N/N, D N / N , Z ( D ) N / N ; U _ 1 N / N , ¦ , N U _ k / N ) is a Robinson complex of G/NºG/NG / N and Z(D)N/N=Z(D/N)··Z(D)N/N=Z(D/N)Z ( D ) N / N = Z ( D / N ). (4) If p(Z(D))‹·p ( Z ( D ) ), then p(Z(Ui²))‹superscriptsubscript²p ( Z ( U _ i ^ ² ) ) for some iii. In particular, p{2,3}23p { 2 , 3 }. Proof. Let Z:=Z(D)=Î¦(D)assign·Î¦·Z:=Z(D)= := Z ( D ) = roman_Î¦ ( D ). (1) First observe that Ui=Ui²Z=Ui”Zsubscriptsuperscriptsubscript²superscriptsubscript”U_{i}=U_{i}^{ _ i = U _ i ^ ² Z = U _ i ^ fraktur_S Z, where ”” is the class of all soluble groups, since Ui/ZsubscriptU_{i}/ZU _ i / Z is a simple non-abelian group and so Ui”¤Ui²¤Ui”superscriptsubscript”superscriptsubscript²superscriptsubscript”U_{i}^{ U_{i}^{ U_{i}^{ _ i ^ fraktur_S ¤ U _ i ^ ² ¤ U _ i ^ fraktur_S . Hence Ui”=Ui²superscriptsubscript”superscriptsubscript²U_{i}^{ _ i ^ fraktur_S = U _ i ^ ² is perfect. On the other hand, Ui/Z=Ui²Z/ZƒUi²/(Ui²©Z)subscriptsuperscriptsubscript²similar-to-or-equalssuperscriptsubscript²superscriptsubscript²U_{i}/Z=U_{i}^{ U_{i}^{ Z)U _ i / Z = U _ i ^ ² Z / Z ƒ U _ i ^ ² / ( U _ i ^ ² © Z ) is a simple non-abelian group. Therefore Ui²©Z=Î¦(Ui²)=Z(Ui²)superscriptsubscript²Î¦superscriptsubscript²superscriptsubscript²U_{i}^{ Z= _ i ^ ² © Z = roman_Î¦ ( U _ i ^ ² ) = Z ( U _ i ^ ² ) since Î¦(Ui²)¤Î¦(D)Î¦superscriptsubscript²Î¦· ( U _ i ^ ² ) ¤ roman_Î¦ ( D ). (2), (3) See Remark 1.6.8 in [14] or Lemma 3.1 in [11]. (4) Assume that p(Z(Ui²))‹superscriptsubscript²p ( Z ( U _ i ^ ² ) ) for all iii and let Z=ZpÃVsubscriptZ=Z_{p} VZ = Z _ p Ã V, where ZpsubscriptZ_{p}Z _ p is the Sylow ppp-subgroup of ZZZ. Then Zp©Ui²=1subscriptsuperscriptsubscript²1Z_{p} U_{i}^{ _ p © U _ i ^ ² = 1, so Ui²©Z=Ui²©V=Î¦(Ui²)=Z(Ui²)superscriptsubscript²superscriptsubscript²Î¦superscriptsubscript²superscriptsubscript²U_{i}^{ Z=U_{i}^{ V= _ i ^ ² © Z = U _ i ^ ² © V = roman_Î¦ ( U _ i ^ ² ) = Z ( U _ i ^ ² ) for all iii. On the other hand, D=U1‹¯Uk=ZU1²‹¯Uk²=Zp(V(U1²‹¯Uk²)D=U_{1} U_{k}=ZU_{1}^{ U_{k}^{ } U_{k}^{ = U _ 1 ‹¯ U _ k = Z U _ 1 ^ ² ‹¯ U _ k ^ ² = Z _ p ( V ( U _ 1 ^ ² ‹¯ U _ k ^ ² ), so D=V(U1²‹¯Uk²)·superscriptsubscript1²‹¯superscriptsubscript²D=V(U_{1}^{ U_{k}^{ = V ( U _ 1 ^ ² ‹¯ U _ k ^ ² ) since Zp¤Î¦(D)subscriptÎ¦·Z_{p} _ p ¤ roman_Î¦ ( D ). Hence Z¤V(U1²‹¯Uk²)superscriptsubscript1²‹¯superscriptsubscript²Z V(U_{1}^{ U_{k}^{ ¤ V ( U _ 1 ^ ² ‹¯ U _ k ^ ² ). But VVV and every subgroup Ui²superscriptsubscript²U_{i}^{ _ i ^ ² has no a composition factor of order ppp by Lemma 2.11, a contradiction. Therefore p(Z(Ui²))‹superscriptsubscript²p ( Z ( U _ i ^ ² ) ) for some iii, where Ui²superscriptsubscript²U_{i}^{ _ i ^ ² is a quasi-simple group by Part (1). But then |Z(Ui²)|superscriptsubscript²|Z(U_{i}^{ Z ( U _ i ^ ² ) | /s the order of the Schur multiplier M(Ui²/Z(Ui²))superscriptsubscript²superscriptsubscript²M(U_{i}^{ ( U _ i ^ ² / Z ( U _ i ^ ² ) ) of Ui²/Z(Ui²)superscriptsubscript²superscriptsubscript²U_{i}^{ _ i ^ ² / Z ( U _ i ^ ² ). Hence (Z(Ui²))Š†{2,3}‹superscriptsubscript²23 ( Z ( U _ i ^ ² ) ) Š† { 2 , 3 } (see Section 4.15(A) in [22, Ch. 4]. Therefore p{2,3}23p { 2 , 3 }. Hence we have (4). The lemma is proved. Lemma 2.14. Let UUU and NŠGŠºN GN Š G be subgroups of GºGG, where UUU is of prime power order. Suppose that UN/NUN/NU N / N is a modular non-subnormal subgroup of G/NºG/NG / N. Then where UGN/(UN)GsuperscriptºsubscriptºU^{G}N/(UN)_{G}U ^ G N / ( U N ) _ G is a non-abelian PƒPP-group of order prime to |K/UNG|¾subscriptº|K/UN_{G}|| K / U N _ G |. Proof. The subgroup UN/NƒU/(U©N)similar-to-or-equalsUN/N U/(U N)U N / N ƒ U / ( U © N ) of G/NºG/NG / N is of prime power order, so we can apply Lemma 2.8(1). First observe that (UN/N)G/N=(UN)G/Nsubscriptºsubscriptº(UN/N)_{G/N}=(UN)_{G}/N( U N / N ) _ G / N = ( U N ) _ G / N and (UN/N)G/N=(UN)G/N=UGN/Nsuperscriptºsuperscriptºsuperscriptº(UN/N)^{G/N}=(UN)^{G}/N=U^{G}N/N( U N / N ) ^ G / N = ( U N ) ^ G / N = U ^ G N / N. Therefore, by Lemma 2.8(1), where is a non-abelian PƒPP-group of order prime to |(K/N)/(UN/N)G/N|¾subscriptº|(K/N)/(UN/N)_{G/N}|| ( K / N ) / ( U N / N ) _ G / N | and so to |K/(UN)G|¾subscriptº|K/(UN)_{G}|| K / ( U N ) _ G |. The lemma is proved. A group GºGG is called ‹ if GºGG has a normal Hall ‹ The following lemma is well-known [20, Chapter A, 13.2]. Lemma 2.15. If H»HH is a normal subgroup of GºGG and H/(H©Î¦(G))»»Î¦ºH/(H / ( H © roman_Î¦ ( G ) ) is ‹ then H»HH is ‹ Recall that a group GºGG is said to be a PsuperscriptƒP^{*}P ^ -group if G=A‹Š¨t©ºright-normal-factor-semidirect-productdelimited-¨©¡G=A t = A ‹Š ¨ t ©, where AAA is an elementary abelian subgroup of GºGG, |t|=rn¡superscript|t|=r^{n}| t | = r ^ n for some prime rrr and t¡tt induces a power automorphism of prime order on AAA [1, p. 69]. Lemma 2.16. Let G=A‹Š¨t©ºright-normal-factor-semidirect-productdelimited-¨©¡G=A t = A ‹Š ¨ t © be a PsuperscriptƒP^{*}P ^ -group and let |¨t©|=pn.delimited-¨©¡superscript| t ¨ t © | = p ^ n . Then Z(G)=¨tp©=Î¦(G)ºdelimited-¨©superscript¡Î¦ºZ(G)= t^{p} ( G ) = ¨ t ^ p © = roman_Î¦ ( G ), G/Z(G)ººG/Z(G)G / Z ( G ) is a non-abelian PƒPP-group and the lattice (G)º{ L}(G)caligraphic_L ( G ) is modular. The following lemma is a corollary of Theorem C. Lemma 2.17. If GºGG is a soluible QƒTQ TQ ƒ T-group, then every subgroup of GºGG is a QƒTQ TQ ƒ T-group. Proof of Theorem E. First assume that GºGG is a QƒTQ TQ ƒ T-group. Then GºGG is a PTƒPTP T-group and every quotient G/NºG/NG / N is a QƒTQ TQ ƒ T-group by Lemma 2.4. Let D·DD be the soluble residual of GºGG. Then D·DD is perfect and G/Dº·G/DG / D is a soluble group QƒTQ TQ ƒ T-group, so Condition (i) holds for GºGG. Now assume that D 1·1D 1D 1. Then, in view of Theorem D, GºGG has a Robinson complex (D,Z(D);(D,Z(D);( D , Z ( D ) ; U1,¦,Uk)U_{1}, _ 1 , ¦ , U _ k ) such that for any set {i1,¦,ir}Š†{1,¦,k}subscript1¦subscript1¦ i _ 1 , ¦ , i _ r } Š† { 1 , ¦ , k }, where 1¤r<k11 r<k1 ¤ r < k, GºGG and G/Ui1²‹¯Uir²ºsuperscriptsubscriptsubscript1²‹¯superscriptsubscriptsubscript²G/U_{i_{1}}^{ U_{i_{r}}^{ / U _ i _ 1 ^ ² ‹¯ U _ i _ r ^ ² satisfy psubscript{ N}_{p}N _ p for all p{2,3}©(Z(D))23‹·p { 2 , 3 } © ( Z ( D ) ) and psubscript{ P}_{p}P _ p for all p(D)‹·p ( D ). Moreover, in view of Lemma 2.5, GºGG and G/Ui1²‹¯Uir²ºsuperscriptsubscriptsubscript1²‹¯superscriptsubscriptsubscript²G/U_{i_{1}}^{ U_{i_{r}}^{ / U _ i _ 1 ^ ² ‹¯ U _ i _ r ^ ² satisfy ƒPsubscriptƒ{ Q}_{ P}Q _ ƒ P and, in particular, satisfy ƒ(p,q)subscript{ Q}_{ _ ƒ ( p , q ) for all pairs {p,q}©(D) …‹· p , q } © ( D ) …. Hence Conditions (ii) and (iii) hold for GºGG. Therefore the necessity of the condition of the theorem holds. Now, assume, arguing by contradiction, that GºGG is a non-QƒTQ TQ ƒ T-group of minimal order satisfying Conditions (i), (ii), and (iii). Then, in view of Lemma 2.13(4) and Theorem D, GºGG is a PTƒPTP T-group, so D 1·1D 1D 1 and GºGG has a ƒ UUU such that UUU is not ƒ in GºGG but every ƒ subgroup U0subscript0U_{0}U _ 0 of GºGG with U0<Usubscript0U_{0}<UU _ 0 < U is ƒ in GºGG. Let Z=Z(D).·Z=Z(D).Z = Z ( D ) . (1) If NNN is either a non-identity normal nilpotent subgroup of GºGG or N=Ui²superscriptsubscript²N=U_{i}^{ = U _ i ^ ² for some iii, then G/NºG/NG / N is a QƒTQ TQ ƒ T-group. First assume that k=11k=1k = 1 and N=U1²superscriptsubscript1²N=U_{1}^{ = U _ 1 ^ ² . Then D²=D=U1=U1²=Nsuperscript·²·subscript1superscriptsubscript1²D^{ ^ ² = D = U _ 1 = U _ 1 ^ ² = N. Therefore G/N=G/Dºº·G/N=G/DG / N = G / D is a QƒTQ TQ ƒ T-group by Condition (i). Now assume that k>11k>1k > 1 and N=Ni²superscriptsubscript²N=N_{i}^{ = N _ i ^ ² . We can assume without loss of generality that i=11i=1i = 1. Then (G/N)/(D/N)ƒG/Dsimilar-to-or-equalsº·º·(G/N)/(D/N) G/D( G / N ) / ( D / N ) ƒ G / D is a aoluble QƒTQ TQ ƒ T-group and (D/N)²=D²/N=D/Nsuperscript·²superscript·²·(D/N)^{ D / N ) ^ ² = D ^ ² / N = D / N. From Lemma 2.13(2) it follows that (D/N,Z(D/N);U2N/N,¦,UkN/N)··subscript2¦subscript(D/N,Z(D/N);U_{2}N/N, D / N , Z ( D / N ) ; U _ 2 N / N , ¦ , U _ k N / N ) is a Robinson complex of G/NºG/NG / N and U1/N=ZN/N=Z(D/N)subscript1·U_{1}/N=ZN/N=Z(D/N)U _ 1 / N = Z N / N = Z ( D / N ), where ZN/NƒZ/(Z©N)similar-to-or-equalsZN/N Z/(Z N)Z N / N ƒ Z / ( Z © N ). Moreover, by Condition (iii), if {i1,¦,ir}Š†{2,¦,k}subscript1¦subscript2¦ i _ 1 , ¦ , i _ r } Š† { 2 , ¦ , k }, where 2¤r<k22 r<k2 ¤ r < k, then the quotients G/N=G/U1²ººsuperscriptsubscript1²G/N=G/U_{1}^{ / N = G / U _ 1 ^ ² and, in view o lemma 2.12(2), satisfy psubscript{ N}_{p}N _ p for all p{2,3}©(ZN/N)23‹p { 2 , 3 } © ( Z N / N ), psubscript{ P}_{p}P _ p for all p(D/N))Š†(D)p ( D / N ) ) Š† ( D ), and ƒ(p,q)subscript{ Q}_{ _ ƒ ( p , q ) for all pairs {p,q}©(D/N) …‹· p , q } © ( D / N ) …. Therefore the hypothesis holds for G/N=G/U1²ººsuperscriptsubscript1²G/N=G/U_{1}^{ / N = G / U _ 1 ^ ² , so G/Ui²ºsuperscriptsubscript²G/U_{i}^{ / U _ i ^ ² is a QƒTQ TQ ƒ T-group for all iii by the choice of GºGG. Similary, it can be proved that if NNN is a non-identity nilpotent normal subgroup of GºGG, then the hypothesis holds for G/NºG/NG / N and so G/NºG/NG / N is a QƒTQ TQ ƒ T-group. (2) UUU is supersoluble. It is clear that D=G”=G”·superscriptº”superscriptº”D=G^{ = G ^ fraktur_S = G ^ fraktur_U and UUi²/Ui²superscriptsubscript²superscriptsubscript²UU_{i}^{ U _ i ^ ² / U _ i ^ ² is ƒ in G/Ui²ºsuperscriptsubscript²G/U_{i}^{ / U _ i ^ ² by Lemma 2.3(2). Therefore UUi²/Ui²superscriptsubscript²superscriptsubscript²UU_{i}^{ U _ i ^ ² / U _ i ^ ² is ƒ in G/Ui²ºsuperscriptsubscript²G/U_{i}^{ / U _ i ^ ² by Claim (1) for all iii. Hence UUU is supersoluble by Lemma 2.6(1). (3) Suppose that NNN is either a non-identity normal nilpotent subgroup of GºGG or N=Ui²superscriptsubscript²N=U_{i}^{ = U _ i ^ ² for some iii. If X‹XX is a subgroup of GºGG such that XN/N‹XN/NX N / N is ƒ in G/NºG/NG / N, then XN/N‹XN/NX N / N is ƒ in G/NºG/NG / N and XN‹XNX N is ƒ in GºGG. In particular, UG=1subscriptº1U_{G}=1U _ G = 1. In view of Claim (1), G/NºG/NG / N is a QƒTQ TQ ƒ T-group, so XN/N‹XN/NX N / N is ƒ in G/NºG/NG / N and hence XN‹XNX N is ƒ in GºGG by Lemma 2.1(3). Therefore, since UUU is supersoluble by Claim (2), the choice of UUU implies that UG=1subscriptº1U_{G}=1U _ G = 1. (4) UUU is a cyclic ppp-group for some prime ppp and V:=U©Z(G)assignsubscriptºV:=U Z_{ := U © Z _ ( G ) is the maximal subgroup of UUU. Let NNN be a nilpotent non-identity normal subgroup of GºGG. Then UN/NUN/NU N / N is ƒ in G/NºG/NG / N by Lemma 2.3(2), so UN/NUN/NU N / N is ƒ in G/NºG/NG / N by Claim (1). Hence UUU is a cyclic ppp-group for some prime ppp by Claim (2) and Lemma 2.6(2). Now, let VVV be the maximal subgroup of UUU. Then V=U”superscriptsuperscript”V=U^{{ = U ^ fraktur_A ^ is subnormal in GºGG by Lemma 2.3(4) since UUU is a cyclic ppp-group, hence VVV is quasinormal in GºGG since GºGG is a PTƒPTP T-group. Therefore V¤Z(G)subscriptºV Z_{ ¤ Z _ ( G ) by [14, Corollary 1.5.6] since VG=1=UGsubscriptº1subscriptºV_{G}=1=U_{G}V _ G = 1 = U _ G by Claim (3). (5) GºGG has a normal subgroup Cqsubscript¶C_{q}C _ q of order qqq for some prime qqq. If Z 11Z 1Z 1, it is clear. Now assume that Z=11Z=1Z = 1. Then D=U1Ã‹¯ÃUk·subscript1‹¯subscriptD=U_{1} U_{k}D = U _ 1 Ã ‹¯ Ã U _ k , where UisubscriptU_{i}U _ i is a simple non-abelian minimal normal subgroup of GºGG for all iii. Let E=UiUsubscriptE=U_{i}UE = U _ i U, where Ui°Unot-less-than-nor-greater-thansubscriptU_{i} UU _ i ° U. We show that Ui¤NG(U)subscriptsubscriptºU_{i} N_{G}(U)U _ i ¤ N _ G ( U ). Since UUU is a ƒ subgroup of EEE by Lemma 2.3(1), there is a subgroup chain U=E0<E1<‹¯<Et1<Et=Esubscript0subscript1‹¯subscript¡1subscript¡U=E_{0}<E_{1}< = E _ 0 < E _ 1 < ‹¯ < E _ t - 1 < E _ t = E such that Ei1subscript1E_{i-1}E _ i - 1 is a maximal ƒ subgroup of EisubscriptE_{i}E _ i for all i=1,¦,t1¦¡i=1, = 1 , ¦ , t and for M=Et1subscript¡1M=E_{t-1}M = E _ t - 1 we have M=U(M©Ui)subscriptM=U(M U_{i})M = U ( M © U _ i ). Moreover, M©UisubscriptM U_{i}M © U _ i is ƒ in EEE and M©Ui<UisubscriptsubscriptM U_{i}<U_{i}M © U _ i < U _ i since M<EM<EM < E, so M©Ui=1subscript1M U_{i}=1M © U _ i = 1. Therefore U=MU=MU = M is a maximal ƒ subgroup of EEE. Assume that UUU is not normal in EEE. Then, by Lemma 2.2, E/UEsubscriptE/U_{E}E / U _ E is a group of order qrqrq r for primes qqq and rrr. But this is imposible since UiƒUiUE/UE¤E/UEsimilar-to-or-equalssubscriptsubscriptsubscriptsubscriptsubscriptU_{i} U_{i}U_{E}/U_{E} E/U_{E}U _ i ƒ U _ i U _ E / U _ E ¤ E / U _ E . Therefore Ui¤NE(U)subscriptsubscriptU_{i} N_{E}(U)U _ i ¤ N _ E ( U ) for all iii, so D¤NG(U)·subscriptºD N_{G}(U)D ¤ N _ G ( U ) and hence U©D¤Op(D)=1·subscript‚·1U D O_{p}(D)=1U © D ¤ O _ p ( D ) = 1 by Claim (4). It follows than DU=DÃU··DU=D UD U = D Ã U, so 1<U¤CG(D)1subscript¶º·1<U C_{G}(D)1 < U ¤ C _ G ( D ). But CG(D)©D=1subscript¶º··1C_{G}(D) D=1C _ G ( D ) © D = 1 since Z=Z(D)=1·1Z=Z(D)=1Z = Z ( D ) = 1. Therefore CG(D)ƒCG(D)D/Dsimilar-to-or-equalssubscript¶º·subscript¶º···C_{G}(D) C_{G}(D)D/DC _ G ( D ) ƒ C _ G ( D ) D / D is soluble. Hence for some prime qqq dividing |CG(D)|subscript¶º·|C_{G}(D)|| C _ G ( D ) | we have Oq(CG(D)) 1subscript‚subscript¶º·1O_{q}(C_{G}(D)) 1O _ q ( C _ G ( D ) ) 1. But Oq(CG(D))subscript‚subscript¶º·O_{q}(C_{G}(D))O _ q ( C _ G ( D ) ) is characteristic in CG(D)subscript¶º·C_{G}(D)C _ G ( D ), so Oq(CG(D))subscript‚subscript¶º·O_{q}(C_{G}(D))O _ q ( C _ G ( D ) ) is normal in GºGG and hence we have (5). (6) UGsuperscriptºU^{G}U ^ G is soluble. The subgroup CqU/Cqsubscript¶subscript¶C_{q}U/C_{q}C _ q U / C _ q is ƒ in G/Cqºsubscript¶G/C_{q}G / C _ q by Lemma 2.3(2), so this subgroup is ƒ and hence modular in G/Cqºsubscript¶G/C_{q}G / C _ q by Claim (3). First assume that CqU/Cqsubscript¶subscript¶C_{q}U/C_{q}C _ q U / C _ q is not subnormal in G/Cqºsubscript¶G/C_{q}G / C _ q . Then, by Lemma 2.14, CqUG/(CqU)Gsubscript¶superscriptºsubscriptsubscript¶ºC_{q}U^{G}/(C_{q}U)_{G}C _ q U ^ G / ( C _ q U ) _ G is a non-abelian PƒPP-group, so CqUG/(CqU)Gsubscript¶superscriptºsubscriptsubscript¶ºC_{q}U^{G}/(C_{q}U)_{G}C _ q U ^ G / ( C _ q U ) _ G is soluble and hence is soluble since CqUsubscript¶C_{q}UC _ q U is soluble. Hence UGsuperscriptºU^{G}U ^ G is soluble. Now assume that CqU/Cqsubscript¶subscript¶C_{q}U/C_{q}C _ q U / C _ q is subnormal in G/Cqºsubscript¶G/C_{q}G / C _ q , so by Claim (4). Hence UGsuperscriptºU^{G}U ^ G is soluble. (7) UUU is not subnormal in GºGG. Assume that UUU is subnormal in GºGG. Then UUU is quasinormal and so ƒ in GºGG since GºGG is a PTƒPTP T-group, a contradiction. Hence we have (7). (8) |U|=p|U|=p| U | = p. Assume that |U|>p|U|>p| U | > p. Then 1<V¤R:=Op(Z(G))1…assignsubscript‚subscriptº1<V R:=O_{p}(Z_{ < V ¤ R := O _ p ( Z _ ( G ) ) by Claim (4) and U°Rnot-less-than-nor-greater-than…U RU ° R by Claim (7). Denote E=RU…E=RUE = R U. Then EG=UGRsuperscriptºsuperscriptº…E^{G}=U^{G}RE ^ G = U ^ G R and, in view of Claims (4) and (7), EEE is not subnormal in GºGG. Moreover, EEE is ƒ and so modular in GºGG by Claim (3). The group UR/RƒU/(U©R)=U/Vsimilar-to-or-equals………UR/R U/(U R)=U/VU R / R ƒ U / ( U © R ) = U / V has order ppp, so (RU)G=Rsubscript…º…(RU)_{G}=R( R U ) _ G = R. In view of Lemma 2.14, where RUG/RƒUG/(UG©R)similar-to-or-equals…superscriptº…superscriptºsuperscriptº…RU^{G}/R U^{G}/(U^{G} R)R U ^ G / R ƒ U ^ G / ( U ^ G © R ) is a non-abelian PƒPP-group of order prime to K/R¾…K/RK / R. The group UG/(UG©R)superscriptºsuperscriptº…U^{G}/(U^{G} R)U ^ G / ( U ^ G © R ) is qqq-closed for some prime qqq dividing its order and so UGsuperscriptºU^{G}U ^ G is qqq-closed by Lemma 2.9 since UG©R¤Z(UG)superscriptº…subscriptsuperscriptºU^{G} R Z_{ ^ G © R ¤ Z _ ( U ^ G ). If QQQ is the normal Sylow qqq-subgroup of UGsuperscriptºU^{G}U ^ G , then U°Qnot-less-than-nor-greater-thanU QU ° Q by Claim (7), so q pq pq p. Therefore UG/(UG©R)superscriptºsuperscriptº…U^{G}/(U^{G} R)U ^ G / ( U ^ G © R ) is a non-abelian PƒPP-group of type (q,p)(q,p)( q , p ), so UG=Q‹ŠPsuperscriptºright-normal-factor-semidirect-productƒU^{G}=Q PU ^ G = Q ‹Š P, where PƒPP is a non-normal Sylow ppp-subgroup of UGsuperscriptºU^{G}U ^ G , containing UUU, and QQQ is subnormal in GºGG. In particular, UGsuperscriptºU^{G}U ^ G and RUG/R…superscriptº…RU^{G}/RR U ^ G / R are ‹ where ={p,q}‹ = { p , q }, so GºGG is ‹ and hence D·DD and D/Z·D/ZD / Z are ‹ groups. Assume that UG©D 1superscriptº·1U^{G} D 1U ^ G © D 1. Since UG©D¤Z¤Î¦(D)superscriptº·Î¦·U^{G} D Z ^ G © D ¤ Z ¤ roman_Î¦ ( D ) by Claim (6), for some iii and for some r{p,q}r { p , q } the mumber rrr /s |Ui/Z|subscript|U_{i}/Z|| U _ i / Z |. It follows that Ui/ZsubscriptU_{i}/ZU _ i / Z is an abelian group, a contradiction. Therefore UG©D=1superscriptº·1U^{G} D=1U ^ G © D = 1 and so where G/Dº·G/DG / D is a soluble QƒTQ TQ ƒ T-group by Condition (i). Therefore, in view of Theorem C, G/D=T‹ŠLº·right-normal-factor-semidirect-product¿G/D=T LG / D = T ‹Š L, where T=(G/D)”ƒsuperscriptº·subscript”T=(G/D)^{ = ( G / D ) ^ fraktur_N _ ƒ and the following hold: TTT is an abelian Hall subgroup of G/Dº·G/DG / D and all subgroups of TTT are normal G/Dº·G/DG / D and the lattice (L)¿{ L}(L)caligraphic_L ( L ), of all subgroups of L¿LL, is modular. Then PD/D°Tnot-less-than-nor-greater-thanƒ··PD/D TP D / D ° T, so UD/D¤PD/D¤Lx··ƒ··superscript¿¥UD/D PD/D L^{x}U D / D ¤ P D / D ¤ L ^ x for some xG/D¥º·x G/Dx G / D. First assume that QD/D¤T··QD/D TQ D / D ¤ T. Since UD/D··UD/DU D / D is a ƒ ppp-subgroup of G/Dº·G/DG / D by Lemmma 2.3(2), T¤CG/D(UD/D)subscript¶º···T C_{G/D}(UD/D)T ¤ C _ G / D ( U D / D ) by Lemma 2.7, so a contradiction. Hence QD/D°Tnot-less-than-nor-greater-than··QD/D TQ D / D ° T and so (QD/D)‹Š(PD/D)¤Lxright-normal-factor-semidirect-product··ƒ··superscript¿¥(QD/D) L^{x}( Q D / D ) ‹Š ( P D / D ) ¤ L ^ x since TTT and Lxsuperscript¿¥L^{x}L ^ x are Hall subgroups of G/Dº·G/DG / D and QD/D··QD/DQ D / D is a subnormal qqq-subgroup of G/Dº·G/DG / D. In view of Theorem 2.4.4 in [1], Lxsuperscript¿¥L^{x}L ^ x is a direct product of PsuperscriptƒP^{*}P ^ -groups PisubscriptƒP_{i}P _ i and primary groups QjsubscriptQ_{j}Q _ j (that is, QjsubscriptQ_{j}Q _ j is of prime power order) with relatively prime orders. Then for any factor QjsubscriptQ_{j}Q _ j of Lxsuperscript¿¥L^{x}L ^ x we have Qj¤Z(Lx)subscriptsubscriptsuperscript¿¥Q_{j} Z_{ _ j ¤ Z _ ( L ^ x ), so QD/D°Qjnot-less-than-nor-greater-than··subscriptQD/D Q_{j}Q D / D ° Q _ j and PD/D°Qjnot-less-than-nor-greater-thanƒ··subscriptPD/D Q_{j}P D / D ° Q _ j for all jjj since UGƒUGD/DƒQD/D‹ŠPD/Dsimilar-to-or-equalssuperscriptºsuperscriptº··similar-to-or-equalsright-normal-factor-semidirect-product··ƒ··U^{G} U^{G}D/D QD/D PD/DU ^ G ƒ U ^ G D / D ƒ Q D / D ‹Š P D / D is not nilpotent. Therefore for some iii and kkk we have QD/D¤Pi··subscriptƒQD/D P_{i}Q D / D ¤ P _ i and PD/D¤Pkƒ··subscriptƒPD/D P_{k}P D / D ¤ P _ k , where [Pi,Pk]=1subscriptƒsubscriptƒ1[P_{i},P_{k}]=1[ P _ i , P _ k ] = 1 for i ki ki k, so i=ki=ki = k. Hence QD/D‹ŠPD/D¤Pi=A‹Š¨t©right-normal-factor-semidirect-product··ƒ··subscriptƒright-normal-factor-semidirect-productdelimited-¨©¡QD/D PD/D P_{i}=A t D / D ‹Š P D / D ¤ P _ i = A ‹Š ¨ t ©, where AAA is elementary abelian subgroup of PisubscriptƒP_{i}P _ i , |t|=rn¡superscript|t|=r^{n}| t | = r ^ n for some prime rrr and t¡tt induces a power automorphism of prime order on AAA. Therefore AAA is a qqq-group and t¡tt is a ppp-element of PksubscriptƒP_{k}P _ k by Lemma 2.16. Hence PƒPD/Dsimilar-to-or-equalsƒƒ··P PD/DP ƒ P D / D is a cyclic ppp-group. Since UG/(UG©R)superscriptºsuperscriptº…U^{G}/(U^{G} R)U ^ G / ( U ^ G © R ) is a non-abelian PƒPP-group and U°UG©Rnot-less-than-nor-greater-thansuperscriptº…U U^{G} RU ° U ^ G © R, U(UG©R)/(UG©R)superscriptº…superscriptº…U(U^{G} R)/(U^{G} R)U ( U ^ G © R ) / ( U ^ G © R ) is a Sylow ppp-subgroup of UG/(UG©R)superscriptºsuperscriptº…U^{G}/(U^{G} R)U ^ G / ( U ^ G © R ) and so U(UG©R)superscriptº…U(U^{G} R)U ( U ^ G © R ) is a cyclic Sylow ppp-subgroup of UGsuperscriptºU^{G}U ^ G . It follows that either U(UG©R)=Usuperscriptº…U(U^{G} R)=UU ( U ^ G © R ) = U or U(UG©R)=UG©Rsuperscriptº…superscriptº…U(U^{G} R)=U^{G} RU ( U ^ G © R ) = U ^ G © R. In the former case we have UG©R=V¤UGsuperscriptº…subscriptºU^{G} R=V U_{G}U ^ G © R = V ¤ U _ G , which is impossible by Claim (3), so U(UG©R)=UG©Rsuperscriptº…superscriptº…U(U^{G} R)=U^{G} RU ( U ^ G © R ) = U ^ G © R and hence UUU is subormal in GºGG, contrary to Claim (7). Therefore we have (8). (9) U°Dnot-less-than-nor-greater-than·U DU ° D. Assume U¤D·U DU ¤ D. From Claim (7) it follows that U°Znot-less-than-nor-greater-thanU ZU ° Z and then, by Claim (8) and Lemma 2.3(1)(2)(5), for some iii we have UƒUZ/Z=Ui/Zsimilar-to-or-equalssubscriptU UZ/Z=U_{i}/ZU ƒ U Z / Z = U _ i / Z, a contradiction. Hence we have (9). (10) Op(D)=1subscript‚·1O_{p}(D)=1O _ p ( D ) = 1. Assume that GºGG has a normal subgroup Zp¤Z=Î¦(D)subscriptÎ¦·Z_{p} Z= _ p ¤ Z = roman_Î¦ ( D ) of order ppp. Then ZpUsubscriptZ_{p}UZ _ p U is not subnormal in GºGG by Claim (7) and, also, (ZpU)G=Zpsubscriptsubscriptºsubscript(Z_{p}U)_{G}=Z_{p}( Z _ p U ) _ G = Z _ p by Claim (8) and (ZpU)G=ZpUGsuperscriptsubscriptºsubscriptsuperscriptº(Z_{p}U)^{G}=Z_{p}U^{G}( Z _ p U ) ^ G = Z _ p U ^ G , so G/ZpƒZpUG/ZpÃK/Zp,similar-to-or-equalsºsubscriptsubscriptsuperscriptºsubscript¾subscriptG/Z_{p} Z_{p}U^{G}/Z_{p} K/Z_{p},G / Z _ p ƒ Z _ p U ^ G / Z _ p Ã K / Z _ p , where ZpUG/ZpsubscriptsuperscriptºsubscriptZ_{p}U^{G}/Z_{p}Z _ p U ^ G / Z _ p is a non-abelian PƒPP-group of order paqbsuperscriptsuperscriptp^{a}q^{b}p ^ a q ^ b prime to |K/Zp|¾subscript|K/Z_{p}|| K / Z _ p | by Lemma 2.14. Hence G/ZpºsubscriptG/Z_{p}G / Z _ p , D/Zp·subscriptD/Z_{p}D / Z _ p , and D·DD are {p,q} p , q }-soluble and ppp /s |D/Zp|·subscript|D/Z_{p}|| D / Z _ p |. Hence Op(D/Z) 1subscript‚·1O_{p}(D/Z) 1O _ p ( D / Z ) 1. This contradiction completes the proof of the claim. (11) Ui²U=Ui²ÃUsuperscriptsubscript²superscriptsubscript²U_{i}^{ UU _ i ^ ² U = U _ i ^ ² Ã U and so Ui²Usuperscriptsubscript²U_{i}^{ _ i ^ ² U is not subnormal in GºGG for all iii. In view of Claims (8) and (9), it is enough to show that Ui²¤NG(U)superscriptsubscript²subscriptºU_{i}^{ N_{G}(U)U _ i ^ ² ¤ N _ G ( U ). By Lemma 2.13(1), Ui²©Z=Î¦(Ui²)=Z(Ui²)superscriptsubscript²Î¦superscriptsubscript²superscriptsubscript²U_{i}^{ Z= _ i ^ ² © Z = roman_Î¦ ( U _ i ^ ² ) = Z ( U _ i ^ ² ) and Ui²/Î¦(Ui²)superscriptsubscript²Î¦superscriptsubscript²U_{i}^{ _ i ^ ² / roman_Î¦ ( U _ i ^ ² ) is a simple non-abelian group. In particular, Ui²superscriptsubscript²U_{i}^{ _ i ^ ² is quasi-simple. Let E=Ui²U=Ui²‹ŠUsuperscriptsubscript²right-normal-factor-semidirect-productsuperscriptsubscript²E=U_{i}^{ UE = U _ i ^ ² U = U _ i ^ ² ‹Š U. Then E²=Ui²superscript²superscriptsubscript²E^{ ^ ² = U _ i ^ ² . Let U=E0<E1<‹¯<Et1<Et=Esubscript0subscript1‹¯subscript¡1subscript¡U=E_{0}<E_{1}< = E _ 0 < E _ 1 < ‹¯ < E _ t - 1 < E _ t = E be a subgroup chain such that Ei1subscript1E_{i-1}E _ i - 1 is a maximal ƒ subgroup of EisubscriptE_{i}E _ i for all i=1,¦,t1¦¡i=1, = 1 , ¦ , t and for M=Et1subscript¡1M=E_{t-1}M = E _ t - 1 we have M=U(M©Ui²)superscriptsubscript²M=U(M U_{i}^{ = U ( M © U _ i ^ ² ). Then, by Lemma 2.2, we have either M=Et1subscript¡1M=E_{t-1}M = E _ t - 1 is a maximal normal subgroup of EEE or MMM is a maximal subgroup of EEE such that E/MEsubscriptE/M_{E}E / M _ E is a ƒ non-abelian group of order qrqrq r for some primes qqq and rrr. First assume that MMM is normal in EEE. From E=Ui²U=Ui²Msuperscriptsubscript²superscriptsubscript²E=U_{i}^{ = U _ i ^ ² U = U _ i ^ ² M it follows that E/MƒUi²/(M©Ui²)similar-to-or-equalssuperscriptsubscript²superscriptsubscript²E/M U_{i}^{ U_{i}^{ / M ƒ U _ i ^ ² / ( M © U _ i ^ ² ) is a simple group and so Ui²/(M©Ui²)superscriptsubscript²superscriptsubscript²U_{i}^{ U_{i}^{ _ i ^ ² / ( M © U _ i ^ ² ) is a simple non-abelian group since Ui²superscriptsubscript²U_{i}^{ _ i ^ ² is perfect. Therefore M©Ui²=Ui²©Z=Î¦(Ui²)superscriptsubscript²superscriptsubscript²Î¦superscriptsubscript²M U_{i}^{ Z= © U _ i ^ ² = U _ i ^ ² © Z = roman_Î¦ ( U _ i ^ ² ) is a p²superscript²p^{ ^ ² -group by Claim (10), so UUU is a Sylow ppp-subgroup of M=U(M©Ui²)superscriptsubscript²M=U(M U_{i}^{ = U ( M © U _ i ^ ² ). Then, by the Frattini argument, E=MNE(U)=(M©Ui²)NE(U)=Î¦(Ui²)NE(U)subscriptsuperscriptsubscript²subscriptÎ¦superscriptsubscript²subscriptE=MN_{E}(U)=(M U_{i}^{ = M N _ E ( U ) = ( M © U _ i ^ ² ) N _ E ( U ) = roman_Î¦ ( U _ i ^ ² ) N _ E ( U ). But Î¦(Ui²)¤Î¦(E)Î¦superscriptsubscript²Î¦ ( U _ i ^ ² ) ¤ roman_Î¦ ( E ), therefore NE(U)=EsubscriptN_{E}(U)=EN _ E ( U ) = E and so Ui²¤NG(U)superscriptsubscript²subscriptºU_{i}^{ N_{G}(U)U _ i ^ ² ¤ N _ G ( U ). Finally, assume that E/MEsubscriptE/M_{E}E / M _ E is a non-abelian group of order qrqrq r with V/ME=(E/ME)²subscriptsuperscriptsubscript²V/M_{E}=(E/M_{E})^{ / M _ E = ( E / M _ E ) ^ ² . Then |(E/ME)/(E/ME)²|=(E/ME)/(V/ME)=|E/V|subscriptsuperscriptsubscript²subscriptsubscript|(E/M_{E})/(E/M_{E})^{ ( E / M _ E ) / ( E / M _ E ) ^ ² | = ( E / M _ E ) / ( V / M _ E ) = | E / V | is a prime, so V=Ui²superscriptsubscript²V=U_{i}^{ = U _ i ^ ² . Hence ME¤Ui²subscriptsuperscriptsubscript²M_{E} U_{i}^{ _ E ¤ U _ i ^ ² and Ui²/MEsuperscriptsubscript²subscriptU_{i}^{ _ i ^ ² / M _ E is a non-identity soluble group, so Ui²superscriptsubscript²U_{i}^{ _ i ^ ² is not perfect. This contradiction shows that we have (11). (12) UGsuperscriptºU^{G}U ^ G is not a non-abelian PƒPP-group. Assume that UGsuperscriptºU^{G}U ^ G is a non-abelian PƒPP-group. Then, in view of Claim (7), UG=Q‹ŠUsuperscriptºright-normal-factor-semidirect-productU^{G}=Q UU ^ G = Q ‹Š U is of type (q,p)(q,p)( q , p ) for some prime qqq. Let ={q,p}‹ = { q , p }. First suppose that ©(D)=…‹‹· © ( D ) = …. Then UG©D=1superscriptº·1U^{G} D=1U ^ G © D = 1, so [UG,D]=1superscriptº·1[U^{G},D]=1[ U ^ G , D ] = 1 and GºGG is ‹ We show that G/Dº·G/DG / D is ‹ Let N=U1²superscriptsubscript1²N=U_{1}^{ = U _ 1 ^ ² and F=NU=NÃU¹F=NU=N UF = N U = N Ã U. Then FG=NUGsuperscript¹ºsuperscriptºF^{G}=NU^{G}F ^ G = N U ^ G and FG=Nsubscript¹ºF_{G}=NF _ G = N by Claim (8). In view of Claims (3) and (7), F/N¹F/NF / N is not subnormal but modular in G/NºG/NG / N and so where NUG/N=O(G/N)superscriptºsubscript‚‹ºNU^{G}/N=O_{ U ^ G / N = O _ ( G / N ) and K/N=O²(G/N)¾subscript‚superscript‹²ºK/N=O_{ / N = O _ ^ ² ( G / N ), by Lemma 2.14. Therefore G/NºG/NG / N is ‹ Hence G/Dº·G/DG / D is ‹ Let EEE be a minimal supplement to D·DD in GºGG. Then E©D¤Î¦(E)·Î¦E D © D ¤ roman_Î¦ ( E ), so EEE is soluble and ‹ that is, E=O(E)ÃO²(E)subscript‚‹subscript‚superscript‹²E=O_{ O_{ = O _ ( E ) Ã O _ ^ ² ( E ) by Lemma 2.15 since G/DƒE/(E©D)similar-to-or-equalsº··G/D E/(E D)G / D ƒ E / ( E © D ). Let xGr¥subscriptºx G_{r}x G _ r , where GrsubscriptºG_{r}G _ r be a Sylow rrr-subgroup of GºGG. Assume that r‹r . Then for some Sylow rrr-subgroup Drsubscript·D_{r}D _ r of D·DD and a Sylow rrr-subgroup ErsubscriptE_{r}E _ r of EEE and some yG¦ºy Gy G we have Gr=DrErysubscriptºsubscript·superscriptsubscript¦G_{r}=D_{r}E_{r}^{y}G _ r = D _ r E _ r ^ y . Hence x=de¥x=dex = d e, where dDrsubscript·d D_{r}d D _ r and eErysuperscriptsubscript¦e E_{r}^{y}e E _ r ^ y . Then d¤CG(U)subscript¶ºd C_{G}(U)d ¤ C _ G ( U ) since [UG,D]=1superscriptº·1[U^{G},D]=1[ U ^ G , D ] = 1. Since |G:Ery|=|DEry:Ery|=|D:D©Ery||G:E_{r}^{y}|=|DE_{r}^{y}:E_{r}^{y}|=|D:D E_{r}^{y}|| G : E _ r ^ y | = | D E _ r ^ y : E _ r ^ y | = | D : D © E _ r ^ y | is a ²superscript‹² ^ ² -mumber, the Hall ‹ O(Ey)subscript‚‹superscript¦O_{ _ ( E ^ y ) of Eysuperscript¦E^{y}E ^ y is a Hall ‹ of GºGG. Hence UG¤O(Ey)superscriptºsubscript‚‹superscript¦U^{G} O_{ ^ G ¤ O _ ( E ^ y ) and so e¤CG(U)subscript¶ºe C_{G}(U)e ¤ C _ G ( U ) since eO²(Ey)subscript‚superscript‹²superscript¦e O_{ O _ ^ ² ( E ^ y ). Therefore x¤CG(U)¥subscript¶ºx C_{G}(U)x ¤ C _ G ( U ) and hence UUU is normal and so modular in ¨x,U©¥ x,U x , U ©. Now let r‹r . Then V=UGGrsuperscriptºsubscriptºV=U^{G}G_{r}V = U ^ G G _ r is a ‹ GºGG, so V©D=1·1V D=1V © D = 1 and therefore VƒVD/Dsimilar-to-or-equals··V VD/DV ƒ V D / D is a soluble QƒTQ TQ ƒ T-group by Lemma 2.17, so UUU is ƒ and so modular in VVV. Hence UUU is modular in ¨x,U©¥ x,U x , U © by [1, Page 201, Property (2)]. Therefore UUU is modular in GºGG by Lemma 2.8(2) and so UUU is ƒ in GºGG, a contradiction. Finally, if ©(D) …‹‹· © ( D ) …, then GºGG satisfies ƒ(p,q)subscript{ Q}_{ _ ƒ ( p , q ) by Condition (iii), so UUU is modular and so ƒ in GºGG. This contradiction completes the proof of the claim. (13) GºGG has a normal subgroup Cqsubscript¶C_{q}C _ q of prime order qqq such that Cq¤Z(U1²)=Î¦(U1²)subscript¶superscriptsubscript1²Î¦superscriptsubscript1²C_{q} Z(U_{1}^{ _ q ¤ Z ( U _ 1 ^ ² ) = roman_Î¦ ( U _ 1 ^ ² ). Let E=U1²U=U1²ÃUsuperscriptsubscript1²superscriptsubscript1²E=U_{1}^{ UE = U _ 1 ^ ² U = U _ 1 ^ ² Ã U. Then EEE is modular and not subnormal in GºGG by Claims (3) and (7). Moreover, EG=U1²subscriptºsuperscriptsubscript1²E_{G}=U_{1}^{ _ G = U _ 1 ^ ² by Claim (8) and E/U1²ƒUsimilar-to-or-equalssuperscriptsubscript1²E/U_{1}^{ UE / U _ 1 ^ ² ƒ U is a modular non-subnormal subgroup of G/U1²ºsuperscriptsubscript1²G/U_{1}^{ / U _ 1 ^ ² . Hence is a non-abelian PƒPP-group by Lemma 2.8(1). Hence 1<UG©U1²¤Z(U1²)1superscriptºsuperscriptsubscript1²superscriptsubscript1²1<U^{G} U_{1}^{ Z(U_{1}^{ < U ^ G © U _ 1 ^ ² ¤ Z ( U _ 1 ^ ² ) by Claims (6) and (12). Hence GºGG has a normal subgroup Cqsubscript¶C_{q}C _ q of prime order qqq such that Cq¤U1²subscript¶superscriptsubscript1²C_{q} U_{1}^{ _ q ¤ U _ 1 ^ ² . But U1²superscriptsubscript1²U_{1}^{ _ 1 ^ ² is a quasi-simple group by Lemma 2.13(1) and so Cq¤Z(U1²)=Î¦(U1²)subscript¶superscriptsubscript1²Î¦superscriptsubscript1²C_{q} Z(U_{1}^{ _ q ¤ Z ( U _ 1 ^ ² ) = roman_Î¦ ( U _ 1 ^ ² ). Final contradiction. From Claims (7), (9) and (11) it follows that E=CqU=CqÃUsubscript¶subscript¶E=C_{q}U=C_{q} UE = C _ q U = C _ q Ã U is not subnormal in GºGG and, in view of Claim (8), EG=Cqsubscriptºsubscript¶E_{G}=C_{q}E _ G = C _ q . Hence G/EGƒEG/EGÃK/EG,similar-to-or-equalsºsubscriptºsuperscriptºsubscriptº¾subscriptºG/E_{G} E^{G}/E_{G} K/E_{G},G / E _ G ƒ E ^ G / E _ G Ã K / E _ G , where is a non-abelian PƒPP-group of order prime to |K/Cq|¾subscript¶|K/C_{q}|| K / C _ q | by Lemma 2.8(1). Hence GºGG is a ‹ group, where =(UG/(Cq©UG))‹‹superscriptºsubscript¶superscriptº U^{G})) = ( U ^ G / ( C _ q © U ^ G ) ). Then D/Z·D/ZD / Z is ‹ But Cq¤Î¦(U1²)¤Î¦(D)=Zsubscript¶Î¦superscriptsubscript1²Î¦·C_{q} _ q ¤ roman_Î¦ ( U _ 1 ^ ² ) ¤ roman_Î¦ ( D ) = Z by Claim (1), so qqq /s |D/Z|·|D/Z|| D / Z |. Hence qqq does not /s |CqUG/Cq|subscript¶superscriptºsubscript¶|C_{q}U^{G}/C_{q}|| C _ q U ^ G / C _ q |. If Cq©UG=1subscript¶superscriptº1C_{q} U^{G}=1C _ q © U ^ G = 1, then UGƒCqUG/Cqsimilar-to-or-equalssuperscriptºsubscript¶superscriptºsubscript¶U^{G} C_{q}U^{G}/C_{q}U ^ G ƒ C _ q U ^ G / C _ q is a non-abelian PƒPP-group, contrary to Claim (12), so Cq¤UGsubscript¶superscriptºC_{q} U^{G}C _ q ¤ U ^ G . Then Cqsubscript¶C_{q}C _ q is a Sylow qqq-subgroup of UGsuperscriptºU^{G}U ^ G . Hence UG=Cq‹Š(R‹ŠU)superscriptºright-normal-factor-semidirect-productsubscript¶right-normal-factor-semidirect-product…U^{G}=C_{q} U)U ^ G = C _ q ‹Š ( R ‹Š U ), where R‹ŠUƒUG/Cqsimilar-to-or-equalsright-normal-factor-semidirect-product…superscriptºsubscript¶R U U^{G}/C_{q}R ‹Š U ƒ U ^ G / C _ q is a non-abelian PƒPP-group. Let C=CUG(Cq)¶subscript¶superscriptºsubscript¶C=C_{U^{G}}(C_{q})C = C _ U ^ G ( C _ q ). Then U¤C¶U CU ¤ C by Claim (11) and so, by Lemma 2.10(1), R‹ŠU=UR‹ŠU¤Cright-normal-factor-semidirect-product…superscriptright-normal-factor-semidirect-product…¶R U=U^{R U} CR ‹Š U = U ^ R ‹Š U ¤ C. Hence Cq¤Z(UG)subscript¶superscriptºC_{q} Z(U^{G})C _ q ¤ Z ( U ^ G ). Therefore UG=CqÃ(R‹ŠU)superscriptºsubscript¶right-normal-factor-semidirect-product…U^{G}=C_{q} U)U ^ G = C _ q Ã ( R ‹Š U ), where R‹ŠUright-normal-factor-semidirect-product…R UR ‹Š U is characterisric in UGsuperscriptºU^{G}U ^ G and so it is normal in GºGG. But then UG=R‹ŠU Cq‹Š(R‹ŠU)superscriptºright-normal-factor-semidirect-product…right-normal-factor-semidirect-productsubscript¶right-normal-factor-semidirect-product…U^{G}=R U C_{q} U)U ^ G = R ‹Š U C _ q ‹Š ( R ‹Š U ), a contradiction. The theorem is proved. Proof of Theorem F. In view of Example 1.2(i), Teorem F is a special case of Theorem E, where ƒ={™}™ = { P }. 1. First Consider the special case of Theorem E where ƒ=ƒ1={{p1},¦,{pn},²}superscript1‹subscript1¦subscriptsuperscript‹² = ƒ ^ 1 = { { p _ 1 } , ¦ , { p _ n } , ^ ² } and ={p1,¦,pn}‹subscript1¦subscript = { p _ 1 , ¦ , p _ n } (see Example 1.2(iii)). In this case we say that GºGG is a Q1T1‹Q1 TQ 1 T-group if 11‹1 -quasinormality is a transitive relation on GºGG, and we also say in this case that GºGG satisfies 1(p,q)subscript1‹{ Q}_{1 _ 1 ( p , q ) instead of GºGG satisfies ƒ(p,q)subscript{ Q}_{ _ ƒ ( p , q ) . Observe that GºGG satisfies 1(p,q)subscript1‹{ Q}_{1 _ 1 ( p , q ) if whenever NNN is a soluble normal subgroup of GºGG and P/NƒP/NP / N is a normal non-abelian PƒPP-subgroup of type (p,q)(p,q)( p , q ) of G/NºG/NG / N, where p,q²superscript‹²p,q , q ^ ² , every subgroup of P/NƒP/NP / N is modular in G/NºG/NG / N. Therefore we get from Theorem E the following result. Corollary 4.1. A group GºGG is a Q1T1‹Q1 TQ 1 T-group if and only if GºGG has a perfect normal subgroup D·DD such that: (i) G/Dº·G/DG / D is a soluble Q1T1‹Q1 TQ 1 T-group, (ii) if D 1·1D 1D 1, GºGG has a Robinson complex (D,Z(D);U1,¦,Uk)··subscript1¦subscript(D,Z(D);U_{1}, D , Z ( D ) ; U _ 1 , ¦ , U _ k ) and (iii) for any set {i1,¦,ir}Š†{1,¦,k}subscript1¦subscript1¦ i _ 1 , ¦ , i _ r } Š† { 1 , ¦ , k }, where 1¤r<k11 r<k1 ¤ r < k, the groups GºGG and G/Ui1²‹¯Uir²ºsuperscriptsubscriptsubscript1²‹¯superscriptsubscriptsubscript²G/U_{i_{1}}^{ U_{i_{r}}^{ / U _ i _ 1 ^ ² ‹¯ U _ i _ r ^ ² satisfy psubscript{ N}_{p}N _ p for all p{2,3}©(Z(D))23‹·p { 2 , 3 } © ( Z ( D ) ), psubscript{ P}_{p}P _ p for all p(D)‹·p ( D ), and 1(p,q){ Q}_{1 _ 1 ( p , q ) for all pairs {p,q}Š†²superscript‹² p , q } Š† ^ ² with {p,q}©(D) …‹· p , q } © ( D ) …. 2. Now Consider the special case of Theorem E where ƒ=ƒ={,²}superscript‹‹superscript‹² = ƒ ^ = { , ^ ² } (see Example 1.2(iv)). In this case we say that GºGG is a Q,²T‹superscript‹²Q , ^ ² T-group if ,²‹superscript‹² , ^ ² -quasinormality is a transitive relation on GºGG, and we also say in this case that GºGG satisfies ,²(p,q)subscript‹superscript‹²{ Q}_{ _ , ^ ² ( p , q ) instead of GºGG satisfies ƒ(p,q)subscript{ Q}_{ _ ƒ ( p , q ) . Observe that GºGG satisfies ,²(p,q)subscript‹superscript‹²{ Q}_{ _ , ^ ² ( p , q ) if whenever NNN is a soluble normal subgroup of GºGG and P/NƒP/NP / N is a normal non-abelian PƒPP-subgroup of type (p,q)(p,q)( p , q ) of G/NºG/NG / N, where p,q0{,²}subscript‹0‹superscript‹²p,q , q _ 0 { , ^ ² }, every subgroup of P/NƒP/NP / N is modular in G/NºG/NG / N. Therefore we get from Theorem E the following result. Corollary 4.2. A group GºGG is a Q,²T‹superscript‹²Q , ^ ² T-group if and only if GºGG has a perfect normal subgroup D·DD such that: (i) G/Dº·G/DG / D is a soluble Q,²T‹superscript‹²Q , ^ ² T-group, (ii) if D 1·1D 1D 1, GºGG has a Robinson complex (D,Z(D);U1,¦,Uk)··subscript1¦subscript(D,Z(D);U_{1}, D , Z ( D ) ; U _ 1 , ¦ , U _ k ) and (iii) for any set {i1,¦,ir}Š†{1,¦,k}subscript1¦subscript1¦ i _ 1 , ¦ , i _ r } Š† { 1 , ¦ , k }, where 1¤r<k11 r<k1 ¤ r < k, the groups GºGG and G/Ui1²‹¯Uir²ºsuperscriptsubscriptsubscript1²‹¯superscriptsubscriptsubscript²G/U_{i_{1}}^{ U_{i_{r}}^{ / U _ i _ 1 ^ ² ‹¯ U _ i _ r ^ ² satisfy psubscript{ N}_{p}N _ p for all p{2,3}©(Z(D))23‹·p { 2 , 3 } © ( Z ( D ) ), psubscript{ P}_{p}P _ p for all p(D)‹·p ( D ), and ,²(p,q){ Q}_{ _ , ^ ² ( p , q ) for all pairs {p,q}©(D) …‹· p , q } © ( D ) …. 3. In the case when ƒ=ƒ1={{2},{3},{5}¦}superscript1235¦ = ƒ ^ 1 = { { 2 } , { 3 } , { 5 } ¦ } (see Example 1.2(ii)) we get from Theorem E the following clarification of Theorem D. Corollary 4.3. GºGG is a PTƒPTP T-group if and only if GºGG has a normal perfect subgroup D·DD such that: (i) G/Dº·G/DG / D is a soluble PTƒPTP T-group, and (i) if D 1·1D 1D 1, GºGG has a Robinson complex (D,Z(D);U1,¦,Uk)··subscript1¦subscript(D,Z(D);U_{1}, D , Z ( D ) ; U _ 1 , ¦ , U _ k ) and (iii) for any set {i1,¦,ir}Š†{1,¦,k}subscript1¦subscript1¦ i _ 1 , ¦ , i _ r } Š† { 1 , ¦ , k }, where 1¤r<k11 r<k1 ¤ r < k, GºGG and G/Ui1²‹¯Uir²ºsuperscriptsubscriptsubscript1²‹¯superscriptsubscriptsubscript²G/U_{i_{1}}^{ U_{i_{r}}^{ / U _ i _ 1 ^ ² ‹¯ U _ i _ r ^ ² satisfy psubscript{ N}_{p}N _ p for all p{2,3}©(Z(D))23‹·p { 2 , 3 } © ( Z ( D ) ) and psubscript{ P}_{p}P _ p for all p(D)‹·p ( D ). 4. In the paper [21], the following special case of Theorem F was proved. Corollary 4.4. A group GºGG is an MTMTM T-group if and only if GºGG has a perfect normal subgroup D·DD such that: (i) G/Dº·G/DG / D is an MMM-group, (ii) if D 1·1D 1D 1, GºGG has a Robinson complex (D,Z(D);U1,¦,Uk)··subscript1¦subscript(D,Z(D);U_{1}, D , Z ( D ) ; U _ 1 , ¦ , U _ k ) and (iii) for any set {i1,¦,ir}Š†{1,¦,k}subscript1¦subscript1¦ i _ 1 , ¦ , i _ r } Š† { 1 , ¦ , k }, where 1¤r<k11 r<k1 ¤ r < k, GºGG and G/Ui1²‹¯Uir²ºsuperscriptsubscriptsubscript1²‹¯superscriptsubscriptsubscript²G/U_{i_{1}}^{ U_{i_{r}}^{ / U _ i _ 1 ^ ² ‹¯ U _ i _ r ^ ² satisfy psubscript{ N}_{p}N _ p for all p(Z(D))‹·p ( Z ( D ) ), psubscript{ P}_{p}P _ p for all p(D)‹·p ( D ), and p,qsubscript{ M}_{p,q}M _ p , q for all pairs {p,q}©(D) ….‹· p , q } © ( D ) … . Remark 4.5. Theorem F not only strengthens Corollary 4.4 but also gives a new proof of it.",
        "keywords": ""
    },
    {
        "id": 10,
        "title": "VR Isle Academy: A VR Digital Twin Approach for Robotic Surgical Skill Development",
        "abstract": "Abstract.Contemporary progress in the field of robotics, marked by improved efficiency and stability, has paved the way for the global adoption of surgical robotic systems (SRS). While these systems enhance surgeons™ skills by offering a more accurate and less invasive approach to operations, they come at a considerable cost. Moreover, SRS components often involve heavy machinery, making the training process challenging due to limited access to such equipment. In this paper we introduce a cost-effective way to facilitate training for a simulator of a SRS via a portable, device-agnostic, ultra realistic simulation with hand tracking and feet tracking support. Error assessment is accessible in both real-time and offline, which enables the monitoring and tracking of users™ performance. The VR application has been objectively evaluated by several untrained testers showcasing significant reduction in error metrics as the number of training sessions increases. This indicates that the proposed VR application denoted asVR Isle Academyoperates efficiently, improving the robot - controlling skills of the testers in an intuitive and immersive way towards reducing the learning curve at minimal cost.",
        "corpus": "Contemporary progress in the field of robotics, marked by improved efficiency and stability, has paved the way for the global adoption of surgical robotic systems (SRS). While these systems enhance surgeons™ skills by offering a more accurate and less invasive approach to operations, they come at a considerable cost. Moreover, SRS components often involve heavy machinery, making the training process challenging due to limited access to such equipment. In this paper we introduce a cost-effective way to facilitate training for a simulator of a SRS via a portable, device-agnostic, ultra realistic simulation with hand tracking and feet tracking support. Error assessment is accessible in both real-time and offline, which enables the monitoring and tracking of users™ performance. The VR application has been objectively evaluated by several untrained testers showcasing significant reduction in error metrics as the number of training sessions increases. This indicates that the proposed VR application denoted as VR Isle Academy operates efficiently, improving the robot - controlling skills of the testers in an intuitive and immersive way towards reducing the learning curve at minimal cost. In recent years, the trajectory of medical training has significantly shifted, incorporating the latest technological advancements. However, this transition is not extensively adopted in universities or other institutions that focus on medical training, primarily due to the limited availability of market products and the high cost of the required training equipment. To this day, due to the aforementioned reasons, the majority of surgical training courses still adhere to a pattern of the 16th-century training procedure (Kostylev, 2000) where the trainees simply observe the expert-surgeon/tutor perform surgery. Contemporary advancements in the field of robotics have established robotic surgical systems as a viable option for performing highly precise minimally invasive operations enabling the surgeon to operate while seated. Some of the surgical robotic systems that are out in the market are the da Vinci surgical system (https://www.intuitive.com/en-us/products-and-services/da-vinci), Senhance surgical system (https://www.asensus.com/) and Flex robotic system (https://novusarge.com/en/medical-products/flex-robotic-system/). The da Vinci Surgical System (Intuitive, 2024) has is one of the most widely used robotic surgical systems (Peters et al., 2018). This system has been used for many different operations such as cardiac, colorectal, general, gynecologic, head and neck, thoracic, and urologic surgeries (Peters et al., 2018). In 2021, 6500 da Vinci Surgical system were installed in 67 different countries and 55.000 doctors were trained to use it (int, 2021; Xue and Liu, 2022). The cost of acquiring and maintaining the above Surgical System is significant. Due to cost considerations of acquiring it and the low amount of systems around the world, various companies have capitalized on private training courses tailored for doctors and surgeons. The field of Virtual Reality (VR) has undergone major advancements with powerful VR headsets being able to render entire worlds in real-time. This has introduced a new market for VR, in medical training. VR training offers an immersive experience for the trainees who enhance their hard-skills inside the virtual world and gain experience by training their hard-skills. Researchers across the globe have directed their efforts towards enhancing the scientific domain of VR medical training by introducing innovative solutions to address existing challenges, such as those highlighted in (Liao et al., 2022), (Chiang et al., 2013). Recognizing the necessity for a more convenient, affordable, and portable approach to utilize SRS, we suggest an advanced VR Ultra Realistic training simulation for surgical robotic systems. Figure 1a illustrates the user utilizing the machine that controls the robotic arms, with figure 1b depicting the view from the simulated training scenario. Figures 1c and 1d respectively demonstrate a user being trained in the same scenario using our application along with his view within VR. This VR simulation democratizes the training of these systems with a device-agnostic strategy by reducing the cost of training and smoothing out the learning curve. The incorporation of feet tracking enhances user immersion, providing an authentic training experience for a surgical robotic system. The main contribution of this work is to present a complete digital-twin of the SRS training process. To the best of our knowledge, VR Isle Academy is the first approach that provides the full training experience entirely in VR. In this paper, we selected da Vinci as the reference point due to its renowned reputation within the global community of surgeons. However, the work accomplished can be adapted to replicate any SRS system and not just the mechanics and training scenarios of the reference SRS. By leveraging available tools, we™ve developed a VR simulation enabling trainees to undergo SRS training conveniently, irrespective of location or time constraints. This addresses the challenge posed by the limited availability of SRS training devices in certain geographical areas, thereby saving both time and expenses associated with traditional training methods. A Digital Twin is a virtual representation, mirroring a physical object or process in the digital realm with a high-fidelity resemblance. The term was publicly introduced by Michael Grieves for a product lifecycle management (Batty, 2018). In the modern age, digital twins are extensively utilized across various sectors including power generation equipment, structures, manufacturing operations, automotive industry, healthcare services, and urban planning (IBM, 2024). Specifically, in domains such as SRS training, digital twins offer opportunities to simulate either real-life procedures, like laparoscopic surgery using the SRS, or typical training scenarios utilized for doctor training in SRS procedures. Within the framework of our project, we™ve developed a digital twin of the SRS training simulator, which can simulate real surgical operations when necessary. Numerous efforts have been made to expedite the training and education process in the medical field using VR. Recently, the cost of acquiring and maintaining commercial VR head-mounted displays (HMDs) has decreased. Furthermore, the contemporary advancements in HMD technology significantly enhance the overall performance of standalone applications. To this end, VR technology has been widely adopted for facilitating medical training not only for students but also for health care professionals. Several research papers and examples have demonstrated that VR training in the medical field reduces malpractices, training time, and the learning curve (Cevallos et al., 2022; Blumstein et al., 2020; McKinney et al., 2022; Kenanidis et al., 2023). To facilitate the development of medical training scenarios, several software development kits (SDK) have been released. MAGES SDK (Zikas et al., 2023) is an innovative SDK that empowers developers with numerous tools to efficiently create fast and effective medical training scenarios. Paul Zikas et al. (Zikas et al., 2023) highlight the latest advancements in the aforementioned SDK, including 5G edge-cloud remote rendering, a physics dissection layer, real-time simulation of organic tissues as soft-bodies within 10 ms, a highly realistic cutting and tearing algorithm, neural network assessment for user profiling, and a VR recorder for recording, replaying, and debriefing training simulations from any perspective. Fundamental Core (fun, 2017) is an all-in-one SDK for Unity game engine. The developer has the capability to establish a scoring system for real-time results at the end of a playthrough. Additionally, they provide a ready-to-use multiplayer service enabling users to connect and train together, complete with voice communication. Lastly, the SDK is device-agnostic and compatible with various VR headsets. The da Vinci machine, a Surgical Robotic System developed by Intuitive (https://intuitive.com), stands as the most widely utilized SRS globally. This surgical system provides the surgeon with an advanced set of instruments for conducting robotic-assisted minimally invasive surgery. It consists of a surgeon™s console, the four robotic arms that are scissors, scalpel, 3D cameras and forceps that are connected and moved from the surgeon™s console and the vision cart which makes the connection between the surgeon™s console and the robotic arms. Moreover, the surgeon is provided with a superior vision, through the 3D real-time high-definition view with a magnifier that can reach up to 10 times more than the human eye can see. Moreover, the ergonomic design of the surgeon™s console allows the surgeon to operate while seated for extended periods, ensuring high efficiency in incisions. The design also provides the surgeon with the capability to utilize hand controllers and foot pedals for the machine™s various functionalities. Lastly, the machine offers various functionalities triggered by pedals, masters, or the touch screen. The Camera Pedal allows adjustment of the position and orientation of the camera attached to a robotic arm. The Clutch Pedal is used to extend or shorten the robotic arm. Four energy pedals control the electro-surgical instruments. The 30-degree view pedal toggles between different camera views. Lastly, the red circle indicates the output of the cameras. The current bibliography includes numerous VR training simulations incorporating advanced cognitive and psychomotor techniques aimed at maximizing educational advantages for trainees, exemplified by references such as (Zikas et al., 2022) and (Kenanidis et al., 2023). However, despite this proliferation, simulations tailored for training in SRS within XR environments remain relatively scarce. Sketchy Neurons (https://sketchyneurons.com/) have created a VR game called Minimally Invasive (https://store.steampowered.com/app/2331420/Minimally_Invasive/) that uses a surgical robotic system. The concept of the game is that the user is a transplant surgeon and will have to burn, slice and operate aliens. They claim to have realistic physics and that the tools that are used are developed by actual surgeons. In the VR game, you can train on how to use and operate the robotic arms. They have created a menu where you can select to train and learn how to move and operate the robotic arms. The user will encounter six scenarios designed to teach functionalities similar to those of a clutch and camera. While they have embraced a device agnostic approach by incorporating SteamVR (https://store.steampowered.com/app/250820/SteamVR/), it™s worth noting that the game requires tethering, which significantly reduces portability. Moreover, the application lacks realism by not imposing hand restrictions on the user, as opposed to a real SRS. Lastly, the physics appears to be implemented in a manner that lacks realism. Surgical Robot Simulator (https://store.steampowered.com/app/1727070/Surgical_Robot_Simulator/) is another VR serious game that incorporates the fundamentals of an SRS. The game provides tutorials on controller usage and offers a range of scenarios to engage with. A notable feature of this game is the utilization of deformation algorithms, allowing users to cut and manipulate deformable meshes. Despite its realistic graphics and the mesh deformation algorithm, the control scheme for manipulating forceps and robotic arms is neither optimal nor intuitive. Users can only control the movement and rotation of the robotic arms, while the rotation of the forceps is adjusted via the thumbstick. This configuration makes it challenging to intuitively and seamlessly manipulate the medical tool of the robot. Xiaoyu Cai et al. (Cai et al., 2023) proposed a robotic minimally invasive surgical simulator based on VR digital. In their research they used Pimax (https://pimax.com/) for the VR headset and two 3DSystems (https://www.3dsystems.com/haptics-devices/touch) Touch devices and two UR5 robots (https://www.universal-robots.com/). While they have successfully linked the virtual and physical realms, there are still certain elements they are missing. Primarily, the application lacks portability, as it necessitates the presence of the robotic arm, touch devices, and the large machine designed to simulate the SRS. Furthermore, they do not incorporate any pedals, whether virtual or physical, to activate essential functions such as the clutch and camera. Finally, their solution is not cost-effective, mainly due to the necessary equipment required for the system to function effectively. Marco Ferro et al. (Ferro et al., 2019) proposed a portable da Vinci simulator in VR using cheap haptic interfaces and an Oculus Rift (https://www.oculus.com) to replicate the master console of the da Vinci. Despite its affordability, the system lacks portability due to the use of two styluses and a tethered connection to a desktop PC. Additionally, immersion is constrained, with users confined to a training scene, while clutch functionality is implemented through stylus™ buttons. Unity (https://unity.com/) is a cross-platform game engine that can be used to create two-dimensional and three-dimensional games. The engine offers a primary scripting API in C#. We used a variety of external plugins in order to create this digital twin environment. The core plugin we used in order to create each training scenario is MAGES SDK, a robust tool enabling the creation of immersive XR simulations. From this kit, we used the analytics engine in order to capture the events needed to provide the user with realtime and offline feedback, and scores depending on their performance. Also, the VR editor facilitates the quick and straightforward development of scenarios. Using scenegraph, a virtual editor that lets you create action nodes and modify existing ones in order to form a training scenario, we were able to create some of the basic scenarios of a SRS. More about them in the section 3.3. Action nodes correspond to a certain task that has to be completed in VR. The developer can either use one of the predefined action types, such as insert action, remove action, use action, or create his own action type. Although our implementation is Unity-based, our approach can be leveraged and implemented into any modern game engine. The surgical robotic system employed in the VR environment consists of two parts, which are commonly coined as master and slave (Low and Phee, 2004). The master platform is controlled by the user with two-joystick-like controls (Fig. 3) and two pedals that can be utilized for changing functionalities when necessary. On the other hand, the slave is located in a different area within the scene and consists of two 6-Degrees of Freedom (DoFs) robotic arms. Each robotic arm is equipped with a 1-DoF two-jaw gripper end effector (the tool mounted at the end of the robotic arm) featuring multiple box collider (Fig. 3), enhancing the realism and physics accuracy of haptic interactions with objects of various shapes within the scene. The master platform was acquired from (ope, 2023) and it was modified appropriately to improve rendering speed and the overall efficiency of the model. The robotic arm was designed entirely by our team while drawing inspiration from the specifications of the da Vinci surgical robot (https://www.intuitive.com/en-us/products-and-services/da-vinci), the most widely utilized robotic system for minimally invasive surgeries (Xue and Liu, 2022). Moreover, we aimed to replicate the surgeon™s console, allowing users to customize the head height of the machine and adjust the position of the pedals to optimize ergonomic posture. Users can also re-calibrate the trackers to easily configure their height. These operations can be performed conveniently through the virtual tablet on the console. In our application, we simulated eleven scenarios basic resembling those found in a modern SRS. These scenarios are designed to familiarize the user with the control of the robotic arms, the clutch pedal, the camera pedal, and the 30-degree camera. The purpose of these activities is to provide the user with an interactive introduction to the surgical system™s features. More precisely, some exercises try to mix various functionalities in a single scenario, while other lessons concentrate on a single one, such as the camera function in the Camera 0 scenario. For example, in the Sea Spikes exercises the user must learn how to manage delicate wrist movements while combining the camera and clutch functionalities effectively. Through a User-Interface (UI) menu (Fig. 7), the users can choose to train in any one of the 12 scenarios. Instructions (Fig. 4) detailing the objective of the exercise and the user™s responsibilities are provided for each scenario. For instance, in Wrist Articulation 1, users are instructed to touch the ball inside the glass cube, without breaking the exterior glass. The implementation of the exercises was carried out using the scenegraph framework from the MAGES SDK. In this framework, exercises can be seen as Actions that users have to perform. Exercises that require repetition, such as Wrist Articulation 1 or Camera 0, are implemented as one action that is repeated X‹XX times, where X‹XX is the amount of total iterations the scenario requires. To exemplify, in Wrist Articulation 1, the users have to perform two actions (in this case, place the instruments on a specific position and touch the glowing ball) ten times, but on a different angle. Lastly, more scenarios can be seamlessly added to the application in order to enhance the variety of the training options. In this section, we elucidate the scoring system we include in our application, used for the qualitative assessment of the users™ performance. Through error detection and analytic metrics for each exercise, the users can monitor their progress and improve their skills. Each exercise contains a list of efficiency and penalty scores used to assess the overall score of the session. The scoring factors and analytics metrics of VR Isle Academy were designed and implemented based on the corresponding factors of a modern SRS, with a focus on retaining the different weight and importance of each metric to the final score of each exercise. The main focus of VR Isle Academy regarding the scoring system is to retain the importance of each metric when providing user performance feedback. In order to extract information regarding the scoring factors and their importance, an iterative procedure was followed, where we carefully examined the scores and their breakdown in the actual simulation for each and every exercise. Each metric score is calculated using a weight factor. This weight factor varies from metric to metric. For instance, the drops metric which represents the times an object held by the instruments fell on the environment, has a bigger weight factor than the excessive force metric, which measures the times excessive force was applied by the instruments to an object, while the tower detach metric fails the exercise immediately. The total score of an action is formed as such: As can be observed, the analytic metrics are split into two main categories: Efficiency Metrics and Penalty Scores. The first category includes two metrics; total time, representing the total time needed to complete the exercise, and economy of motion, which is the travel distance of the instruments during the exercise. Both metrics have an initial score of fifty (50) points. When the duration for completing the scenario exceeds a predetermined threshold or the user makes unnecessary movements (thus increasing the total distance), points are deducted. For the implementation of the scoring system, we used the analytics framework from the MAGES SDK. We mainly utilized the custom scoring factors that monitor data from objects. For instance, in order to compute the economy of motions metric, we summarize the total changes of both position and rotation for each bone in our Inverse Kinematic (IK) chain. More details about the IK solver will be discussed in Section 4.1. Upon exercise completion a detailed breakdown of the score is presented to the user that also includes the penalty deducted points through a User Interface (see Fig. 8-Top). Furthermore, MAGES automatically uploads analytics data for each exercise to a web browser portal (see Fig. 8-Bottom) after finishing a training scenario. Users can log in using their credentials and gain access to a detailed log of each training session that showcases their analytics. The primary objective of this section is to present the control framework designed to guide the robot in accurately tracking the user™s movements in an intuitive way. The user holds the VR controllers and by moving them, he can translate and rotate the two machine controllers within the virtual reality environment. These machine controllers, which will be referred to as masters, directly control the robotic arms in the operation room. Each robot arm™s end effector (EE) should achieve the corresponding desired pose (as extracted by the masters) accurately in real-time without significant delay. To this end, the computational complexity of the code is of pivotal importance when developing such a framework. First, the 6-DoF pose (position and orientation) of each master is mapped into the desired pose for the corresponding EE (left and right). The mathematical expressions in this section will be formulated for one master and one EE since the left and right arms are considered equivalent. To prevent gimbal locks and other issues associated with representing rotations using Euler angles, rotation matrices are employed to express rotations, while transformation matrices are utilized for poses. Let »WMSE(3)superscriptsubscript»Š†3 SE(3)T _ W ^ M S E ( 3 ) be the pose of the master with respect to an inertial frame, or, world frame. SE(3)†3SE(3)S E ( 3 ) is the Special Euclidean group in three dimensions, while »WMsuperscriptsubscript»Š _ W ^ M is a 4Ã4444 44 Ã 4 homogeneous transformation matrix represents the translation and rotation from world frame (W)Š(W)( W ) to the master frame (M)(M)( M ) and it is defined as: where ¹WMSO(3)superscriptsubscript¹Š†‚3 SO(3)R _ W ^ M S O ( 3 ) is the 3Ã3333 33 Ã 3 rotation matrix which belongs to the Special Orthogonal group, 3superscript3 R ^ 3 the translation part, =[x,y,z]Tsuperscript¥¦§ = [ x , y , z ] ^ T and =[0,0,0]0000 = [ 0 , 0 , 0 ]. At each iteration, the orientation of the master is mapped to the EE™s frame to extract the desired orientation (Rd)subscript…(R_{d})( R _ d ): where, RWtMsuperscriptsubscript…Š¡R_{Wt}^{M}R _ W t ^ M is the current orientation of the master at time t¡tt, (RW0M)Tsuperscriptsuperscriptsubscript…Š0(R_{W0}^{M})^{T}( R _ W 0 ^ M ) ^ T is the transpose (or inverse) of the initial orientation of the master and, RW0EEsuperscriptsubscript…Š0R_{W0}^{EE}R _ W 0 ^ E E is the initial orientation of the EE. For computing the desired position of the EE, d:=[xd,yd,zd]assignsubscriptsubscript¥subscript¦subscript§ _ d := [ x _ d , y _ d , z _ d ], with respect to its own initial position, the following formula is utilized: where 0EEsuperscriptsubscript0 _ 0 ^ E E is the initial position of the EE, tMsuperscriptsubscript¡ _ t ^ M is the current position of the master, 0Msuperscriptsubscript0 _ 0 ^ M is the initial position of the master and finally Î±¼ which denotes the translation sensitivity multiplier. Moreover, Î±¼ is a hyperparameter that specifies the amount of change in the position of the EE for a certain displacement of the master. Given that this implementation is specifically tailored for surgical systems, enhancing the overall accuracy requires reducing the movement of the EE to considerably less than that of the master™s displacement (Î±ª1)much-less-than¼1 1 Î± ª 1 ). At every time step, after extracting the desired pose of the EE using equations (2) and (3), the corresponding control inputs (u6superscript6u R ^ 6 ) that achieve this pose are calculated using Inverse Kinematics (IK). To this end, we have developed a numerical-approximation IK solver that is utilized to determine the control inputs for each fully-actuated manipulator as follows. Consider the forward kinematics equation ™=(½)™½ = f ( Î ), where :6†6:†superscript6superscript6 : R ^ 6 † R ^ 6 is the forward kinematics function, ™6™superscript6 R ^ 6 is the pose of the EE and ½6½superscript6 R ^ 6 are the joint angles of the robot. The three wrist joint angles are depicted in Fig. 9. By deploying the Newton-Raphson method to solve the equation (½d)=™d(½d)=0subscript½subscript™subscript½0 ( Î _ d ) = x _ d - f ( Î _ d ) = 0, we extract the desired joint angles ½dsubscript½ _ d that will result the desired pose for the EE ™dsubscript™ _ d by using the 1stsuperscript1 ¡1^{st}1 ^ s t order Taylor expansion (Lynch and Park, 2017): where, ½tsubscript½¡ _ t is the current state of the robot, ™dsubscript™ _ d and ™tsubscript™¡ _ t are the desired and current pose of the EE respectively. ±1(½d)superscript±1subscript½ ^ - 1 ( Î _ d ) is the inverse Jacobian matrix that maps changes from the task space into joint space. Ultimately, after computing ½dsubscript½ _ d , we apply it to Unity™s articulation drive component (https://docs.unity3d.com/Manual/class-ArticulationBody.html), which utilizes the information to extract the necessary forces for a smooth transition from the current to the desired joint angles. All SRS machinery is typically controlled via hand movements of the user through a controller-like system, as depicted in Figure 2. Similarly, this approach can be adopted in the corresponding VR digital-twin, utilizing the HMD controller. However in order to increase the system™s portability whilst enhancing the intuitiveness of controlling the robotic arms (Huang et al., 2021), we opt to explore replicating controller movements through hand tracking technology. This technology could enhance the training experience, particularly because the pitcher-like movement of the actual SRS cannot be replicated with a commercially standard VR controller. Therefore, hand gestures could be readily identified using hand tracking. In VR Isle Academy, we explored hand tracking with Wave SDK on the HTC Vive XR Elite headset and the XR Hands (https://docs.unity3d.com/Packages/com.unity.xr.hands@1.1/manual/index.html) with the OpenXR (https://www.khronos.org/openxr/) loader on the Meta Quest 2/3 headsets. We aimed to replicate users™ interactions and hand poses on the surgeon™s console of an SRS, which led to the integration of hand tracking as can be observed in Fig. 10. Notably, the Wave SDK doesn™t offer a straightforward method for pinch-based grabbing, in contrast to Unity™s XR Hands, which include a built-in interaction mechanism. Notably, hand tracking, by relying solely on onboard VR sensors, has limitations. For instance, the user™s hands might exceed the Field of View (FOV) of the VR cameras which results in loss of tracking. Another significant issue is self-occlusion in instances that the user engages in intricate hand poses. This complication led to the VR system being unable to accurately recognize when the user was pinching, consequently preventing the forceps from closing in the digital twin. While we successfully addressed the FOV problem by incorporating two HTC Vive Wrist Trackers on the user™s wrists, the persistent self occlusion issue prompted. The hand tracking feature reserved solely for specific scenarios where pinch gesture is not required (such as Wrist Articulation). As a result, the pinch-like gesture that an SRS user would perform is instead performed via trigger buttons by the VR trainee. Our primary objective was to implement a foot tracking solution to interact with physical pedals within the VR, which in turn would trigger specific functionalities of the surgical robot. Initially, HTC Vive Trackers 3.0 were considered, but their reliance on external lighthouse cameras made them less suitable for our standalone application and their setup was not as straightforward as desired. Consequently, we opted for the HTC Vive Ultimate Trackers (https://www.vive.com/us/accessory/vive-ultimate-tracker/), primarily due to their compatibility with standalone VR system such as the HTC Vive XR Elite, and particularly for Android applications. HTC Vive Ultimate Trackers are an enhancement of the standard model. They are distinguished by the inclusion of two 3D cameras, which significantly refine the mapping of the user™s surroundings. This addition allows for a more nuanced interaction within virtual environments. The Wave Tracker Manager from the Essence package is utilized in our application. It contains class references for each tracker, including the tracker ID. The manager provides two key functionalities: activating the initial start tracker, triggering the tracker interface for direct communication when the API starts, and enabling the use of XR Device to retrieve tracker data from UnityEngine. XR. InputDevice (https://docs.unity3d.com/ScriptReference/XR.InputDevice.html). This class is part of Unity™s XR input subsystem, managing input devices in XR applications, defining an XR input device, and handling input features and haptic feedback. The script checks for instances where trackers may become stuck, specifically when they have a valid rotation but no positional data, issuing warnings to the user. It also verifies tracker connections based on their specific ID within the application. Once communication is established between the trackers and the headset, we implement the rotation and position data from the trackers onto the feet of the Full-Body avatar, ensuring an ongoing synchronization with the user™s movements. Mini-map in video games is a small heads-up display (HUD) map which is usually placed at the corners of the screen to help the player navigate inside the virtual world. Usually, the mini-map contains topographical information regarding key objectives and world features. The utilization of feet trackers enables the user to literally press the pedals inside the virtual world. However, such action requires some form of feedback that will notify the user when the pedal has been pressed successfully. However, a physical form of feedback (vibration) is unavailable in HTC Vive Ultimate Trackers. To this end, we propose a visual feedback scheme by utilizing the minimap. We created a User-Interface (UI) that represents the placement of the pedals and two UI elements, one for each foot (Fig. 11). The mini-map is placed on the left side of the machine™s screen (Fig. 12). For the pedals, the user can see the Clutch, Camera, Switch and all the energy types. These pedals are static, they cannot change position or orientation. From the other half, the user™s feet UIs change position and scale. when the user moves his feet, the UI will change position. Then, by pressing one of the buttons, the UI icon of the particular pedal will turn black and an audible click sound will be triggered in order to inform the user that he has pressed a pedal. Moreover, our approach solves another issue that corresponds to the height of the user™s feet. If the user raises his leg, the feet UI will scale-up, signifying the foot displacement with the respect to the ground. To assess the effectiveness of our application and the impact on facilitating the medical training procedure, we conducted experiments involving 4 testers. The testers are medical students that do not possess any prior knowledge on operating an SRS. Each tester played 4 scenarios, repeating them 3 times. The order of scenarios was Wrist Articulation 1, Clutch, Camera 0, Sea Spikes 1, and Roller Coaster 1. Wrist Articulation 1 is the easiest scenario, whereas Roller Coaster 1 is the most challenging. The first three scenarios aim to familiarize the user with the core functionalities of the VR Isle Academy, namely, the control of the robotic arms, the clutch pedal and the camera movement. The remaining two scenarios involve a combination of robotic arms and camera movements, requiring users to execute precise actions. In Fig. 13, it is noticeable that the users achieved a higher score as they progress the training scenarios. This scenario is relatively easy, involving the task of learning how to manoeuvre the forceps correctly with minimal wrist movements. In Fig. 14, a distinction is evident between play 1 and play 2. This is attributed to the scenario™s combination of correct wrist movement and pedal functionalities, making it challenging to grasp in a single attempt. The testers found it easy and intuitive to operate the surgeon™s console for various training scenarios. The more they played, the easier it became for them to use. The most challenging concept to grasp was the feet trackers. Initially, users reported difficulty localizing their feet and whether they were pressing a pedal. This might stem from the fact that HTC Vive Ultimate trackers do not provide haptic feedback; hence, the mini-map and sound served as the only source of feedback. Upon familiarization, the users improved at controlling the trackers, getting accustomed to the mini-map. Even though the testers we selected are aspiring doctors, their background may not be fully aligned with the performance they achieved in the training scenarios. The ability to control a robotic arm should be intuitive and easy regardless of the user™s background. Lastly, users who have actually tried the VR Isle Academy prior to using the real machine have reported an intuitive transition between the two. The required training time with an instructor was significantly reduced and their score was higher than the average. VR Isle Academy is a cost-effective solution that enables unsupervised training on operating an advanced SRS. Whether utilizing trackers, VR controllers, or Hand Tracking, the user enjoys the freedom to train in various settings. Thus, we minimized the need of using an explicit, bulky device. Also, the availability of the application is 24/7. Also, there™s no need to schedule a slot or incur additional costs for a teaching service; the user can independently learn how to operate a Surgical Robotic System (SRS). Moreover, the advancements in VR tracking technology, especially with the introduction of HTC Vive Trackers and HTC Vive Ultimate Trackers, have substantially augmented the depth and breadth of virtual reality applications. These developments have not only heightened the immersive quality of VR but also expanded its practical applications across diverse fields. The ongoing evolution of VR technology promises further enhancements in tracking precision and user engagement. Even as users strive to familiarize themselves with tracker usage, the inclusion of a mini-map depicting the position and orientation of the feet has proven highly beneficial. This approach effectively addresses the feedback-related challenges by providing visual and auditory cues, enabling users to orient themselves correctly and press the intended pedal. The accurate error detection and comprehensive analytics are pivotal in such simulations. Surgeons undergoing training in this VR digital twin will benefit significantly from robust error detection mechanisms and detailed analytics. The user can check his analytics in real-time, while using the VR headset, or offline, by accessing a portal page. Although we explored the Hand Tracking approach, we found it less suitable. The ergonomic design of the machine often led to hands going beyond the FOV of the VR cameras. Additionally, self-occlusion occurred when adopting unnatural hand poses. This presented challenges as the application couldn™t accurately detect when the user was tapping or closing their fingers, impacting the simulation™s fidelity. While we addressed the FOV issue by incorporating wrist trackers from HTC Vive, the problem of self-occlusion persisted. Although readily available tools were utilized for developing VR Isle Academy, the novelty lies in their effective integration. The combination of different features and techniques provides the users with a unique, immersive educational experience and value. Development will continue, introducing an additional twelve scenarios aimed at training users in utilizing the energy pedals, the switch pedal, and a pedal facilitating the transition between two robotic arms. Additionally, there will be diversification in the types of forceps, incorporating both mono-polar and bi-polar options, enabling the utilization of different energy sources. Currently, the sole method of employing the trackers independently is through the Wave SDK. However, there are plans to ensure compatibility of the Vive Ultimate Trackers with OpenXR. Preparations have been made, having already executed a port of the VR application to OpenXR for other HMDs like Meta Quest 2 and Meta Quest 3. Finally, controlled clinical trials involving surgeons will be conducted to assess the fidelity and resemblance between the real and the digital twin SRS. Two groups will be formed. Both groups will use the real SRS while only one of them will have been trained using VR Isle Academy. Subsequently, we will compare the efficiency and time taken by each user, comparing the usage of our application and an SRS versus using only an SRS.",
        "keywords": "Digital Twin , Medical Training , Virtual Reality\n, Inverse Kinematics , Surgical Robotic System"
    },
    {
        "id": 11,
        "title": "Navigating the Future of Federated Recommendation Systems with Foundation Models",
        "abstract": "AbstractIn recent years, the integration of federated learning (FL) and recommendation systems (RSs), known as Federated Recommendation Systems (FRSs), has attracted attention for preserving user privacy by keeping private data on client devices. However, FRS faces inherent limitations such as data heterogeneity and scarcity, due to the privacy requirements of FL and the typical data sparsity issues of RSs. Foundation models (FMs), such as diffusion models (DMs) and large language models (LLMs), which are widely recognized for producing high-quality content in the image and NLP domains, focus on understanding and mimicking the underlying distribution of the training data. Unlike discriminative models, which learn the boundaries between categories, FMs aim to learn the entire probability distribution of the input data. Thus, the achievements of FMs inspire the design of FRS and suggest a promising research direction: integrating foundation models to address the above limitations. In this study, we conduct a comprehensive review of FRSs with FMs. Specifically, we: 1) summarise the common approaches of current FRSs and FMs; 2) review the challenges posed by FRSs and FMs; 3) discuss potential future research directions; and 4) introduce some common benchmarks and evaluation metrics in the FRS field. We hope that this survey provides the necessary background and guidance to explore this interesting and emerging topic.",
        "corpus": "In recent years, the integration of federated learning (FL) and recommendation systems (RSs), known as Federated Recommendation Systems (FRSs), has attracted attention for preserving user privacy by keeping private data on client devices. However, FRS faces inherent limitations such as data heterogeneity and scarcity, due to the privacy requirements of FL and the typical data sparsity issues of RSs. Foundation models (FMs), such as diffusion models (DMs) and large language models (LLMs), which are widely recognized for producing high-quality content in the image and NLP domains, focus on understanding and mimicking the underlying distribution of the training data. Unlike discriminative models, which learn the boundaries between categories, FMs aim to learn the entire probability distribution of the input data. Thus, the achievements of FMs inspire the design of FRS and suggest a promising research direction: integrating foundation models to address the above limitations. In this study, we conduct a comprehensive review of FRSs with FMs. Specifically, we: 1) summarise the common approaches of current FRSs and FMs; 2) review the challenges posed by FRSs and FMs; 3) discuss potential future research directions; and 4) introduce some common benchmarks and evaluation metrics in the FRS field. We hope that this survey provides the necessary background and guidance to explore this interesting and emerging topic. In the digital age, the exponential growth of information has created a need for systems that navigate, filter, and personalize data for individual users. Recommendation Systems (RS) nowadays become crucial tools for filtering online information and helping users discover products, content, and services that align with their preferences [1]. However, the systems traditionally rely heavily on centralized data collection and processing, posing significant privacy risks and operational bottlenecks. The importance of user privacy has never been greater, particularly with stringent data protection regulations such as the General Data Protection Regulation [2] in Europe, which emphasises the need to store user data on theirs local devices, instead of uploading it to central servers. As a novel approach to address these privacy concerns, Google introduced Federated Learning (FL) [3] as a framework designed to train models across decentralized devices, while keeping data localized. This paradigm shift in data processing leverages the computational capabilities of individual devices for local data analysis. Specifically, FL alternates between local model training at the user end and global parameter aggregation from these models on a central server. The integration of FL with RS becomes essential for safeguarding user privacy in recommendation services, which has given rise to the burgeoning field of Federated Recommendation Systems (FRS) [4]. In this field, typically each client consists of a single users device. Therefore, unless specified otherwise in this paper, the terms user, client, and device all refer to an individual user. FRS recently have shown promising results in many areas, such as service providing [5, 6], daily scheduling [7], driving planing [8, 9] and more, significantly impacting different facets of daily life. Similar to FL, FRS is required to keep user data local to protect user privacy. However, in most cases, each client typically contains only the data of one users accessed item, which is extremely small compared to the total number of items, creating a serious data sparsity problem. In addition, different users have different behaviours and preferences, which can lead to data heterogeneity. The presence of both issues can lead to sub-optimal models and reduced effectiveness. More recently, the emergence of a novel paradigm for the construction of artificial intelligence (AI) systems has garnered considerable interest in the wake of the remarkable success of ChatGPT [10] and Stable Diffusion [11] in the tasks of language understanding [12, 13, 14, 15] and image generation [16, 17], which we refer to as Foundation Models (FM) [18]. As shown in Fig. 1, this paradigm is built by using self-supervised optimization of training goals to determine how to update model parameters based on model predictions on the large amount of unlabelled training data. This process is referred to as pre-training. Language models, e.g. BERT [19]and RoBERT [20], are usually trained using the next token prediction goal, which refers to the extent to which the model is able to predict the next token in the sequence. One of the most successful examples of language models is ChatGPT, which is based on GPT-3.5. By training on large amounts of text data, it aligns the capabilities of large language models with human intent [21]. Vision models like ViT [22] are typically trained using either contrast learning or diffusion training targets. For contrast learning [23, 24], images are randomly augmented before evaluating the similarity of the model representations. For diffusion models [25, 26], noise is added to the images and the model is gradually de-noised by the target. There are also multi-modal training targets, some of which separate images and text during training, while others consider both. The foundation model, once trained, is used as a plug-in in combination with adaptations to achieve results for a wide variety of downstream tasks. CLIP [27] and DALL·E [28] are two multimodal AI models developed by OpenAI. CLIP focuses on understanding images through natural language, while DALL·E focuses on generating images based on text descriptions. While both models are trained on large datasets of images and text, CLIP is primarily used for image retrieval and classification, while DALL·E excels at generating new images that match text descriptions [29]. As FM is frequently trained using a substantial quantity of data from multiple sources, they are capable of incorporating a considerable amount of additional knowledge when performing a specific downstream task. This feature enables FM to provide diverse training data for task-specific models in downstream task scenarios, thus effectively alleviating the problem of data scarcity. Given these considerations, the application of FM to FRS is not only a viable approach but also holds significant research potential. Such integration will drive innovation in FRS. Although the application of FM to FRS has the potential to be highly beneficial, it must be acknowledged that this field is still in its nascent stages, with an insufficient understanding of the challenges, viable methods, and directions for development. This paper aims to bridge this knowledge gap through an in-depth analysis of the integration of FM and FRS. The article provides a comprehensive examination of the motivations and challenges associated with integrating these two paradigms, with a particular focus on several representative technologies. Additionally, it outlines future development trends and their applications. By elucidating the intersection of FRS and FM, this study aims to promote further exploration and innovation within this emerging field, thereby facilitating its rapid advancement. Paper Organization. The reminder is structured to provide a thorough examination of the current research landscape, technical challenges, application cases, and future directions of FRS. The organization is as follows: Sec. II outlines the existing surveys within the field of FRS, and distinctly highlights our novel contributions, setting our survey apart from the current literature. Sec. III introduce foundational knowledge in the field of FRS and FM, including definitions, basic architectures, and current scenario classifications. Sec. IV introduces the potential applications and functions of FM according to the three classical phases of FRS. Sec. V delves into the main technical challenges faced by FRS with FM, including data heterogeneity, model aggregation difficulties, and privacy concerns, as well as current methods and their limitations in addressing these challenges. Sec. VII showcases applications of FRS across various domains, such as e-commerce, lifestyle and healthcare, discussing the outcomes and challenges of these real-world implementations. Sec. VIII summarizes the datasets and evaluation methods commonly used in FRS with FM, explaining how to quantify the recommendation performances. Sec. VI explores future research directions in FRS with FM, including unresolved issues and emerging research areas, providing guidance and inspiration for researchers in the field. Sec. IX concludes the survey with a summary of key findings and perspectives, offering insights into the trends and potential future of FRS. In the field of FRSs, various studies have converged on methodologies, privacy preservation, and challenges, albeit with distinct focal points. Yang et al. [30] discuss the practical implementation and evaluation of federated recommendation systems, with a focus on system architecture and algorithmic efficiency. Alamgir et al. [31] provides a comprehensive overview of federated recommendation systems, highlighting techniques, prevailing challenges, and future directions. Javeed et al. [32] focus on the security and privacy concerns in personalized recommendation systems and propose targeted solutions to these challenges. Finally, Sun et al. [33] conduct a survey that compares existing federated recommendation research, highlighting the strengths and limitations of various approaches. All studies emphasize the critical importance of privacy protection and the challenges posed by data heterogeneity and model aggregation. This body of work enhances our understanding of FRSs and provides a foundation for future research to address their inherent challenges. Although a number of surveys [34, 35, 36, 37, 38, 39] focusing on FL with FM already exist, our paper is the first survey work in our current knowledge that focuses on combining FRS with FM. Contribution. This survey aims to provide a clear theoretical framework for applying FM to FRS and to elaborate on their principles and methods of application. By analyzing existing technologies, this paper strives to drive innovation in integrating the pre-training capabilities of FM into FRS. The article discusses in detail the technical challenges and practical issues faced when integrating FM, such as privacy protection, data heterogeneity, communication efficiency, and model generalization capabilities. It also identifies current research gaps and future directions, aiming to guide subsequent academic research and technological development. Additionally, through application studies, this paper demonstrates how to apply the integration of FM with FRS in practical scenarios, thereby deepening the integration of theory and practice. A Federated Recommendation System is a technology that uses distributed algorithms for personalized information filtering. It aims to improve the accuracy of information filtering and the effectiveness of personalized services, while maintaining user privacy. FRS facilitate collaborative learning by pooling analysis and learning capabilities across multiple clients (e.g. users or locations), without the need for direct exchange of raw user data. Fig. 3 illustrates that this type of system typically involves three processes: client model update, communication, and global aggregation. The client model update allows users to train their models locally on their devices using their own data and then upload the updated intermediate parameters to the server. The server performs global aggregation on the parameters sent by all participants, integrating the unique information from each of them. The aggregated parameters are then distributed to the next round of participants, initiating a new training round. The process of uploading and distributing is collectively referred to as communication. These approaches effectively preserves privacy and security. As illustrated in Fig. 5, FRS can be categorized based on different criteria [40, 41]. According to used data distribution, FRS can be /d into Horizontal FRS, Vertical FRS, and Transfer FRS. Each type addresses different data collaboration and learning scenarios. Specifically: Horizontal FRS deals with situations where the feature spaces are similar across different entities, but the sample spaces differ, as shown in Fig. 2a. For instance, two customers who shop at the same store may purchase some of the same products. The horizontal method aggregates model updates from various sources to train recommendation models without sharing raw user data. This approach enhances the accuracy and efficiency of recommendation systems while safeguarding user privacy. Horizontal FRS is currently the most common type, with many studies [42, 43, 44] based on the assumption of this data distribution. Vertical FRS is applied when the feature spaces differ but the sample spaces are similar. This scenario is common when different entities possess different types of data about the same users. An example would be a bank and an online retailer holding distinct perspectives on the same customers, i.e., the bank has credit history while the retailer has shopping history. Vertical FRS [45, 46, 47, 48] trains recommendation models by securely integrating different data features between parties, utilizing richer user information for more accurate personalized recommendations and ensuring data privacy and security. Transfer FRS utilizes principles of transfer learning [49] to transfer knowledge from one domain to another. This type is suited for scenarios with significant differences in both feature and sample spaces across parties. Transfer learning allows the system to leverage data and knowledge from the source domain to enhance recommendation performance in the target domain, even when the target domain lacks sufficient data for independent model training. Transfer FRS [50, 51] is particularly beneficial for emerging markets or user groups with limited data, drawing insights from related domains. As shown in Fig. 4, FRS can be categorized into centralized, semi-decentralized and decentralized based on their communication architecture. Each type addresses privacy and scalability in different ways: Centralized FRS operates within a FL framework where a central server orchestrates the learning process. User devices (or clients) locally compute updates based on their data and send these updates to the server. The server aggregates these updates to improve the global model, which is then distributed back to the users. While this approach improves privacy by not requiring the sharing of raw data, it still relies on a central authority to manage the model. It effectively addresses privacy concerns by allowing the model to learn from decentralized data sources without centralizing the data itself. This architecture is typically used to overcome data silo issues and improve the performance of RSs without compromising user privacy and data security. Due to its simplicity and intuitive nature, this architecture has become the dominant framework within the field of FRS. There is a great deal of work [52, 43, 44] based on it currently. Semi-decentralized FRS introduces an intermediate layer between the central server and the users, such as edge servers or devices that can perform additional computational or storage tasks. This setup aims to reduce the communication overhead and latency associated with sending updates to a central server, especially in large-scale applications. A specific example is the Semi-decentralized Federated Ego Graph Learning (SemiDFEGL) framework [53], which improves scalability and reduces communication costs by introducing new device-to-device collaborations. It augments local subgraphs with predicted interacted item nodes to exploit high-order collaborative information between users and items in a privacy-preserving manner, which is particularly useful for recommendations based on collaborative filtering and graph neural networks. Decentralized FRS, which employs peer-to-peer communication architecture [54], distributes the learning process completely to all participating devices, without the need for a central server for model aggregation. This approach maximizes privacy and data ownership, but presents challenges in coordinating model updates and ensuring model convergence. Zheng et al. [55] proposed a FRS model called DGREC by adopting a decentralized graph neural network, which constructs a local intra-item hypergraph and a global inter-user graph for each user, allowing users to freely choose whether to disclose their interactions. Li et al. [56] introduced DFedRec, which uses a privacy-aware client-level structured graph to share model parameters only with relevant neighboring users, thereby reducing communication costs and protecting user privacy. Each of these architectures offers a trade-off between privacy, communication efficiency and the degree of decentralization. Centralized systems simplify model aggregation, but rely on a central authority. Semi-decentralized systems aim to balance efficiency and privacy with an intermediate layer, while decentralized systems offer the highest level of privacy at the cost of more complex model coordination. The rapid increase in the performance of computer hardwares, e.g., GPUs, the increasing maturity of transformer architectures, and the public availability of large amounts of training data have been three key factors in the emergence of FM [18]. According to the work of Stanford HAI [18], we have the following definition of FM: A foundation model is defined as any model trained on extensive data (typically using large-scale self-supervised learning) that is capable of adapting to a wide range of downstream tasks, for instance, through fine-tuning. In recent years, the scale and scope of FM have greatly expanded our imagination of potential applications. Such models typically have billions or even trillions of parameters, allowing them to learn more complex patterns and knowledge. They can be adapted to new tasks through fine-tuning or zero-sample learning. For instance, the GPT-3 model [57], comprising 175 billion parameters, is capable of adapting and perform various tasks with the aid of natural language prompts, despite the fact that a significant proportion of these tasks have not been explicitly trained. FM is distinguished by two key characteristics: 1) Emergence, which refers to the implicit induction of system behavior from examples, as opposed to explicit design; and 2) Homogenization, which indicates that the method of constructing machine learning systems tends to be unified across a wide range of applications. Although FM is based on standard deep learning and transfer learning, their scale brings new emergent abilities, and their effectiveness in many tasks has motivated homogenization. While homogenization provides strong leverage, it also requires caution, as defects in foundation models can be inherited by all downstream adapted models. Moreover, despite the broad deployment prospects of foundation models, due to their emergent properties, our understanding of how they work, when they fail, and what they can actually do is still quite limited. As shown in Fig. 6, similar to FRS, FM can also be classified based on data type or functionality. Specifically, based on the type of data used during training, FM is mainly /d into followings [18]: Language FM: These types of FM mainly deal with textual data and are trained on large textual datasets to understand and generate language. They are very effective in natural language processing (NLP) tasks such as machine translation, text summarization, and sentiment analysis. Typical language models include BERT [19], GPT-3 [57], and T5 [58], which typically use deep transformer architectures that are able to capture complex linguistic regularities and demonstrate excellent performance on multilingual tasks. Vision FM: Vision FM focuses on processing and understanding image data. These models are able to perform tasks such as object recognition, image segmentation, and visual reasoning by learning large amounts of image data. For example, DINOv2 [59] and SAM [60] are basic models trained specifically for visual tasks, and using self-supervised learning methods, these models learn valid visual representations without labelling the data. Multimodal FM: Multimodal models can simultaneously process and understand multiple types of data, such as text and images. Such models can excel in cross-modal tasks such as image captioning and visual quizzing by integrating information from different modalities. CLIP [27] and DALL·E [28] are representative of such models that are able to understand the relationship between an image and its corresponding textual descriptions, demonstrating flexibility and robustness in handling multiple data types. Functionally, FMs are generally categorized into two types [18, 61]: Discriminative FM: The main task of discriminative FM is to distinguish or predict specific outputs from given input data. These models are generally based on the BERT family and are more concerned with learning decision boundaries from the data to perform classification, regression, or other predictive tasks. Generative FM: The core goal of generative FM is to learn the distribution of the data and be able to generate new data samples. These models, such as GPT-3, DALL·E, etc., are able to capture the underlying structure of the data and thus generate new instances that are similar but different from the training data. Generative FM has a wide range of applications in areas such as text generation and image generation. Fig. 1 illustrates a commonly used technique for pre-trained models, known as the Adapter. This technique adds new lightweight layers while maintaining the original parameters of the pre-trained model, enabling fine-tuning and extension for specific tasks. This approach is suitable for multi-task learning and tasks performed in resource-limited environments. Adapters, such as LoRA [62] and QLoRA [63], have found wide applications in natural language processing [62] and computer vision tasks. As described earlier in Fig. 3, a typical FRS typically consists of three stages: client model update, communication, and global aggregation. Integration with FM should also occur at these three stages. In FRS, the clients have the following characteristics: As previously described, the data needed for model training in FRS, such as user information and interaction history, are privacy-sensitive and are therefore required to stay on the users device, creating data silos. Moreover, each users data is minimal compared to the total dataset, and each user only accesses a small portion of the item set, leading to data sparsity. The data on the client is influenced by user preferences, such as user behavior and product preferences. The resulting data distribution often does not meet the independent and identically distributed (IID) assumption, leading to the challenge of data heterogeneity. Additionally, user devices are generally consumer products like mobile phones and personal computers, characterized by unstable communication and limited computational resources. This requires that the computational load of the models deployed on the client and the amount of information exchanged with the server be kept as low as possible. FM is typically pre-trained on large, diverse datasets, acquiring a broad range of features and knowledge. This pre-training endows them with prior knowledge that allows for rapid adaptation to specific client data through fine-tuning. Consequently, these models can learn general feature representations, providing a certain level of adaptability to different data distributions. Therefore, clients can effectively fine-tune the model with a minimal amount of local data, achieving good performance on specific downstream tasks. Additionally, by fine-tuning foundation models locally, sensitive data does not need to leave the device, thereby enhancing data privacy. However, it is important to note that when FM is applied to data significantly different from the training distribution, performance degradation may occur. This issue, known as out-of-distribution generalization, represents a challenging aspect that FMs need to overcome. Moreover, if biases exist in the training dataset, FMs might learn and amplify these biases, leading to unfair outcomes across different data distributions. Although fine-tuning FMs requires significantly fewer resources than training from scratch, it usually still demands substantial computational resources for effective fine-tuning and updating. This requirement could limit their application on resource-constrained clients, particularly in FRS, where each user represents a client, thus potentially restricting the deployment of FM due to limited computational resources. Communicate Efficiency. Similar to FL, FFRS involves significant data transfer between numerous clients and a central server, making communication efficiency a critical factor in the duration of the entire learning process. To enhance communication efficiency, model compression techniques such as quantization or sparsification of model parameters can be utilized to reduce the volume of data transmitted. Periodic averaging is another strategy that reduces communication overhead by decreasing the frequency of model parameter uploads. FM, due to its parameter sharing and hierarchical features, allow clients to transmit only fine-tuned parameter updates rather than the entire models parameters. This approach can significantly reduce communication overhead while maintaining or even enhancing the models performance on specific tasks. Similarly, since foundation models have already learned rich feature representations during the pre-training phase, they can adapt more quickly to new tasks. This means that in FRS, clients may achieve satisfactory performance with fewer iterations, thus reducing the frequency of communication. Client Selection. In each iteration of FRS, considering resource limitations and privacy concerns, the central server may choose to communicate only with a subset of clients. Research indicates that clients should not participate in consecutive training rounds as attackers could potentially infer information about the client from transmitted gradients or model parameters. Client selection strategy determines which clients will participate in the current round based on various factors such as computational power, network stability, data diversity, and quality. Client selection can also be conducted through random sampling or based on statistical characteristics of the data to ensure the model learns from diverse data sources, enhancing its generalization ability. Since FM already possesses a broad knowledge base and generalization capabilities, when selecting clients for model training, greater emphasis can be placed on data compatibility and coverage. Specifically, priority can be given to clients that can provide data types less represented or missing in the pre-training phase, thereby supplementing and enhancing the models performance in these areas. For instance, clients whose data can significantly improve the models performance on specific tasks can be considered more valuable. Incentive mechanisms can be set up to encourage the participation of these high-value data providers in the training process. However, it is important to ensure that the same client does not participate in consecutive training rounds to minimize the risk of privacy breaches. Privacy Protection. A major advantage of federated learning is the ability to train models without compromising user privacy, making privacy protection a primary consideration in designing communication protocols. Privacy risks are mitigated by transmitting model updates, such as gradients or parameters, instead of raw data from clients to the server. Additionally, technologies like homomorphic encryption, secure multi-party computation (SMC), or differential privacy can further enhance privacy protection during communication. Global aggregation in FRS integrates model parameters independently trained by various clients to form a unified global model. This process not only enhances the accuracy and generalization of the recommendation system but also strengthens the systems robustness by protecting client data privacy, optimizing resource usage, ensuring model synchronization and fairness, thereby improving recommendation performance and efficient resource utilization without compromising user privacy. The Typical Average scheme, proposed with FedAvg [3], is the most common aggregation strategy where the server computes a simple arithmetic average of the model updates received from client devices. Its popular due to its simplicity and effectiveness in many scenarios In the Weighted Average strategy, model updates are weighted based on certain criteria such as the volume of data on each client or the reliability of the data source, allowing more significant contributions to have a proportionally greater impact on the global model. This approach can be more effective than the typical average scheme, especially in non-IID data environments, where data distribution varies significantly across devices When integrating FM, exploring new aggregation technologies becomes particularly important to address the challenges posed by the scale and complexity of the models. Beyond the traditional weighted average method, some emerging aggregation strategies have shown potential in managing these challenges. For instance, the technique called Model Soups enhances model accuracy and robustness by averaging the weights of models that have been fine-tuned with different hyperparameters. Additionally, aggregation strategies based on the Mixture of Experts (MoE) utilize multiple specialized sub-models, each optimized for specific tasks or data types. These strategies dynamically adjust model weights based on data-driven methods, allowing for flexible adjustment of the aggregation process in response to real-time data changes, thus improving the overall model performance and adaptability. These emerging technologies not only improve the effectiveness of aggregation but also better address the challenges from large-scale distributed data, making them valuable aggregation strategies worth further research and application in the field. Additionally, leveraging machine learning techniques to dynamically adjust aggregation methods based on real-time data quality and integrity assessments could improve both the robustness and efficiency of FRS. The integration of generative models into federated recommendation systems heralds a new frontier in personalized content delivery, promising to enhance user experiences with tailored suggestions while respecting privacy concerns. However, this promising union is not without its complexities. As we venture further into this domain, a myriad of challenges emerges, each posing unique hurdles that must be carefully navigated. These challenges span the spectrum from data heterogeneity and privacy concerns to communication overheads and resource-intensive computations. Understanding and addressing these issues is paramount to the successful deployment and operation of federated recommendation systems that are both effective and secure. In this section, we delve into each of these challenges, exploring their implications and discussing potential strategies for overcoming them. The diversity in user-generated data can complicate the generative models ability to identify a common representation that accurately reflects all users preferences. This challenge necessitates the development of robust mechanisms capable of handling varied data inputs and providing nuanced, precise recommendations. Techniques such as transfer learning and multi-task learning can be employed to better adapt to the heterogeneity of data. The collection of user data is essential for refining recommendation algorithms, but it also poses significant privacy and security risks. While federated learning offers a solution by processing data locally, generative models must be carefully designed to prevent the leakage of sensitive information through their generated outputs. Privacy-preserving methods like differential privacy and secure multi-party computation can be integrated to enhance data security. Membership inference attacks aim to determine whether specific data samples were used in training a model. Such attacks can reveal the presence of sensitive data, for example, by disclosing whether a patients medical records were used to train a disease prediction model. Under the FL setting, attackers might use model updates obtained from various participants to infer which data were used for training. Particularly in FRS with FM settings, the complexity and depth of the models make membership inference attacks more covert, thereby complicating defense efforts. Data reconstruction attacks aim to rebuild or approximate the actual data used in training. This is typically achieved through optimization techniques such as model inversion and gradient matching, where attackers attempt to generate data samples similar to the training data by accessing the models outputs or gradients. Under the FL setting, although the original data does not leave the owners device, aggregated updates may still leak sufficient information to enable attackers to reconstruct the original data. Particularly when large-scale foundation models are used, this risk may be exacerbated due to the models high capacity for data representation. In FL, poisoning attacks represent a security threat aimed at disrupting or manipulating the learning process and outcomes through malicious modifications to data or model parameters. These attacks primarily take two forms: data poisoning attacks and model poisoning attacks. Specifically: Untargeted Attacks: Untargeted attacks aim to disrupt the entire training process, causing the global model to fail to converge or significantly degrade in performance. These attacks are typically executed by injecting noisy data or incorrect information into the model training process. In a FL environment, attackers might submit model updates containing erroneous gradients or parameter updates, thereby disturbing the learning process of the aggregated global model. The challenge of these attacks lies in the ability of the attackers to pass the poisoned data through the FL systems normal aggregation process without detection. Targeted Attacks: In contrast, targeted attacks aim to cause the model to produce incorrect outputs for specific inputs, without affecting the models performance on most other inputs. This type of attack is usually carried out by injecting a small amount of carefully designed poisoned samples into the training data. These samples contain specific triggers that, when encountered by the model, lead to predetermined incorrect outputs. This attack method is particularly covert in FL, as attackers can embed these triggers during local training, and these modifications may only affect a small part of the model, making them difficult to detect in the global model. Federated systems necessitate ongoing communication between users and servers to synchronize model updates. The large parameter size of generative models can significantly increase the demand for communication resources. To address this, efficient communication protocols and model compression techniques can be utilized to reduce the bandwidth required for updates. The scarcity of user data is a significant challenge, particularly for new or less active users, as it can hinder the models ability to learn accurate preferences. To combat this, data augmentation strategies and synthetic data generation can be employed to enrich the training data and improve model generalization. Generative models are computationally demanding, requiring substantial storage and processing capabilities. This can be particularly challenging in resource-constrained environments, such as mobile devices. Optimizing model architectures and leveraging distributed computing can help alleviate the strain on resources. The quality of synthetic data generated by generative models is crucial. Low-quality synthetic data can negatively impact the models performance if used in training. Ensuring the synthetic data closely resembles real data distributions is essential for maintaining the integrity of the recommendation system. Generative models within federated recommendation systems must be robust against data anomalies and capable of maintaining stable performance even when faced with shifts in data distribution. This requires the implementation of robust learning algorithms that can adapt to changes in the data landscape without significant degradation in performance. This section discusses applications that highlight the great potential of generative models in federated recommender systems. These applications not only have the potential to improve recommendation quality, but also to provide personalised services while protecting user privacy. As technology continues to advance, we can expect generative models to play an increasingly important role in future FRSs. Generative models such as VAEs and GANs can be used to generate personalized user profiles within federated recommendation systems. These models can capture latent features of user preferences and generate recommendations that reflect these characteristics, providing more accurate personalized suggestions. Privacy is paramount in federated recommendation systems. Generative models can protect user data privacy by generating representations of user preferences locally and then only sending these representations to a central server for recommendations, without disclosing raw user data. Diffusion models and large language models can be used to generate additional training data, which is particularly useful in federated learning environments where data sparsity is often an issue. By generating synthetic data, the diversity of the training set can be increased, enhancing the performance of the recommendation system. Federated recommendation systems allow multiple organizations to share in the training of a recommendation model without sharing user data. Generative models can be used within this framework to generate cross-domain recommendations, sharing generated latent user features across different organizations rather than raw data. For new users or new items, traditional recommendation systems face the so-called ”cold start” problem. Generative models can predict the latent features of these new entities and generate initial recommendations, quickly initiating the recommendation process. Large language models can be used to generate explanations for recommendations, helping users understand why a particular piece of content is recommended. This is crucial for increasing user trust and acceptance of recommendation systems. Generative models can quickly produce recommendations, which is useful for recommendation scenarios that require real-time feedback, such as news recommendations or real-time event recommendations. By generating a diverse range of recommendations, generative models can help reduce biases in recommendation systems, providing more equitable recommendation outcomes. In federated recommendation systems, generative models can be used to simulate various user behaviors and preferences, thereby testing and improving the robustness of the recommendation system. Combining multiple types of data such as images, text, and audio, generative models can be used to create multimodal recommendations, such as recommending travel destinations that combine images with textual descriptions. eHealthcare systems leverage modern technology to enhance the quality and efficiency of medical services, such as remote diagnosis and patient monitoring. Federated recommendation systems allow different medical institutions to share insights from patient data without sharing the data itself, which is crucial for protecting sensitive health information. Generative models use this data to provide doctors and patients with personalized treatment plans and health advice, improving diagnostic accuracy and treatment outcomes. Thus, federated recommendation systems combined with generative models can integrate patient health data across institutions to create complex treatment pathways and simulate disease progression, enabling doctors to better predict patients responses to specific treatments and provide more precise medical services. In manufacturing, automation and intelligent technologies significantly enhance production efficiency and safety. Federated recommendation systems foster technological collaboration between different enterprises by sharing non-sensitive data, accelerating product design and optimization processes. This not only shortens product development cycles but also protects corporate trade secrets. Generative models can design new mechanical parts or product prototypes by simulating different design variables, quickly proposing multiple design solutions; they can also help predict machine failures, perform maintenance in advance, and reduce downtime, thereby enhancing production efficiency. Federated recommendation systems combined with generative models enable the sharing of improvements and innovations while maintaining data privacy between companies. Analyzing consumer behavior helps businesses deeply understand customer needs. Federated recommendation systems can integrate data from different service providers while protecting user privacy, achieving personalized services. Generative models provide personalized recommendations based on this data, enhancing user experience. For example, in travel recommendation systems, models generate customized travel itineraries based on users historical preferences. By simulating different consumer behavior patterns, businesses can predict market trends, adjust service strategies, and enhance competitiveness. Federated recommendation systems combined with generative models analyze user behavior data from different service providers to generate service plans that meet individual needs, promoting personalized service development. Applying recommendation systems to online shopping and retail provides a personalized shopping experience. Federated recommendation systems in the retail and e-commerce sectors analyze consumer shopping habits to generate personalized product recommendations, increasing users purchasing desire and satisfaction. Retailers and e-commerce platforms can use generative models to predict consumer buying behavior, thereby offering more accurate product recommendations, optimizing inventory, and enhancing sales efficiency. Federated recommendation systems combined with generative models allow retailers and e-commerce platforms to consider market conditions comprehensively, predicting consumer buying behavior and providing more precise product recommendations, optimizing inventory, and enhancing sales efficiency. In the financial sector, recommendation systems use data analysis and prediction models to optimize financial services, applicable for credit assessments, stock market analysis, etc., offering clients tailored financial products. Federated recommendation systems ensure transaction data privacy while offering clients tailored financial products. Generative models can simulate market behaviors, predicting stock prices or financial product returns. These models help financial institutions formulate more accurate investment strategies while reducing risks. In finance, federated recommendation systems combined with generative models help financial institutions protect client privacy while simulating market changes and user behavior, offering advice on investment products and credit strategies, thereby enhancing clients investment returns and satisfaction. In smart city applications, systems need to process and analyze large-scale urban data. From optimizing traffic flow to devising public safety strategies, federated recommendation systems can help city planners use resident data without directly sharing it to recommend strategies for improving public services and infrastructure, enhancing urban management efficiency and improving residents quality of life. Additionally, using generative models to simulate the effects of different strategies can find optimal solutions for managing and allocating urban resources. For example, by simulating traffic flow, models can predict congestion and propose solutions, improving urban traffic efficiency. Federated recommendation systems combined with generative models can help city planners optimize resource allocation and management efficiency without sharing resident data. Social platforms facilitate interpersonal communication and information sharing online, using recommendation systems to suggest content to users, such as friends or topics of interest, to increase user engagement. Federated recommendation systems can enhance user engagement and satisfaction while protecting user privacy. Generative models can generate personalized content recommendations, enhancing user engagement and satisfaction while ensuring the protection of user privacy, increasing platform attractiveness and user retention. In the field of education, using recommendation systems can improve teaching methods by making appropriate recommendations based on students progress and abilities, providing a personalized learning experience. Generative models can generate personalized learning materials and courses based on students progress and interests. These customized learning resources can enhance students learning efficiency and interest. Federated recommendation systems combined with generative models can generate personalized learning paths based on students feedback and learning history. By simulating teaching scenarios and student learning processes, these systems can provide resources that better meet individual learning needs. In FRS, the datasets used can be categorized into two types based on user feedback: explicit feedback and implicit feedback, as shown in Fig. 7. Explicit feedback includes direct responses from users about their preferences. This typically includes ratings (such as a 1-5 scale), user comments, and like/dislike statements. Such data is clear and provides straightforward insights into user preferences, making it highly valuable for training recommendation models. Common datasets with explicit feedback include the following shown in Table ?????????I: Amazon Reviews111https://cseweb.ucsd.edu/ jmcauley/datasets.html#amazon_reviews: The Amazon Reviews dataset is a large-scale dataset that contains product information across various categories such as Books, CDs, and Music. It includes reviews (ratings, text, helpfulness votes) and product metadata (description, category information, price, brand, and image features). There are three updated versions of this dataset from the years 2014 [64, 65], 2018 [66], and 2023 [67]. MovieLens Datasets222https://grouplens.org/datasets/movielens/: The MovieLens datasets [68], initially released in 1998, capture individuals stated movie preferences. These preferences are recorded as tuples, with each tuple showing a persons rating (from 0 to 5 stars) for a movie at a specific time. Users enter these ratings through the MovieLens website, which provides personalized movie suggestions based on these ratings. Yelp Datasets333https://www.yelp.com/dataset: This dataset is a subset of Yelps business, review, and user data. It was initially developed for the Yelp Dataset Challenge, which allows students to study or analyze Yelp data and present their insights. In total, there are four versions of the Yelp datasets. Anime444https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database: This dataset collects user preference data from the MyAnimeList website. It contains information from 73,516 users on 12,294 anime titles. Users can add anime to their completed list and rate them, and this dataset compiles these ratings. Book Crossing555https://grouplens.org/datasets/book-crossing/: The Book-Crossing dataset is a well-structured collection of data collected by Cai-Nicolas Ziegler in a 4-week crawl from the Book-Crossing community. This dataset primarily comprises user interactions that include book ratings, ranging from 0 to 10. Douban666https://www.kaggle.com/datasets/utmhikari/doubanmovieshortcomments: The Douban Movie dataset is a Chinese website where internet users can post their opinions and comments about films. This dataset contains over 2 million short comments on 28 movies from the Douban Movie website. Epinions777https://cseweb.ucsd.edu/ jmcauley/datasets.html#social_data [69]: This dataset was collected from Epinions.com, a popular online consumer review site. It includes trust relationships between users and covers a period from January 2001 to November 2013. Goodreads888https://www.kaggle.com/datasets/jealousleopard/goodreadsbooks: This dataset includes reviews from the book review website Goodreads, along with various attributes describing the books. Importantly, the dataset captures different levels of user interaction, from adding books to a shelf, to rating them, to reading them. Jester999https://eigentaste.berkeley.edu/dataset/: The Jester dataset focuses exclusively on jokes. Users of the Jester online platform rate jokes and these ratings are then used to personalize joke recommendations for them. Netflix101010https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data: Netflix provided a training dataset consisting of 100,480,507 ratings from 480,189 users for 17,770 films. Each rating is represented as a set of four elements: ¡user, movie, rating date, rating score¿. The user and the movie are identified by integer IDs, and the rating scores range from 1 to 5 stars, also as integers. Yahoo Music111111https://webscope.sandbox.yahoo.com/catalog.php?datatype=r: The Yahoo Music dataset is known for its large scale and diversity. It contains a large collection of user ratings on different musical elements such as tracks, albums, artists and genres. This dataset was used in the KDD-Cup 2011 competition, where participants were asked to analyse user preferences in music based on these ratings. Implicit feedback, on the other hand, is derived from user actions that indirectly indicate preferences, such as bookmarks, video/music play history, or click-throughs. Although implicit feedback does not directly express user likes or dislikes, it is rich and captures user behaviour more comprehensively. MIND121212https://msnews.github.io/ [70]: The MIND dataset, sourced from the Microsoft News website, is a large-scale collection of approximately 160,000 English news articles and over 15 million user interaction records. It has been designed to advance research in news recommendation systems. It includes detailed textual content for each story and anonymized user interaction data to ensure privacy. Tenrec131313https://github.com/yuangh-x/2022-NIPS-Tenrec [71]: The Tenrec dataset is a comprehensive benchmark dataset for RSs, featuring user interactions from two recommendation platforms across four dataset files: QK-video and QB-video for video actions, and QK-article and QB-article for article actions. Adressa141414https://reclab.idi.ntnu.no/dataset/ [72]: The Adressa dataset is a corpus of Norwegian news articles related to anonymous users. It is a collaborative project between the Norwegian University of Science and Technology and Adressavisen. The objective is to gain insight into the nature of news articles and their readers. Foursquare151515https://sites.google.com/site/yangdingqi/home/foursquare-dataset [73]: This dataset comprises check-in data from New York City, collected over a period of approximately ten months (from 12 April 2012 to 16 February 2013). It encompasses 227,428 check-ins in New York City, with each check-in recorded with its timestamp, GPS coordinates, and detailed venue category. Gowalla161616https://snap.stanford.edu/data/loc-gowalla.html [74]: Gowalla is a location-based social networking website where users can post their whereabouts by checking in. The dataset comprises data collected from the public API, which represents an undirected friendship network with 196,591 nodes and 950,327 connections. Additionally, it records 6,442,890 check-ins made by these users between February 2009 and October 2010. Last.FM171717https://grouplens.org/datasets/hetrec-2011/ [75]: The Last.FM dataset represents a valuable resource that has been extensively utilized in the field of music information retrieval and RSs. It captures detailed information regarding music listening events from users. Each listening event is further enhanced with user demographics and specific descriptors that reflect their music tastes and consumption behaviours. Pinterest181818https://github.com/edervishaj/pinterest-recsys-dataset [76]: The Pinterest dataset represents a valuable resource for a variety of research and analytical purposes. It encompasses a diverse range of data, including images, user features, interests, and user interactions. Steam191919https://github.com/kang205/SASRec [77]: The Steam dataset is a collection of information about games published on the Steam platform. It includes details such as game names, release dates, genres, developers, publishers, and other relevant information. TaFeng202020https://www.kaggle.com/datasets/chiranjivdas09/ta-feng-grocery-dataset: The TaFeng dataset is a comprehensive collection of supermarket shopping data, including detailed transaction records from the Ta Feng supermarket in Taiwan, covering a period from November 2000 to February 2001. The dataset comprises a variety of data points, including customer demographics, product categories, and detailed item descriptions along with quantities purchased. Tmall212121https://tianchi.aliyun.com/dataset/53[78]: The Tmall dataset is a comprehensive collection from Tmall, comprising anonymized user shopping records over a six-month period up to and including the ”Double 11” event. It should be noted that the data is selectively sampled to address privacy concerns. Both types of feedback play a critical role in the development of FRSs, providing diverse insights into user preferences that help improve the accuracy and relevance of the recommendations provided. In the field of FRSs, the use of evaluation metrics is fundamental to assessing and refining the performance of our algorithms. Metrics serve as a quantitative lens through which we can observe how closely the systems suggestions match users actual interests and preferences. For predicting how well a system can estimate user preferences, there are several metrices measuring the prediction errors to judge the accuracy of the predictions, such as Mean Absolute Error (MAE), Mean Squared Error (MSE) and Root Mean Squared Error (RMSE). When it comes to classifying items, i.e., determining whether a user will like a product or not, we look at metrics such as Precision, Recall, Hit Ratio (HR), F1 Score, Accuracy and AUC. These tell us how correctly the RS is classifying items, and the F1 score helps us balance the Precision and Recall. Then to measure the item ranking ability, which is about listing recommendations in the right order, Average Precision (AP) and Mean Average Precision (MAP) are key to this, as they assess the quality of the order of recommendations. Metrics such as Mean Reciprocal Rank (MRR), Normalized Mutual Rank (NMR) and Normalised Discounted Cumulative Gain (NDCG) also contribute by assessing how well the top recommended items are ranked. On a broader scale, we consider recommendation-centric metrics such as Diversity, which ensures that a variety of items are suggested, and Coverage, which measures how many items from the catalogue are recommended. There are also some user-centric metrics include Novelty, which measures how new or surprising the recommendations are, and Degree of Agreement (DOA), which quantifies the level of concordance between the ranking of items produced by a recommendation system and the ranking preferred by the user. Moreover, business metrics such as Click-Through Rate (CTR) are critical to assessing the systems impact on user engagement and the companys bottom line, and Conversion Rate (CVR) measures how efficient an algorithm is at providing recommendations that lead to user purchases. There also are some metrics for measuring other functionality. For example, Gini Index evaluates the fairness of recommendation distribution, with lower values indicating more equitable distribution across items. Furthermore, in the generative recommendation scenario, FMs such as DMs and LLMs can generate items that have never appeared in historical data and recommend them to users. In this case, how to evaluate the generative recommendation capability of these generative FMs remains an open question. This study comprehensively examines the integration of FRSs with FMs, a direction that has gained attention for its ability to protect user privacy. The article begins by summarizing common approaches of FRSs and FMs, then delves into the challenges faced during integration. To address these challenges, the paper proposes various strategies including using transfer learning and multitask learning techniques to adapt to data diversity, employing privacy-preserving methods like differential privacy and secure multi-party computation, and reducing communication overhead through model compression and efficient communication protocols. This work also discusses the future research directions directions indicating that FRSs can provide more accurate personalized recommendations while better protecting user privacy. Additionally, the paper showcases applications of FRS in various fields, demonstrating their potential and value in the real world. Through this study, we aim to provide theoretical guidance for integrating FRS with FM, directing future research and technological advancements to collectively advance this field.",
        "keywords": "Index Terms: \nFederated Recommendation System, Foundation Model, Privacy Preserving, Security,"
    },
    {
        "id": 12,
        "title": "Evaluating Transfer Learning in Deep Learning Models for Classification on a Custom Wildlife Dataset: Can YOLOv8 Surpass Other Architectures?",
        "abstract": "AbstractBiodiversity plays a crucial role in maintaining the balance of the ecosystem. However, poaching and unintentional human activities contribute to the decline in the population of many species. Hence, active monitoring is required to preserve these endangered species. Current human-led monitoring techniques are prone to errors and are labor-intensive. Therefore, we study the application of deep learning methods like Convolutional Neural Networks (CNNs) and transfer learning, which can aid in automating the process of monitoring endangered species. For this, we create our custom dataset utilizing trustworthy online databases like iNaturalist and ZooChat. To choose the best model for our use case, we compare the performance of different architectures like DenseNet, ResNet, VGGNet, and YOLOv8 on the custom wildlife dataset. Transfer learning reduces training time by freezing the pre-trained weights and replacing only the output layer with custom, fully connected layers designed for our dataset. Our results indicate that YOLOv8 performs better, achieving a training accuracy of 97.39 % and an F1 score of 96.50 %, surpassing other models. Our findings suggest that integrating YOLOv8 into conservation efforts could revolutionize wildlife monitoring with its high accuracy and efficiency, potentially transforming how endangered species are monitored and protected worldwide.",
        "corpus": "Biodiversity plays a crucial role in maintaining the balance of the ecosystem. However, poaching and unintentional human activities contribute to the decline in the population of many species. Hence, active monitoring is required to preserve these endangered species. Current human-led monitoring techniques are prone to errors and are labor-intensive. Therefore, we study the application of deep learning methods like Convolutional Neural Networks (CNNs) and transfer learning, which can aid in automating the process of monitoring endangered species. For this, we create our custom dataset utilizing trustworthy online databases like iNaturalist and ZooChat. To choose the best model for our use case, we compare the performance of different architectures like DenseNet, ResNet, VGGNet, and YOLOv8 on the custom wildlife dataset. Transfer learning reduces training time by freezing the pre-trained weights and replacing only the output layer with custom, fully connected layers designed for our dataset. Our results indicate that YOLOv8 performs better, achieving a training accuracy of 97.39 % and an F1 score of 96.50 %, surpassing other models. Our findings suggest that integrating YOLOv8 into conservation efforts could revolutionize wildlife monitoring with its high accuracy and efficiency, potentially transforming how endangered species are monitored and protected worldwide. *joint first authors. Convolutional Neural Network (CNN), Endangered Species Detection, Image Classification, Transfer Learning, YOLO. Each living creature in our world plays a vital role in maintaining the balance of the ecosystem. However, activities like poaching and illegal trade, reduction of prey base, habitat loss and degradation, and human-wildlife conflict have led to a rapid decline in the number of some species, including critically endangered ones [11]. Currently, wildlife monitoring is done using camera traps that capture the images of animals, which are then analyzed and studied by humans. This is time-consuming, tedious, and prone to errors. The introduction of various deep learning techniques and their use in computer vision shows optimistic results. Convolutional Neural Networks (CNNs), introduced by LeCun et al. in 1998 [14], have been a significant milestone in image classification tasks [22, 26, 12]. In these networks, images are fed through convolutional and pooling layers for feature extraction, followed by fully connected layers. Transfer learning, highlighted by Pan and Yang [18], is fine-tuning pre-trained models on large datasets for specified tasks that significantly reduce training time with increased performance. Densely connected convolutional networks, popularized by Huang et al. in 2017 [8], and residual networks, put forward by He et al. in 2016 [7], are deep and complex networks that facilitate effective feature extraction. However, VGGNet, proposed by Simonyan and Zisserman in 2014 [25], is somewhat computationally expensive, although its results are excellent in some cases.YOLOv8 is an evolution in Redmon et al.™s object detection and has shown remarkable results in various computer vision tasks [20]. El Abbadi et al. achieved a classification accuracy of 97.5% using a deep convolutional neural network model for the automated classification of vertebrate animals, thereby demonstrating the effectiveness of deep learning in animal recognition tasks [6]. Similarly, Villa et al. demonstrated in their study that very deep convolutional neural networks achieved 88.9% accuracy in a balanced dataset and 35.4% in an unbalanced one for automatic species identification in camera-trap images, marking a notable advancement in non-intrusive wildlife monitoring [27]. In 2020 Ibraheam et al. reported in their paper that their deep learning-based system achieved 99.8% accuracy in distinguishing between animals and humans, and 97.6% in identifying specific animal species, significantly improving safety in wildlife-human and wildlife-vehicle encounters [9]. Brust et al. have illustrated that ResNet50 outperformed VGG16 and Inception v3 on a wildlife image dataset, with the highest accuracy of 90.3 %. It provided insights into the strengths and weaknesses of different CNN architectures for wildlife classification [3]. Similarly, Beery et al. 2018 validated the same on a challenging dataset of wildlife images with occlusions, varying lighting conditions, and motion blur. They found the Faster R-CNN model achieved the highest accuracy of 88.7 %, indicating the importance of model robustness in real-world applications [2]. Similarly, Yilmaz et al. in 2021 demonstrated that the YOLOv4 algorithm achieved a high classification accuracy of 92.85 % for cattle breed detection [29], emphasizing its effectiveness in wildlife classification tasks. M. Kumar et al. found that YOLOv4 was the most effective of the several deep learning-based models, including SSD and YOLOv5, achieving an accuracy of 95.43 % in their bird classification task [13]. Hung Nguyen et al. achieved 96.6 % accuracy in detecting animal images and 90.4 % accuracy in species identification using deep learning, highlighting its potential for automatically monitoring wildlife [17]. This paper answers some of the burning questions regarding selecting deep learning models for practical wildlife conservation tasks, such as which models provide the best accuracy and efficiency, how YOLOv8 compares to DenseNet, ResNet, and VGGNet, and what challenges and limitations exist in applying these models to wildlife conservation. Our results show that YOLOv8 is best suited for automated wildlife monitoring with much better accuracy and efficiency than models such as DenseNet, ResNet, and VGGNet. This work will supply essential guidance to researchers and practitioners on the choice of appropriate models for endangered species conservation. Addressing these research questions is crucial as the current methodologies for wildlife monitoring are labor-intensive and error-prone. This work aims to fill this gap by systematically evaluating different deep learning models and providing essential guidance to researchers and practitioners on the choice of appropriate models for endangered species conservation. In this paper, we begin by reviewing related works and current methodologies in wildlife conservation. In Section 2, we describe our dataset and the preprocessing steps involved and a detailed overview of the methodologies employed in our study. Following this, Section 3 elaborates on the performance and evaluation metrics used for our machine learning models. In Section 4, we present and discuss the experimental results. Finally, we conclude in Section 5 with a summary of our findings and suggestions for future work. Our dataset includes 23 each carefully selected based on its conservation status and the need for monitoring. These species cover many endangered animals, including mammals, reptiles, and amphibians as shown in table 1. Our study began by collecting the data from different sources on the internet. Each class is represented by 50 filtered images, resulting in 1150 images in our dataset. We maintained a balanced dataset to reduce the bias towards any particular species and to perform the balanced model training. The internet houses a huge amount of data, but finding and gathering useful ones is still challenging. To collect images of various animal species, we utilized online repositories like iNaturalist and ZooChat [10, 30]. The good thing about using such sites is their authenticity and fair use policy. For the same reason, we did not use the images shown in regular Google searches or try to automate the process. We /d the preprocessing of the image data into the following steps. We preprocessed all the images utilized for our study to have an aspect ratio of 1:1 and a resolution of 400 x 400. We added padding whenever required, ensuring we did not lose any information from the images during resizing. We normalized every image to increase accuracy and speed up the model™s convergence. It also reduced the variance in the training data. We split the dataset into train (80 %) and val (20 %) sets. We used the split-folders Python package to split the dataset while maintaining the original distribution [4]. Babu et al.™s study examines the impact of image data splitting on the performance of machine learning models [1]. For better generalization, we increased the diversity of data by applying different augmentation techniques, as suggested by Shorten and Khoshgoftaar in 2019 [24]. While their survey offered valuable insights into the importance of data augmentation for enhancing the model™s performance, we customized the specific methods to our dataset and requirements through numerous experiments. Table 2 displays the final set of parameters for data augmentation. Convolutional Neural Networks are a special kind of neural network designed to work with grid data like images. They learn by extracting the features from input data via convolution and pooling operations followed by fully connected layers [14]. CNNs have played a pivotal role in the advancement of computer vision and related tasks. They are especially effective in performing tasks such as image recognition, object detection, and classification [5, 16, 23]. Transfer learning is an approach to deep learning that enables researchers and developers to use the previously trained model in a huge dataset and implement it in downstream tasks [22]. It is especially useful when we have limited data to train the model. Also, it significantly reduces the training time because the feature extraction part remains unchanged. However, it may fail when there is a significant mismatch between the target domain task and the source task. These are some of the models we used to compare transfer learning with our dataset. Figure 3 shows the corresponding architecture of these models. DenseNet: DenseNet was introduced by Gao Huang and colleagues in 2017 [26]. It connects each layer with all other layers densely, meaning each layer receives input from the preceding layers. By efficiently cutting the parameters™ requirements and boosting the network™s ability by reusing the features extracted, it allows for a deeper network. However, the dense connectivity may lead to increased computational cost and memory consumption. ResNet: ResNet was introduced by Kaiming He et al. in 2016 [12]. It utilizes residual blocks, which are shortcut connections that bypass one or multiple layers. It also allows the network to learn residual functions instead of direct mapping. This architecture supports very deep networks without degradation problems. VGG: VGG was introduced by Karen Simonyan and Andrew Zisserman in 2014 [18]. It is known for its simplicity and depth, achieved by stacking small 3*3 convolutional layers, increasing the model™s depth and parameters. The max pooling layers follow the convolutional layers to handle the volume size and end with the fully connected layers. YOLOv8: YOLOv8 is built upon the object detection framework introduced by Joseph Redmon, with contributions from many researchers over successive versions. It enhances speed and accuracy through an advanced backbone architecture, refined loss functions, and anchor-free detections [21]. Figure 4 shows the architecture of the YOLOv8. We loaded the pre-trained models(DenseNet, ResNet and VGG, ) with their respective weights, made these weights untrainable(frozen), and replaced the last layer with custom, fully connected layers corresponding to the number of classes in our dataset. We added a GlobalAveragePooling2D layer to reduce the feature maps™ spatial dimensions and prevent overfitting. This layer is followed by a Dense layer with 128 neurons and activated by ReLU to introduce non-linearity and learn more complex features. Finally, we added a Dense layer with 23 units activated by softmax to match the number of classes in our dataset, enabling the model to output the class probabilities, as shown in Figure 5. We then trained the models using Adam as the optimizer and cross-entropy as the loss function for 100 epochs with a batch size of 32 images. Meanwhile, we used a validation set to monitor the progress to avoid plateauing and adjusting the learning rate dynamically. We experimented with multiple sets of hyperparameters, including learning rates, optimizers, batch sizes, and schedulers, thereby selecting the most favorable settings of hyperparameters that showed optimal performance. Table 3 lists the final set of hyperparameters. Categorical Cross Entropy: Categorical Cross Entropy (CCE) is used for multi-class classification tasks. It measures the difference between the true class labels and the predicted class probabilities. The formula sums the negative log-likelihood of the true class™s predicted probability across all classes. We have used CCE as our loss function. Binary Cross Entropy for One Neuron: Binary Cross Entropy (BCE) is used for binary classification tasks. The binary cross-entropy loss measures the dissimilarity between predicted probability distributions and the ground truth labels. The formula is the negative log-likelihood of the predicted probability if the true label is 1 and 1 minus the predicted probability if the true label is 0. The BCE is utilized in each output layer neuron and used in the YOLOv8 classification task. AdamW Optimizer: AdamW stands for Adaptive Moment Estimation with Weight Decay. It is an extension of the Adam optimizer, including weight decay, to improve generalization by preventing overfitting. AdamW adjusts the learning rate based on the gradients™ first and second moments and consists of a weight decay term to penalize large weights. Where: ÎtsubscriptÎð¡ _ t is the parameter at time step tð¡tt Î±ð¼ is the learning rate mtsubscriptðð¡m_{t}m _ t is the biased first-moment estimate ð•tsubscriptð•ð¡ _ t is the biased second raw moment estimate µitalic-µ is a small constant to prevent division by zero weight_decay is the weight decay term [24]. The performance of our models is not examined by accuracy alone. In addition to accuracy, other metrics like f1-score, precision, recall, and loss were employed for the evaluation. Precision is an indicator of the accuracy of model predictions i.e., the ratio of the true positive predictions to the total number of positive predictions made by the model. The recall is an indicator of the model™s ability to identify all the relevant classes i.e., ratio of true positive predictions to the total number of actual positive instances. The F1 score provides the single value for the evaluation of the model and is the harmonic mean of precision and recall. The loss gives insights into how well the model™s prediction matches the true outcomes and is the difference between the predicted values and the actual values. Our study evaluated multiple deep-learning models for identifying endangered animal species from wildlife images. The results show the varying performance across the models. YOLOv8 outperformed the other models, and the DenseNet and Resnet models also did well, as their results were close to those of YOLOv8. In contrast, the VGG and Vanilla CNN models faced significant challenges. Table 4 shows the performances of different models. DenseNet: DenseNet architectures showed strong performance across all metrics. DenseNet 169 achieved a training accuracy of 98.27% and a validation accuracy of 93.91%. Also, it had F1 scores of 95.22% in training and 93.95% in validation. Impressively, DenseNet 201 outperformed by a slightly better training accuracy of 98.80% and had F1-scores of 96.36% in training and 92.22% in validation, though its validation accuracy was somewhat lower at 92.17%. Figure 7 shows training and validation set loss curves decreasing rapidly, finally reaching a plateau, indicating good learning and model convergence. The relatively smooth curve showed a minimal gap in Figure 7, indicating good generalization and no significant overfitting. ResNet: The ResNets architecture also showed promising results. Resnet 101 V2 reached a training accuracy of 98.74% and a validation accuracy of 92.17%. Its F1 scores were 97.36% in training and 92.09% in validation, showing robust performance. Resnet 152 V2 showed a training accuracy of 98.20% and a validation accuracy of 93.04%, with high F1-scores of 95.79% in training and 93.22% in validation. The training and validation data loss plunged drastically initially and finally tended towards low values, similar to DenseNet169, as shown in Figure 9. The very close train and validation loss curves indicated that the model generalized well without overfitting”a similar trend in the training and validation accuracy from Figure 9. VGG: In contrast, the VGG architectures underperformed significantly compared to DenseNet and ResNet models. VGG 16 and VGG 19 showed much lower training accuracies (46.00% and 32.67%, respectively) and validation accuracies (33.91% and 33.04%, respectively). Their F1 scores were also considerably lower, with VGG 16 at 42.89% for training and 28.65% for validation and VGG 19 at 29.82% for training and 31.19% for validation. The training loss decreased steadily, while the validation loss followed a different pattern, decreasing even slower and with apparent variance, as shown in Figure 11. There was a visible gap between training and validation loss, showing the possible overfitting phenomenon, i.e., a model fitting much better with the training set than the validation set. Also, Figure 11 shows oscillations in the training and validation accuracies. YOLOv8: The YOLOV8 model achieved an accuracy of 97.39% in training and 99.13% in validation with a shallow loss of 0.01175, which showed that despite being known for detection purposes, it surprisingly worked well for our classification task. The F1 score was 96.5% in training and 99.12% in validation. Figure 13 shows that the loss decreased rapidly and plateaued early in training, which shows the model™s effectiveness for fast convergence during transfer learning. Unlike the loss, the accuracy oscillated slightly before it finally plateaued near 50 epochs, as indicated in Figure 13. CNN: We validated different architectures for the vanilla CNN with the same dataset. However, we did not get satisfactory results. Since our dataset contained images of species across 23 classes, simple, shallow CNN could not converge effectively. Nevertheless, we tried adjusting various parameters like the number of layers, regularization techniques, loss functions, and augmentations. Despite these efforts, the performance metrics remained mediocre. Our experimental analysis shows the metrics of various deep-learning models in identifying endangered animals on our custom endangered wildlife dataset, as shown in Figure 14. The dataset was carefully created from reputable online databases, ensuring the species™ authenticity and relevance. Our motto was to train a model that could recognize vulnerable species and assist in their proper monitoring. Furthermore, the other side of our study involved finding the best available architecture for this task. We experimented with various such architectures and analyzed their performance under different metrics. We found that newer version of all the models showed stronger performance. VGG was the oldest among our models, so it performed poorly. Its accuracy was way below the vanilla CNN. Conversely, DenseNet and ResNet offered significantly better performance, so we can easily use them for related tasks. Also, we noted that these networks™ newer and deeper versions did not provide any impactful difference across various metrics. In addition, we implemented transfer learning and did not train them from scratch, benchmarking their ease of use in downstream tasks like ours. We had to freeze the feature extraction layers and only trained the last few fully connected layers. Doing this saved the time and computing required without compromising their performance. Alongside standard CNN models like ResNet, DenseNet, and VGGNet, specially designed for classification tasks, we also experimented with YOLOv8. YOLOs are primarily designed and used as go-to models for detection and segmentation tasks. To our great surprise, the metrics surpassed other standard classification models. Thus, YOLO might work well for classification tasks for custom datasets in similar niches. Why does YOLOv8 perform the best? YOLOv8™s superior performance might be due to its advanced architecture, which combines efficient feature extraction with fast processing capabilities by integrating components like CSPNet (Cross Stage Partial Networks) and PANet (Path Aggregation Network) [28, 15]. CSPNet reduces the computational cost while maintaining accuracy. It /s the feature map into two parts and merges them through the cross-stage hierarchy. On the other hand, PANet enhances the information flow between various layers, which is excellent for object detection across multi-scales. But, this proved beneficial for classification tasks, too. Even more importantly, the multiscale capability of YOLOv8 allows handling images with object sizes that can have significant variations and differing resolutions. It also has advanced augmentations, such as mosaic augmentation, which places four training images into one with diverse contexts in one image and hence helps the model generalize better to varying lighting and environmental conditions in training. All these features, together, carry out fast processing and effective feature extraction. To sum up, we performed experimental analysis on transfer learning of various deep-learning CNN architectures on our custom dataset containing images of endangered mammals from Nepal. Our findings featured the superior performance of YOLOv8 compared to other models like DenseNet, ResNetV2, and VGG. It demonstrated higher accuracy, precision, and recall, making it practical for our and similar classification tasks. Although lagging by a narrow margin, other models like ResNet and DenseNet also performed well and competed neck and neck with YOLOv8. Transfer learning proved beneficial, drastically reducing training time and data required while maintaining high performance, which is crucial for tasks with limited data availability. In the future, we will explore the ensemble methods that combine the strengths of multiple CNN architectures potentially enhancing classification accuracy and robustness, especially in diverse and challenging environmental conditions. We will also incorporate real-time monitoring capabilities in the future to provide feedback for conservation and take timely action in preventing the loss of endangered species. Hence, this study shows the reliance and robustness of deep-learning models in monitoring wildlife. This research received no external funding. The authors declare that there is no conflict of interest regarding the publication of this paper. The authors thank their supervisor, Dr. Mansi Bhavsar, for their invaluable guidance and support. Both authors contributed equally to this work.",
        "keywords": ""
    },
    {
        "id": 13,
        "title": "Bonus-malus Systems vs Delays in Claim Settlements: Analysis of Ruin Probabilities",
        "abstract": "AbstractOur paper explores a discrete-time risk model with time-varying premiums, investigating two types of correlated claims: main claims and by-claims. Settlement of the by-claims can be delayed for one time period, representing real-world insurance practices. We examine two premium principles based on reported and settled claims, using recursively computable finite-time ruin probabilities to evaluate the performance of time-varying premiums. Our findings suggest that, under specific assumptions, a higher probability of by-claim settlement delays leads to lower ruin probabilities. Moreover, a stronger correlation between main claims and their associated by-claims results in higher ruin probabilities. Lastly, the premium adjustment principles based on settled claims experience contribute to higher ruin probabilities compared to those based on reported claims experience, assuming all other factors remain constant. Notably, this difference becomes more pronounced when there is a high likelihood of by-claim delays.Keywords:Discrete-time risk model; Finite-time ruin; Recursive computation; Bonus-malus; Delayed claim",
        "corpus": "Our paper explores a discrete-time risk model with time-varying premiums, investigating two types of correlated claims: main claims and by-claims. Settlement of the by-claims can be delayed for one time period, representing real-world insurance practices. We examine two premium principles based on reported and settled claims, using recursively computable finite-time ruin probabilities to evaluate the performance of time-varying premiums. Our findings suggest that, under specific assumptions, a higher probability of by-claim settlement delays leads to lower ruin probabilities. Moreover, a stronger correlation between main claims and their associated by-claims results in higher ruin probabilities. Lastly, the premium adjustment principles based on settled claims experience contribute to higher ruin probabilities compared to those based on reported claims experience, assuming all other factors remain constant. Notably, this difference becomes more pronounced when there is a high likelihood of by-claim delays. Keywords: Discrete-time risk model; Finite-time ruin; Recursive computation; Bonus-malus; Delayed claim Due to the nature of the insurance business, certain insurers often need to deal with the issue of delayed claim settlements. Many factors prevent insurers from settling claims promptly after the claims are lodged. One of the main causes of delayed claims settlement is the investigation time insurers spend on verifying and assessing the reported claims. A typical example is casualty insurance. According to the usual claiming process of casualty insurance policies, after the policyholders notify the insurance company of the incident that causes loss or damage to their property, the surveyor/loss assessor will detect the reported damage to evaluate the repair/replacement cost. This process may also involve the police department and some third parties, so it may require a lot of time, which results in delayed claims settlement. Another cause of delayed claims settlement is delayed claim reporting. This issue occurs when the policyholder reports a previously incurred insurable loss to the insurer after their insurance policy has expired. In insurance terminology, this type of claim is known as incurred-but-not-reported claims or simply IBNR claims. As the name says, these claims are not reported in a timely manner which certainly delays the whole process of dealing with the claims. In term of the solvency risk, the delayed claims have a significant impact on the loss modelling by actuaries, since the timing of settled claims are inconsistent with the incident occurrence times. It may lead to the underestimation/ overestimation of claim experience in the time period under consideration which will reduce the effectiveness of the insolvency measures developed by usual loss models. Therefore, researchers and practitioners derived methods to deal with delayed or IBNR claims. A well-known approach to dealing with the IBNR claims is the chain ladder method (CLM). It uses the run-off triangles to help insurance companies estimate their required claim reserves involving IBNR losses. In ruin theory, risk models with delayed claims are developed to complement the classical risk model. This type of generalisation relaxes the assumption that claims settlements and claim reporting occur in the same financial period. As a result, the risk models with delayed claims are better connected with real-life insurance practice and attracted much attention from researchers in the literature. Regarding the relevant literature, Waters and Papatriandafylou (1985) derived the upper bounds for the ruin probability of a risk process with delayed claims settlement. Yuen and Guo (2001) studied the ruin probabilities for time-correlated claims in the compound binomial risk model. They introduced the principle of delayed claims in their model by defining the term main claims™, which refers to the initial claims that induce another type of claims, so-called by-claims, with different severity distributions and time occurrence. According to their models, the main claims and by-claims are assumed to be independent, which is a restrictive assumption. Some similar models can be found in Wu and Yuen (2004), which is an extension of Yuen and Guo (2001) by considering the interaction of the dependent classes of business in the models. Xiao and Guo (2007) studied the joint distribution of the surplus immediately prior to ruin and deficit at ruin in the compound binomial risk model with time-correlated claims and its relationship with the classical compound binomial risk model. Moreover, Trufin et al. (2011) and Ahn et al. (2018) studied the ruin probability with IBNR claims. Yuen et al. (2005) applied the martingale theory to obtain the expression for the ultimate ruin probability with the corresponding Lundberg exponent of its non-delayed risk model. Zou and Xie (2010) considered the case that the claims number process follows the Erlang(2) process and derived the explicit expression for the survival probability when both the main claims amount and by-claims amount are exponentially distributed. Dassios and Zhao (2013) obtained an asymptotic expression for the ruin probability with delayed claims by exploiting the non-homogenous Poisson model. Besides, the studies of an approximation of the ruin probability with delayed claims can also be found in Gao et al. (2019) and Yang and Li (2019). For the /nd problem in the risk models with time-delayed claims, Wu and Li (2012) studied the expected present value of /nd payments up to the time of ruin by considering a constant /nd barrier, whereas Zhou et al. (2013) studied a similar problem and assumed that the premium income is governed by the binomial process. Liu and Zhang (2015) considered a randomized /nd strategy for the study of the expected present value of /nd payments up to the time of ruin. Further, the literature concerning the penalty function in the risk models with time-delayed claims can be found in Yuen et al. (2013), Zhu et al. (2014), Liu and Bao (2015), Xie and Zou (2017), Wat et al. (2018), Deng et al. (2018), Zou and Xie (2019) and Liu et al. (2020). In this paper, we will extend the study of Yuen and Guo (2001) by assuming that periodic premiums are adjustable and are controlled by previous claims experience. This extension is inspired by the well-known principle in the non-life insurance business, the so-called Bonus-Malus system, which allows the insurers to determine renewal premium levels based on the historical claims experience of the policyholders under consideration. The traditional bonus-malus systems are at the granular level, i.e. at the policyholder level, which ignores the overall financial status of the insurance company. To address the issue, we adopt the portfolio-dependent premium correction framework that is considered crudely, i.e. on the portfolio level or higher, which enables us to incorporate it into the risk models and to study the corresponding ruin probabilities. As a result, the proposed models in this study can be used to evaluate the risk of ruin for insurers who have to face both delayed claim settlements and varying premiums in their everyday business, such as automobile insurance companies. Studies of risk models with varying premiums can be found in various papers in the literature. For example, Trufin and Loisel (2013) studied the discrete-time risk models with premiums adjusted to the claims by BÃ¼hlmann credibility. Another model in a discrete-time setting can be found in Wu et al. (2015), who used a two-state Markov Chain model to express the ultimate ruin probabilities in terms of both recursive formulae and explicit forms. The related literature regarding the continuous-time setting can be found in Li et al. (2015a) and Constantinescu et al. (2016). In the study of Li et al. (2015a), the premiums were assumed to be adjusted according to the historical claims number, whereas the study of Constantinescu et al. (2016) assumed that the premiums are adjusted according to the change in the inter-arrival time distribution between claims. Additionally, Osatakul and Wu (2021) studied the risk models with claim-dependent premiums and also considered the external environment for their models. Further studies concerning the risk models with varying premiums can also be found in Afonso et al. (2010), Li et al. (2015b), KucerovskÃ½ and Najafabadi (2017), Afonso et al. (2017) and Afonso et al. (2020) for the continuous-time setting, and Dufresne (1988) and Wagner (2001) for the discrete-time setting. In this paper, we inherit the assumptions regarding main claims and by-claims from Wu and Li (2012), which weakened the assumptions in Yuen and Guo (2001) by allowing the dependence between main claims and by-claims. It is worth mentioning that if premiums are to be adjusted by the settled claim experience, then the underlying premium status process would display an in-homogeneous nature, because the transition probability between any two premium levels vary from time to time due to the uncertainty in the settled claims. This property differs from the homogeneity property of the premium status process should the premiums be adjusted by the reported claim experience. This interesting contrast makes our discussions in this paper more realistic. This paper aims to answer to following questions: What is the impact of the probability of claims settlement delays on the ruin probabilities? What is the impact of the correlation between the main claims and by-claims on the ruin probabilities? Which of the premium adjustment strategies should be implemented by the insurers? In our study, we propose four premium adjustment principles: adjusting by aggregate reported claims, by aggregate settled claims, by reported number of claims and by settled number of claims. This paper is organised as follows. Section 2 presents the models and assumptions of our study. Section 3 to 6 presents results for the finite-time ruin probabilities under each of the above four premium adjustment principles respectively. Numerical examples showcasing our theoretical results in this paper are given in Section 7 with detailed discussions. Concluding remarks and potential future research are given in Section 8. We first define a surplus process of discrete times, denoted by UksubscriptU_{k}U _ k , as where U0•:={0,1,2,¦}subscript0•assign012¦U_{0} _ 0 N := { 0 , 1 , 2 , ¦ } is the initial surplus, Stsubscript†¡S_{t}S _ t is the total amount of settled claims during the t¡ttth unit time period payable at time t¡tt, and Ctsubscript¶¡C_{t}C _ t is the premium of the t¡ttth period received at the beginning of the period. In this paper we aim to study varying premiums. Let :={c1,c2,¦,cl}assignsubscript1subscript2¦subscript™ := { c _ 1 , c _ 2 , ¦ , c _ l } be the set of premium levels and “={1,2,¦,l}“12¦™ = { 1 , 2 , ¦ , l }. Without losing generality, we let c1<¦<cl•+:={1,2,¦}subscript1¦subscript™superscript•assign12¦c_{1}<...<c_{l} _ 1 < ¦ < c _ l N ^ + := { 1 , 2 , ¦ }. As we mentioned previously, there are two types of reported individual claims, i.e. main claims and the associated by-claims. They are denoted by Xtsubscript‹¡X_{t}X _ t and Ytsubscript¡Y_{t}Y _ t respectively for t•+¡superscript•t N ^ + . In this paper, we only consider a very simple case where there is at most one main claim in any time period, and one main claim generates at most one by-claim. Both {Xt}t•+subscriptsubscript‹¡¡superscript• X _ t } _ t N ^ + and {Yt}t•+subscriptsubscript¡¡superscript• Y _ t } _ t N ^ + are independent and identically distributed (i.i.d.) sequences of random variables with common probability mass function (p.m.f.) fX(x),x•subscript“‹¥¥•f_{X}(x),x _ X ( x ) , x N, and fY(y),y•subscript“¦¦•f_{Y}(y),y _ Y ( y ) , y N, respectively. On the other hand, Xtsubscript‹¡X_{t}X _ t and Ytsubscript¡Y_{t}Y _ t are assumed to be correlated with common joint p.m.f. fXY(x,y)subscript“‹¥¦f_{XY}(x,y)f _ X Y ( x , y ), x,y•¥¦•x,y , y N. Not surprisingly, one can see that fXY(0,y)=0subscript“‹0¦0f_{XY}(0,y)=0f _ X Y ( 0 , y ) = 0 for y 0¦0y 0y 0. Assume that main claims are always settled at the end of the reporting time period, which is not the case for by-claims. When a by-claim YksubscriptY_{k}Y _ k occurs, there is a probability 0¤q¤1010 q 10 ¤ q ¤ 1 that its settlement will be delayed to the end of the (k+1)1(k+1)( k + 1 )th period. Further, the settlement delays of Yk,k=1,2,¦,formulae-sequencesubscript12¦Y_{k},k=1,2, _ k , k = 1 , 2 , ¦ , are independent of each other. Thus, the aggregate claim amount settled in time period t¡tt is For a given time horizon n•+superscript•n N ^ + , the finite-time ruin probability of UksubscriptU_{k}U _ k with initial premium level cisubscriptc_{i}c _ i , for i““i caligraphic_L, is defined as where the subscript uuu represents the condition U0=usubscript0U_{0}=uU _ 0 = u. We have i(u,n)=1subscript“1 _ i ( u , n ) = 1 for u<0,n¥0formulae-sequence00u<0,n 0u < 0 , n ¥ 0 and i(u,0)=0subscript“00 _ i ( u , 0 ) = 0 for u¥00u 0u ¥ 0 by convention. Remark We assume that there is no delayed by-claim from the time period before the initial time 0. Then, S1subscript†1S_{1}S _ 1 can only be X1subscript‹1X_{1}X _ 1 or X1+Y1subscript‹1subscript1X_{1}+Y_{1}X _ 1 + Y _ 1 . Next we shall develop some recursive algorithm to compute the finite-time ruin probabilities under the above proposed risk framework. To enable our derivations, we define the following auxiliary surplus process with an up-front delayed by-claim where Y0>0subscript00Y_{0}>0Y _ 0 > 0 is the up-front delayed by-claim and other notations are exactly the same as those in model (2.1). Assume that Y0subscript0Y_{0}Y _ 0 is independent of all other random components in model (2.4) and follows the p.m.f. fY(y)subscript“¦f_{Y}(y)f _ Y ( y ). The corresponding nnn-period finite time ruin probability with initial premium level cisubscriptc_{i}c _ i , i““i caligraphic_L, is defined as Again, i²(u;z,n)=1superscriptsubscript“²§1 _ i ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( u ; z , n ) = 1 for u<0,n¥0formulae-sequence00u<0,n 0u < 0 , n ¥ 0 and i²(u;z,0)=0superscriptsubscript“²§00 _ i ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( u ; z , 0 ) = 0 for u¥00u 0u ¥ 0 by convention. In the following sections, we shall consider four different premium changing principles, i.e. premiums adjusted according to aggregate reported claims, premiums adjusted according to aggregate settled claims, premiums adjusted according to the reported claim frequency, and premiums adjusted according to the settled claim frequency, respectively. The premium changing rule considered in this section allows the next periodic premium to be determined based on the current premium level as well as the total reported claims in the current time period. In our previous model setting, we can see that the total reported claims in time period kkk is Xk+Yksubscript‹subscriptX_{k}+Y_{k}X _ k + Y _ k . Whether the settlement of YksubscriptY_{k}Y _ k is delayed or not does not have impact on the next periodic premium level. We define a bonus-malus system Î”=(“,,i)Î”“ = ( T , c , i ), where i““i caligraphic_L is the state of initial premium level; “={tij(s)}i,j“;s•“subscriptsubscript¡ formulae-sequence“ • = { t _ i j ( s ) } _ i , j caligraphic_L ; s N denotes a general set of time-homogeneous rules for premium variations. For any s• •s N and k•+superscript•k N ^ + , tij(s)=1subscript¡ 1t_{ij}(s)=1t _ i j ( s ) = 1 if the total reported claim amount s ss in time period kkk leads to the transition from premium level Ck=cisubscript¶subscriptC_{k}=c_{i}C _ k = c _ i to Ck+1=cjsubscript¶1subscriptC_{k+1}=c_{j}C _ k + 1 = c _ j and tij(s)=0subscript¡ 0t_{ij}(s)=0t _ i j ( s ) = 0 otherwise. For any k•+superscript•k N ^ + , the probability that the premium level moves from level cisubscriptc_{i}c _ i in time period kkk to level cjsubscriptc_{j}c _ j in time period k+11k+1k + 1 is defined by Using (3.6), one can obtain a one-step transition probability matrix for the premium level Markov process Before we present our first main result, we would like to show a simple relationship between the two finite-time ruin probabilities defined before, which will benefit our following discussions. When premiums are adjusted according to aggregate reported claims, the finite-time ruin probabilities “ and ²superscript“² ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT satisfy the following relationship, for n•+superscript•n N ^ + , Proof. Because the premiums are adjusted according to the total reported claims, the up-front delayed claim Y0subscript0Y_{0}Y _ 0 has no impact on how the next premium is going to change. When 0<z¤u0§0<z u0 < z ¤ u, it can be seen from (2.3) that Uk²subscriptsuperscript²U^{ ^ ² _ k with initial surplus uuu is equivalent to UksubscriptU_{k}U _ k with initial surplus uz¥0§0u-z 0u - z ¥ 0. So the first case of (3.10) holds. When z>u+ci,§subscriptz>u+c_{i},z > u + c _ i , the delayed by-claim is large enough to cause ruin, no matter whether there is any new claim in time period 1. ¡¡ Before we present our first main result, we introduce an auxiliary function that is used to simplify our main results given within the rest of this paper: Our first main result is given below. Given initial surplus u¥00u 0u ¥ 0 and initial premium level cisubscriptc_{i}c _ i , i““i caligraphic_L, the finite-time ruin probability with premiums adjusted according to aggregate reported claims without the up-front delayed by-claim satisfies the following recursive formula, for n•+superscript•n N ^ + , where i(u,1)=(1q)y=1Î¾y(u+ci)+x=u+ci+1fX(x)subscript“11superscriptsubscript¦1subscript¦subscriptsuperscriptsubscript¥subscript1subscript“‹¥ _ i ( u , 1 ) = ( 1 - q ) _ y = 1 ^ Î¾ _ y ( u + c _ i ) + _ x = u + c _ i + 1 ^ f _ X ( x ). Proof. From (2.3), we have where the three major terms within the second equality represent all possibilities of the main claim and by-claim within the first time period. The first term is the scenario that the main claim in the first time period is large enough to cause ruin at time 1. It does not matter whether there is a by-claim or not in this case. The second term covers three scenarios: no claims within the first time period at all, i.e. X1=Y1=0subscript‹1subscript10X_{1}=Y_{1}=0X _ 1 = Y _ 1 = 0; only a small by-claim without any by-claims, i.e. X1=x,Y1=0formulae-sequencesubscript‹1¥subscript10X_{1}=x,Y_{1}=0X _ 1 = x , Y _ 1 = 0, 1¤x¤u+ci1¥subscript1 x u+c_{i}1 ¤ x ¤ u + c _ i ; a small main claim and a small by-claim satisfying X1+Y1¤u+cisubscript‹1subscript1subscriptX_{1}+Y_{1} u+c_{i}X _ 1 + Y _ 1 ¤ u + c _ i , where 1¤x¤u+ci1¥subscript1 x u+c_{i}1 ¤ x ¤ u + c _ i , 1¤y¤u+cix1¦subscript¥1 y u+c_{i}-x1 ¤ y ¤ u + c _ i - x. Allowing x=0¥0x=0x = 0 when 1¤y¤u+cix1¦subscript¥1 y u+c_{i}-x1 ¤ y ¤ u + c _ i - x does not hurt as we have made it clear early in Section 2 that fXY(0,y)=0subscript“‹0¦0f_{XY}(0,y)=0f _ X Y ( 0 , y ) = 0 for y 0¦0y 0. The third term represents the scenario that there is a small main claim within the first time period paired with a large by-claim satisfying X1+Y1¥u+ci+1subscript‹1subscript1subscript1X_{1}+Y_{1} u+c_{i}+1X _ 1 + Y _ 1 ¥ u + c _ i + 1, where 1¤x¤u+ci1¥subscript1 x u+c_{i}1 ¤ x ¤ u + c _ i , y¥u+cix+1¦subscript¥1y u+c_{i}-x+1y ¥ u + c _ i - x + 1. The two possibilities that the by-claim is settled within this time period or delayed to the next time period are considered separately, where the non-delay case leads to ruin at time 1. Considering the above scenarios and applying the given rule of premium adjustments for the second time period yield Note that we have Uk²subscriptsuperscript²U^{ ^ ² _ k in the fourth term of both equalities because there is a delayed by-claim Y1subscript1Y_{1}Y _ 1 at the beginning of the second time period. From (2.3) and (2.5), we have One can also verify that ¡¡ Remark. From the definition of Î¾y(n+y)subscript¦¦ _ y ( n + y ), one can show that Also, x=u+ci+1fX(x)=1x=0u+cifX(x)superscriptsubscript¥subscript1subscript“‹¥1superscriptsubscript¥0subscriptsubscript“‹¥ _ x = u + c _ i + 1 ^ f _ X ( x ) = 1 - _ x = 0 ^ u + c _ i f _ X ( x ). Therefore, in the recursive formula given in Theorem 1, there is only one infinite summation left which requires extra attention when use it for computational purposes. To use the recursive formula obtained in Theorem 1, we need to find a way to determine i²(0;z,n)subscriptsuperscript“²0§ ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT _ i ( 0 ; z , n ), 0<z¤u+ci0§subscript0<z u+c_{i}0 < z ¤ u + c _ i , n•+superscript•n N ^ + . The finite-time ruin probability with premiums adjusted according to aggregate reported claims and an up-front delayed by-claim z§zz satisfies the following recursive formula, for 0<z¤u+ci0§subscript0<z u+c_{i}0 < z ¤ u + c _ i and n•+superscript•n N ^ + , where i²(0;z,1)=(1q)y=1Î¾y(ciz)+x=ciz+1fX(x)subscriptsuperscript“²0§11superscriptsubscript¦1subscript¦subscript§superscriptsubscript¥subscript§1subscript“‹¥ )+ ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT _ i ( 0 ; z , 1 ) = ( 1 - q ) _ y = 1 ^ Î¾ _ y ( c _ i - z ) + _ x = c _ i - z + 1 ^ f _ X ( x ). Proof. For u<z¤u+ci§subscriptu<z u+c_{i}u < z ¤ u + c _ i , the same method in the proof of Theorem 1 can be used to derive a recursive formula for i²(u;z,n+1)subscriptsuperscript“²§1 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT _ i ( u ; z , n + 1 ). Using (3.10), (3.12) can be obtained by replacing i²(u;z,n)subscriptsuperscript“²§ ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT _ i ( u ; z , n ) with i²(0;zu,n)subscriptsuperscript“²0§ ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT _ i ( 0 ; z - u , n ) in the formula. ¡¡ Previously, we have discussed the first case of varying premiums based on the total reported claims. In contrast, we shall consider another case where for k•+superscript•k N ^ + , the premium Ck+1subscript¶1C_{k+1}C _ k + 1 is determined by Cksubscript¶C_{k}C _ k and the total settled claims in time period kkk, i.e. Sksubscript†S_{k}S _ k . Other model assumptions are the same as the previous case. It is worth noting that in this case of premium correction, the underlying Markov process governing the periodic premium levels is not time-homogeneous anymore since the distribution of aggregate settled claims Stsubscript†¡S_{t}S _ t takes different forms over time, see (2.2) for details. Further, Lemma 1 does not hold in this case either as having a by-claim delayed from previous time period or not does matter when determining future premiums. However, we can still follow the main idea in previous section to obtain the following main result. Given initial surplus u¥00u 0u ¥ 0 and initial premium level cisubscriptc_{i}c _ i , i““i caligraphic_L, the finite-time ruin probability with premiums adjusted according to aggregate settled claims without the up-front delayed by-claim satisfies the following recursive formula, for n•+superscript•n N ^ + , where i(u,1)=x=u+ci+1fX(x)+(1q)y=1Î¾y(u+ci)subscript“1superscriptsubscript¥subscript1subscript“‹¥1superscriptsubscript¦1subscript¦subscript y=1}^{ _ i ( u , 1 ) = _ x = u + c _ i + 1 ^ f _ X ( x ) + ( 1 - q ) _ y = 1 ^ Î¾ _ y ( u + c _ i ). Proof. From (2.3), we have The scenarios listed in the third equality are the same as those in the second equality within the proof of Theorem 1. Then we have Note that the rule of premium adjustments applied above is different from the rule proposed in Section 3. Here only the settled claims are counted when analysing the aggregate claims for the premium adjustment purposes. Similar to Theorem 1, one can verify that the result for i(u,1)subscript“1 _ i ( u , 1 ) is just a special case of n=11n=1n = 1. ¡¡ To use the recursive formula obtained in Theorem 2, we need to find a way to determine i²(u;z,n)subscriptsuperscript“²§ ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT _ i ( u ; z , n ), 0<z¤ci0§subscript0<z c_{i}0 < z ¤ c _ i , n•+superscript•n N ^ + . The finite-time ruin probability with premiums adjusted according to aggregate settled claims and an up-front delayed by-claim z§zz satisfies the following recursive formula, for 0<z¤u+ci0§subscript0<z u+c_{i}0 < z ¤ u + c _ i and n•+superscript•n N ^ + , where i²(u;z,1)=x=uz+ci+1fX(x)+(1q)y=1Î¾y(uz+ci)subscriptsuperscript“²§1superscriptsubscript¥§subscript1subscript“‹¥1superscriptsubscript¦1subscript¦§subscript }(x)+(1-q) ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT _ i ( u ; z , 1 ) = _ x = u - z + c _ i + 1 ^ f _ X ( x ) + ( 1 - q ) _ y = 1 ^ Î¾ _ y ( u - z + c _ i ). Proof. Using (4.13), (4.14) can be obtained by adding z§zz into the premium rule function tijsubscript¡t_{ij}t _ i j and replacing uuu by uz§u-zu - z in (4.13). ¡¡ In this section, we shall switch the premium correction trigger from aggregate claim experience to claim frequency experience. We still denote the bonus-malus system by Î”=(“,,i)Î”“ = ( T , c , i ), where i““i caligraphic_L; “={tij(k)}i,j“;k•“subscriptsubscript¡formulae-sequence“• = { t _ i j ( k ) } _ i , j caligraphic_L ; k N denotes a general set of time-homogeneous rules with input kkk being the number of claims. For any k••k N and n•+superscript•n N ^ + , tij(k)=1subscript¡1t_{ij}(k)=1t _ i j ( k ) = 1 if the total number of claims in time period nnn leads to the transition from premium level Cn=cisubscript¶subscriptC_{n}=c_{i}C _ n = c _ i to Cn+1=cjsubscript¶1subscriptC_{n+1}=c_{j}C _ n + 1 = c _ j and tij(k)=0subscript¡0t_{ij}(k)=0t _ i j ( k ) = 0 otherwise. Now we consider the first type of claim frequency, i.e. the total number of reported claims. Let NtXsubscriptsuperscript‹¡N^{X}_{t}N ^ X _ t denote the number of main claims in t¡tt-th time period. According to the assumption in section 2, we have ™(NtX=0)=fX(0)™subscriptsuperscript‹¡0subscript“‹0 ( N ^ X _ t = 0 ) = f _ X ( 0 ) and ™(NtX=1)=1fX(0)™subscriptsuperscript‹¡11subscript“‹0 ( N ^ X _ t = 1 ) = 1 - f _ X ( 0 ). Similarly, the number of by-claims in time period t¡tt is denoted by NtYsubscriptsuperscript¡N^{Y}_{t}N ^ Y _ t , where ™(NtY=0)=fY(0)™subscriptsuperscript¡0subscript“0 ( N ^ Y _ t = 0 ) = f _ Y ( 0 ) and ™(NtY=1)=1fY(0)™subscriptsuperscript¡11subscript“0 ( N ^ Y _ t = 1 ) = 1 - f _ Y ( 0 ). We assume that NtYsubscriptsuperscript¡N^{Y}_{t}N ^ Y _ t is observable at time t¡tt no matter if the settlement of Ytsubscript¡Y_{t}Y _ t will be delayed or not, so the total number of reported claims in time period t¡tt is NtX+NtYsubscriptsuperscript‹¡subscriptsuperscript¡N^{X}_{t}+N^{Y}_{t}N ^ X _ t + N ^ Y _ t . For any time period t¡tt, t•+¡superscript•t N ^ + , there are only three cases of reported number of claims: NtX=0subscriptsuperscript‹¡0N^{X}_{t}=0N ^ X _ t = 0 and NtY=0subscriptsuperscript¡0N^{Y}_{t}=0N ^ Y _ t = 0; NtX=1subscriptsuperscript‹¡1N^{X}_{t}=1N ^ X _ t = 1 and NtY=0subscriptsuperscript¡0N^{Y}_{t}=0N ^ Y _ t = 0; NtX=1subscriptsuperscript‹¡1N^{X}_{t}=1N ^ X _ t = 1 and NtY=1subscriptsuperscript¡1N^{Y}_{t}=1N ^ Y _ t = 1. Using a similar method as the one used in Section 3, we obtain the following main result: Given initial surplus u¥00u 0u ¥ 0 and initial premium level cisubscriptc_{i}c _ i , i““i caligraphic_L, the finite-time ruin probability with premiums adjusted according to reported number of claims without the up-front delayed by-claim satisfies the following recursive formula, for n•+superscript•n N ^ + , where i(u,1)=(1q)y=1Î¾y(u+ci)+x=u+ci+1fX(x)subscript“11superscriptsubscript¦1subscript¦subscriptsuperscriptsubscript¥subscript1subscript“‹¥ _ i ( u , 1 ) = ( 1 - q ) _ y = 1 ^ Î¾ _ y ( u + c _ i ) + _ x = u + c _ i + 1 ^ f _ X ( x ). Proof. It is not hard to see that the formula (5.15) can be obtained by replacing the premium rule tijsubscript¡t_{ij}t _ i j in (3.11) with the new version defined at the beginning of this section. Since there are only three cases of total number of reported claims in each time period, one can get (5.15) straightforwardly. ¡¡ The finite-time ruin probability with premiums adjusted according to reported claims number and an up-front delayed by-claim z§zz satisfies the following recursive formula, for 0<z¤ci0§subscript0<z c_{i}0 < z ¤ c _ i and n•+superscript•n N ^ + , where i²(0;z,1)=(1q)y=1Î¾y(ciz)+x=ciz+1fX(x)subscriptsuperscript“²0§11superscriptsubscript¦1subscript¦subscript§superscriptsubscript¥subscript§1subscript“‹¥ c_{i}-z)}+ ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT _ i ( 0 ; z , 1 ) = ( 1 - q ) _ y = 1 ^ Î¾ _ y ( c _ i - z ) + _ x = c _ i - z + 1 ^ f _ X ( x ). Proof. Again, the formula (5.16) can be obtained by plugging in the reported claims number in the new premium rule function tijsubscript¡t_{ij}t _ i j that replaces the one in (3.12). ¡¡ In this section, we consider the second type of claim frequency, i.e. the total number of settled claims in a given time period. For time period t•+¡superscript•t N ^ + , let MtXsubscriptsuperscript‹¡M^{X}_{t}M ^ X _ t denote the number of main claims in this time period; let MtYsubscriptsuperscript¡M^{Y}_{t}M ^ Y _ t denote the number of settled by-claims incurred in the current period; let MtZsubscriptsuperscript¡M^{Z}_{t}M ^ Z _ t denote the number of settled by-claims incurred in precious time period. Based on the assumptions in Section 2, we know that all of these count random variables can only take a value either 0 or 1. The values of MtZsubscriptsuperscript¡M^{Z}_{t}M ^ Z _ t and MtXsubscriptsuperscript‹¡M^{X}_{t}M ^ X _ t have unique interpretations, but the value 0 for MtYsubscriptsuperscript¡M^{Y}_{t}M ^ Y _ t leads to multiple possibilities. To be more specific, MtY=0subscriptsuperscript¡0M^{Y}_{t}=0M ^ Y _ t = 0 means either no by-claim incurred in time period t¡tt or the settlement of the incurred by-claim is delayed to next time period. This implies that Mt+1Z=1subscriptsuperscript¡11M^{Z}_{t+1}=1M ^ Z _ t + 1 = 1 gives MtY=0subscriptsuperscript¡0M^{Y}_{t}=0M ^ Y _ t = 0, but not vice versa. Here we assume that the premium Ct+1subscript¶¡1C_{t+1}C _ t + 1 , t•+¡superscript•t N ^ + , is determined according to the total number of settled claims in time period t¡tt, i.e. MtX+MtY+MtZsubscriptsuperscript‹¡subscriptsuperscript¡subscriptsuperscript¡M^{X}_{t}+M^{Y}_{t}+M^{Z}_{t}M ^ X _ t + M ^ Y _ t + M ^ Z _ t , which can take an integer value from 0 to 3: MtZ=0subscriptsuperscript¡0M^{Z}_{t}=0M ^ Z _ t = 0, MtX=0subscriptsuperscript‹¡0M^{X}_{t}=0M ^ X _ t = 0, MtY=0subscriptsuperscript¡0M^{Y}_{t}=0M ^ Y _ t = 0 MtX+MtY+MtZ=0subscriptsuperscript‹¡subscriptsuperscript¡subscriptsuperscript¡0M^{X}_{t}+M^{Y}_{t}+M^{Z}_{t}=0M ^ X _ t + M ^ Y _ t + M ^ Z _ t = 0; MtZ=0subscriptsuperscript¡0M^{Z}_{t}=0M ^ Z _ t = 0, MtX=1subscriptsuperscript‹¡1M^{X}_{t}=1M ^ X _ t = 1, MtY=0subscriptsuperscript¡0M^{Y}_{t}=0M ^ Y _ t = 0 MtX+MtY+MtZ=1subscriptsuperscript‹¡subscriptsuperscript¡subscriptsuperscript¡1M^{X}_{t}+M^{Y}_{t}+M^{Z}_{t}=1M ^ X _ t + M ^ Y _ t + M ^ Z _ t = 1; MtZ=0subscriptsuperscript¡0M^{Z}_{t}=0M ^ Z _ t = 0, MtX=1subscriptsuperscript‹¡1M^{X}_{t}=1M ^ X _ t = 1, MtY=1subscriptsuperscript¡1M^{Y}_{t}=1M ^ Y _ t = 1 MtX+MtY+MtZ=2subscriptsuperscript‹¡subscriptsuperscript¡subscriptsuperscript¡2M^{X}_{t}+M^{Y}_{t}+M^{Z}_{t}=2M ^ X _ t + M ^ Y _ t + M ^ Z _ t = 2; MtZ=1subscriptsuperscript¡1M^{Z}_{t}=1M ^ Z _ t = 1, MtX=0subscriptsuperscript‹¡0M^{X}_{t}=0M ^ X _ t = 0, MtY=0subscriptsuperscript¡0M^{Y}_{t}=0M ^ Y _ t = 0 MtX+MtY+MtZ=1subscriptsuperscript‹¡subscriptsuperscript¡subscriptsuperscript¡1M^{X}_{t}+M^{Y}_{t}+M^{Z}_{t}=1M ^ X _ t + M ^ Y _ t + M ^ Z _ t = 1; MtZ=1subscriptsuperscript¡1M^{Z}_{t}=1M ^ Z _ t = 1, MtX=1subscriptsuperscript‹¡1M^{X}_{t}=1M ^ X _ t = 1, MtY=0subscriptsuperscript¡0M^{Y}_{t}=0M ^ Y _ t = 0 MtX+MtY+MtZ=2subscriptsuperscript‹¡subscriptsuperscript¡subscriptsuperscript¡2M^{X}_{t}+M^{Y}_{t}+M^{Z}_{t}=2M ^ X _ t + M ^ Y _ t + M ^ Z _ t = 2; MtZ=1subscriptsuperscript¡1M^{Z}_{t}=1M ^ Z _ t = 1, MtX=1subscriptsuperscript‹¡1M^{X}_{t}=1M ^ X _ t = 1, MtY=1subscriptsuperscript¡1M^{Y}_{t}=1M ^ Y _ t = 1 MtX+MtY+MtZ=3subscriptsuperscript‹¡subscriptsuperscript¡subscriptsuperscript¡3M^{X}_{t}+M^{Y}_{t}+M^{Z}_{t}=3M ^ X _ t + M ^ Y _ t + M ^ Z _ t = 3. Similar to Section 4, there is lack of time-homogeneity in the underlying Markov process for premiums. Taking into account the complications illustrated above on the total number of settled claims, we obtain the following result for the finite-time ruin probabilities. Given initial surplus u¥00u 0u ¥ 0 and initial premium level cisubscriptc_{i}c _ i , i““i caligraphic_L, the finite-time ruin probability with premiums adjusted according to settled claims number without the up-front delayed by-claim satisfies the following recursive formula, for n•+superscript•n N ^ + , where i(u,1)=x=u+ci+1fX(x)+(1q)y=1Î¾y(u+ci)subscript“1superscriptsubscript¥subscript1subscript“‹¥1superscriptsubscript¦1subscript¦subscript y=1}^{ _ i ( u , 1 ) = _ x = u + c _ i + 1 ^ f _ X ( x ) + ( 1 - q ) _ y = 1 ^ Î¾ _ y ( u + c _ i ). Proof. Similar to the proof of Theorem 3, (6.17) can be obtained by replacing the function tijsubscript¡t_{ij}t _ i j in (4.13) with the new one defined at the beginning of Section 5. Then we plug in the total number of settled claims in tijsubscript¡t_{ij}t _ i j according to the values of X‹XX and YYY. ¡¡ The finite-time ruin probability with premiums adjusted according to settled claims number and an up-front delayed by-claim z§zz satisfies the following recursive formula, for 0<z¤u+ci0§subscript0<z u+c_{i}0 < z ¤ u + c _ i and n•+superscript•n N ^ + , where i²(u;z,1)=x=uz+ci+1fX(x)+(1q)y=1Î¾y(uz+ci)subscriptsuperscript“²§1superscriptsubscript¥§subscript1subscript“‹¥1superscriptsubscript¦1subscript¦§subscript }(x)+(1-q) ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT _ i ( u ; z , 1 ) = _ x = u - z + c _ i + 1 ^ f _ X ( x ) + ( 1 - q ) _ y = 1 ^ Î¾ _ y ( u - z + c _ i ). Proof. Using (6.17), (6.18) can be obtained by adding 1111 into the premium rule function tijsubscript¡t_{ij}t _ i j and replacing uuu by uz§u-zu - z in (6.17). ¡¡ In this section we shall provide some numerical examples to illustrate the theoretical results obtained under the previously discussed four premium adjustment principles and to further study the commonality and dissimilarity of the four principles. Since we have been focusing on the finite-time ruin probabilities in this paper, we shall adopt the finite-time ruin probabilities with a fixed term (say 20) as the proxy to achieve the aforementioned goals. The possible behaviours of finite-time ruin probabilities under each principle when the term changes are not covered here, mainly due to the significantly increased computational costs involved in the completion of the task. The first numerical example we give in this section applies the premium correction principle allowing premiums to be adjusted according to aggregate reported claims. As mentioned previously in section 2.1, the aggregate claims are assumed to be reported at the end of each policy period even when the settlement of by-claims is delayed. We shall examine three hypothetical scenarios for the degree of correlation between the main claim X‹XX and the by-claim YYY in this example: low correlation, moderate correlation and high correlation. For each scenario, two cases of claim settlement delay are considered: q=0.20.2q=0.2q = 0.2 or q=0.80.8q=0.8q = 0.8. We propose the following joint distributions of X‹XX and YYY: high correlation case: where ”¼(X)”¼‹ ( X )=5, ”¼(Y)”¼ ( Y )=5 and the correlation coefficient XYsubscript‹ _ X Y =1; low correlation case: where ”¼(X)”¼‹ ( X )=5, ”¼(Y)”¼ ( Y )=5 and XYsubscript‹ _ X Y =0.1443; moderate correlation case: we let where ”¼(X)”¼‹ ( X )=5, ”¼(Y)”¼ ( Y )=5 and so XYsubscript‹ _ X Y =0.5401. According to the above assumptions, we can see that X‹XX follows the same marginal geometric distribution in all three cases, i.e. fX(x)=(16)(56)xsubscript“‹¥16superscript56¥f_{X}(x)= _ X ( x ) = ( / 1 6 ) ( / 5 6 ) ^ x , x¥0¥0x 0x ¥ 0. However, the three marginal distributions of YYY differ from each other, which are listed below, for y¥0¦0y 0y ¥ 0, where I{y=0}subscript¼¦0I_{ _ { y = 0 } is an indicator function taking 1 when y=0¦0y=0y = 0 and 0 otherwise. The set of premium levels is assumed to be c={c1,¦,c5}={11,12,14,16,18}csubscript1¦subscript51112141618 = { c _ 1 , ¦ , c _ 5 } = { 11 , 12 , 14 , 16 , 18 } and the initial premium of new policyholders C1subscript¶1C_{1}C _ 1 is c3subscript3c_{3}c _ 3 that is 140% of the expected aggregate reported claims ”¼(X+Y)”¼‹ ( X + Y ) (i.e. a safety loading factor of 40%). Under our assumption, the premium levels range from 110%percent110110 % to 180%percent180180 % of the expected aggregate reported claims. We propose the following rules of premium adjustment: If the reported aggregate claims in the current period is no more than 3, then the premium level for the next period will move to the lower premium level or stay in the lowest one, i.e. for s¤3 3s 3s ¤ 3, t11(s)=1,ti,i1(s)=1,i¥2formulae-sequencesubscript¡11 1formulae-sequencesubscript¡1 12t_{11}(s)=1,t_{i,i-1}(s)=1,i 2t _ 11 ( s ) = 1 , t _ i , i - 1 ( s ) = 1 , i ¥ 2; If the reported aggregate claims in the current period is more than 3 but no more than 14, then the premium level for the next period will remain in the current level, i.e. for 3<s¤143 143<s 143 < s ¤ 14, tii(s)=1,1¤i¤5formulae-sequencesubscript¡ 115t_{ii}(s)=1,1 i 5t _ i i ( s ) = 1 , 1 ¤ i ¤ 5; If the reported aggregate claims in the current period is more than 14, then the premium level for the next period will move to the higher premium level or stay in the highest one, i.e. for s>14 14s>14s > 14, t55(s)=1,ti,i+1(s)=1,i¤4formulae-sequencesubscript¡55 1formulae-sequencesubscript¡1 14t_{55}(s)=1,t_{i,i+1}(s)=1,i 4t _ 55 ( s ) = 1 , t _ i , i + 1 ( s ) = 1 , i ¤ 4. According to the above transition rules, we can calculate the transition probabilities among the premium levels based on (3.6). Let THsubscriptsuperscript» ^ H _ T , TMsubscriptsuperscript ^ M _ T and TLsubscriptsuperscript¿ ^ L _ T denote the transition matrix in each of the above correlation cases respectively, then we have The corresponding long-term stationary distribution of the premium levels are: The long-term expected premium per time period is 13.26, 13.65 and 14.07 in the high, moderate, and low correlation scenario, respectively. Using (3.11) and (3.12), we calculate 3(u,20)subscript“320 _ 3 ( u , 20 ), 0¤u¤10001000 u 1000 ¤ u ¤ 100, with the initial premium c3subscript3c_{3}c _ 3 and the results are summarised in Table LABEL:tab1 and Figure 1. Note that the notations H1»1H1H 1, M11M1M 1 and L1¿1L1L 1 denote the scenarios of high, moderate and low correlation between X‹XX and YYY when q=0.20.2q=0.2q = 0.2, and the notations H2»2H2H 2, M22M2M 2 and L2¿2L2L 2 correspond to the scenarios when q=0.80.8q=0.8q = 0.8. The first observation, a trivial one, from Table LABEL:tab1 and Figure 1 is that 3(u,20)subscript“320 _ 3 ( u , 20 ) decreases when uuu increases. Moreover, we notice that the correlation level between main claim X‹XX and by-claim YYY does affect the finite-time ruin probability. Under our previous assumptions, after fixing uuu and qqq, the higher is the correlation, the higher is the risk of ruin. Although the same premium adjustment rules are applicable for all three correlation scenarios, the joint distribution of X‹XX and YYY differentiates the transition probabilities among premium levels as well as the stationary distribution of individual premium levels. The previously calculated Hsuperscript‹» ^ H , Msuperscript‹ ^ M and Lsuperscript‹¿ ^ L show that the high correlation case has the highest long-term probability to reach low premium levels and the lowest long-term probability for high premium levels. It implies that in long-run, in scenario H»HH, the insurer is expected to receive less total premium income than the other two scenarios, which results in the highest finite-time ruin probabilities among the three scenarios. Similar arguments can be made to explain the ordering between cases MMM and L¿LL. In addition, the differences, in terms of percentages, among the finite-time ruin probabilities under the three scenarios increase when the initial surplus uuu increases. For example, 3H1(0,20)subscriptsuperscript“»13020 ^ H 1 _ 3 ( 0 , 20 ) is only about 5.4% higher than 3M1(0,20)subscriptsuperscript“13020 ^ M 1 _ 3 ( 0 , 20 ) and around 12.9% higher than 3L1(0,20)subscriptsuperscript“¿13020 ^ L 1 _ 3 ( 0 , 20 ). But 3H1(100,20)subscriptsuperscript“»1310020 ^ H 1 _ 3 ( 100 , 20 ) is about three times 3M1(100,20)subscriptsuperscript“1310020 ^ M 1 _ 3 ( 100 , 20 ) and around 68 times 3L1(100,20)subscriptsuperscript“¿1310020 ^ L 1 _ 3 ( 100 , 20 ). This makes sense because when uuu is small, if ruin occurs then it is more likely to occur within the first few periods. As a result, there is only limited time for the main factors, which vary the finite-time ruin probabilities among these scenarios, to take effect. The same initial premium assumption under all three scenarios also contributed to the small differences in percentage among the finite-time ruin probabilities when uuu is small. On the contrary, when uuu is large, if ruin occurs then ruin is more likely to occur in the long run. The dissimilar premium evolving patterns under the three scenarios have plenty of time to drive the underlying surplus processes to different directions, which lead to divergent finite-time ruin probabilities. Last but not least, it is evident from Figure 1 that with all other factors being the same, an increase in qqq from 0.2 to 0.8 shifted the finite-time ruin probabilities downwards. This is reasonable since when the settlement of by-claims is more likely to be delayed, the insurers can receive more premium income that helps to settle the claims. However, this effect reduces when uuu is larger, because delaying by-claims for one time unit would not make a big difference for the worst cases (i.e. getting bankrupted with a large initial capital). This example examines the premium adjustment principle that was discussed in Section 4. This principle is worth exploring because that, in certain circumstances, the total settled claim amounts might better reflect the claims experiences of policyholders than the total reported claim amounts in a given time window due to the fact that in real practice reported claims come with uncertainties in the scale and timing of the real settlements. Therefore, the reported claims are only initial guesses and may not provide accurate information to represent the policyholders™ historical claim experience. In this paper, for the purpose of simplification, we assumed that the reported and settled by-claims amounts are always equal and the length of delay is always 1. Although these restrictive assumptions are not entirely realistic, they serve as good starting points that could motivate more realistic models in future studies. We assume the same claim distributions and premium levels as those in previous example, whilst the transition rules of premium levels are modified as: If the settled aggregate claims in the current period is no more than 3, the premium level for the next period will move to the lower premium level or stay in the lowest one; If the settled aggregate claims in the current period is more than 3 but no more than 14, the premium level for the next period will remain in the current premium level; If the settled aggregate claims in the current period is more than 14, the premium level for the next period will move to the higher premium level or stay in the highest one. By the non-homogeneity nature exhibited under the new rules, there is no constant one-step transition matrix among the premium levels anymore. On the contrary, the one-step transition matrix varies over time and depends on the number of by-claims settled in each given time period. However, we can still study 20-period finite-time ruin probabilities using the recursive formulae (4.13) and (4.14). The results are given in Table LABEL:tab2 and Figure 2. We adopt the same notation to denote the scenarios under consideration. As shown in Table LABEL:tab2 and Figure 2, consistent observations are evident in this aggregate settled claims principle comparing with the aggregate reported claims case. Further, the differences between the two qqq cases in each correlation scenario also behave interestingly differently. In the high correlation scenario, there is a big gap between the two ruin probability curves showing that a high chance of delaying the highly correlated by-claims results in a big reduction in the risk of ruin comparing from the case of low chance of delay. On the contrary, when the correlation between main claims and by-claims is low and uuu is not small, whether delaying the by-claims or not seem not having a significant impact on the finite-time ruin probabilities. A reasonable interpretation is that when the correlation is low, the main difference between the two cases of qqq is that the by-claims settled in each time period are likely to be delayed ones or freshly incurred ones. Since the correlation between the main claims and by-claims is low, the distributions of aggregate settled claims in each period are similar in both cases. Therefore, except the first time period, the surplus process should behave similarly within all remaining time periods in both qqq cases that lead to similar finite-time ruin probabilities. Moreover, we generate comparison results, shown in Figure 3 and Figure 4, regarding 3(u,20)subscript“320 _ 3 ( u , 20 ) in this and the previous numerical examples. The superscripts R…RR and S†SS denote the premium adjustment principle by reported aggregate claims and by settled aggregate claims respectively. As seen in Figure 3, the two premium correction principles lead to marginal differences in the finite-time ruin probabilities in the case of q=0.20.2q=0.2q = 0.2, because when qqq is small, the aggregate reported claims in each period are likely to be the same as the aggregate settled claims. Therefore, the periodic premiums are highly likely to follow the same pattern in both cases, which result in similar finite-time ruin probabilities. On the other hand, according to Figure 4, when q=0.80.8q=0.8q = 0.8 the trends of finite-time ruin probabilities in the two cases differ significantly from one another. However, the differences increase when the correlation between the main claims and by-claims becomes weaker, and they tend to diminish when uuu increases. Moreover, when q=0.80.8q=0.8q = 0.8 the differences among the three correlation scenarios under the aggregate settled claims principle are generally smaller than those in the aggregate reported claims case. A possible interpretation is that when qqq is high, after the first couple of time periods, the aggregate settled claims in each period is highly likely to be the summation of a main claim X‹XX of the current period and a by-claim YYY delayed from the previous period (if any), whilst the aggregate reported claims in each period is a current main claim plus a current by-claim (if any). Due to the independence assumption between main claims and by-claims in different time periods, the within-period correlation between main and by-claims becomes between-period correlation in the aggregate settled claims case, which likely contributes to the above observation. A consistent finding in both qqq cases is that the finite-time ruin probabilities under the aggregated settled claims principle are generally higher than the corresponding ones in the aggregate reported claims case. It implies that if the information regarding reported claims is accurate, then the insurers better adopt the aggregate reported claims principle to adjust their periodic premiums, or they will face a higher insolvency risk otherwise. In the following sections, we shall provide two examples designed to examine the finite-time ruin probabilities with premiums adjustment principles that focus on the claim frequency information. In this example, we assume that the claim distributions and the set of premium levels are the same as the previous examples. The rules of premium corrections are: If the number of reported claims in the current period is 0, then the premium level for the next period will move to the lower premium level or stay in the lowest one; If the number of reported claims in the current period is 1, then the premium level for the next period will remain in the current premium level; If the number of reported claims in the current period is more than 1, then the premium level for the next period will move to the higher premium level or stay in the highest one. Next, we shall explore the impact of the correlation between main claims and by-claims as well as the impact of qqq on the finite-time ruin probabilities. Under the new premium adjustment rules given above, the correlation between the number of main claims NtXsubscriptsuperscript‹¡N^{X}_{t}N ^ X _ t and by-claims NtYsubscriptsuperscript¡N^{Y}_{t}N ^ Y _ t are calculated instead of the correlation between X‹XX and YYY. We find that the correlation between NtXsubscriptsuperscript‹¡N^{X}_{t}N ^ X _ t and NtYsubscriptsuperscript¡N^{Y}_{t}N ^ Y _ t generated by the claims distribution fXYH(x,y)subscriptsuperscript“»‹¥¦f^{H}_{XY}(x,y)f ^ H _ X Y ( x , y ), fXYM(x,y)subscriptsuperscript“‹¥¦f^{M}_{XY}(x,y)f ^ M _ X Y ( x , y ) and fXYL(x,y)subscriptsuperscript“¿‹¥¦f^{L}_{XY}(x,y)f ^ L _ X Y ( x , y ) is NX,NY=1subscriptsuperscript‹superscript1 _ N ^ X , N ^ Y = 1, NX,NY=0.8272subscriptsuperscript‹superscript0.8272 _ N ^ X , N ^ Y = 0.8272 and NX,NY=0.7071subscriptsuperscript‹superscript0.7071 _ N ^ X , N ^ Y = 0.7071, respectively. These surprisingly high correlations between number of claims are rooted in the model assumptions made in Section 2, i.e. one main claim generates at most one by-claim and no main claim means no by-claim. Similar to the example in Section 7.1, we can calculate the transition matrix among premium levels as follows: The corresponding long-term stationary distribution of the premium levels are: The long-term expected premiums in each correlation scenario is 17.50, 17.46 and 17.40 in the H, M and L scenario, respectively. It is worth noting that given the very different joint distributions of X‹XX and YYY in the three correlation scenarios, the corresponding long-term expected premiums are very similar under the current premium correction principle. By (5.15) and (5.16), we obtain results for 3(u,20)subscript“320 _ 3 ( u , 20 ) that are summarized in Table LABEL:tab3 and Figure 5. The notations in Table LABEL:tab3 are defined in the same way as in Table LABEL:tab1 and LABEL:tab2. Again, Table LABEL:tab3 and Figure 5 show us some similar trends to those shown in Table LABEL:tab1 & LABEL:tab2 and Figure 1 & 2. First, 3(u,20)subscript“320 _ 3 ( u , 20 ) decreases when uuu increases and the correlation level between NXsuperscript‹N^{X}N ^ X and NYsuperscriptN^{Y}N ^ Y is positively related to the ruin probabilities. When fixing uuu and qqq, the higher the correlation, the higher is the ruin probability. Additionally, the decrease in qqq from 0.8 to 0.2 also causes a lift in the finite time ruin probabilities in all correlation scenarios. There are two inconsistencies between this example and the previous ones: Firstly, the scales of difference in X,Ysubscript‹ _ X , Y and 3(u,20)subscript“320 _ 3 ( u , 20 ) among all correlation scenarios in Section 7.1 and 7.2 are larger than the corresponding differences in this example. An interpretation is that, as given at the beginning of this section, the differences among the three NX,NYsubscriptsuperscript‹superscript _ N ^ X , N ^ Y values are much smaller than the differences among the three X,Ysubscript‹ _ X , Y values, which makes the three correlation scenarios less distinct from one another. Secondly, the relationship between NX,NYsubscriptsuperscript‹superscript _ N ^ X , N ^ Y and the long-term expected premium in this example is opposite to that in Section 7.1. To be more specific, in Section 7.1, lower X,Ysubscript‹ _ X , Y leads to higher long-term expected premiums, whereas in this example, lower NX,NYsubscriptsuperscript‹superscript _ N ^ X , N ^ Y gives lower long-term expected premiums. A likely justification of this difference is the change of premium correction objectives from aggregate claim experience to claim frequencies. In our last numerical example, we shall duplicate the model assumptions but change the premiums adjustment rules following the settled claims number premium principle. The transition rules of premiums are: If the number of settled claims in the current period is 0, then the premium level for the next period will move to the lower premium level or stay in the lowest one; If the number of settled claims in the current period is 1, then the premium level for the next period will remain in the current premium level; If the number of settled claims in the current period is more than 1, the premium level for the next period will move to the higher premium level or stay in the highest one. We use (6.17) and (6.18) to calculate 3(u,20)subscript“320 _ 3 ( u , 20 ) and the results are summarised in Table LABEL:tab4 and Figure 6, adopting the same notations. Table LABEL:tab4 and Figure 6 show very similar trends to our findings from Table LABEL:tab3 and Figure 5. Again, we generate two comparison graphs, Figure 7 and Figure 8, between the two claim frequency premium principles for q=0.20.2q=0.2q = 0.2 and q=0.80.8q=0.8q = 0.8 respectively. The superscript R…RR denotes the reported claims number principle and S†SS denotes the settled claims number one. From Figure 7 and Figure 8, we can see that when q=0.20.2q=0.2q = 0.2, the finite-time ruin probabilities in this example is slightly higher than the results in section 7.3 for fixed uuu and the correlation level. On the other hand, when q=0.80.8q=0.8q = 0.8, the gaps between the finite-time ruin probabilities of this example and those of 7.3 are larger. This is because when q=0.20.2q=0.2q = 0.2, the by-claim settlements are unlikely to be delayed. Therefore, it is more likely that both main claims and their associated by-claims to be settled in the same time periods, which makes the reported claims number principle and the settled claims number one to work similarly. On the contrary, when q=0.80.8q=0.8q = 0.8, the settled claims number in the first time period is likely to be one since there is no up-front delayed by-claim, while the by-claim™s settlement (if any) is likely to be delayed. Therefore, the settled claims number principle would determine the second premium according to the number of main claims in period one, whereas both the number of main claims and by-claims in period one will be used by the reported claims number principle. As a result, it is likely that the second premium under the reported claims number principle will be higher than the one under the settled claims number principle, which varies the whole sequence of future premiums and results in lower ruin probabilities in the former case. In this paper, we studied a discrete-time risk model with claim-dependent premiums and time-delayed by-claims. Our main goal is to evaluate the impact of the correlation between the main claims and by-claims and the probability of delaying by-claim settlements on the finite-time ruin probabilities under the proposed premium adjustment principles: the aggregate reported claims principle, the aggregate settled claims principle, the reported claims number principle and the settled claims number principle. Under certain assumptions, we found in our numerical studies that a higher probability of delaying the by-claim settlements would result in lower finite-time ruin probabilities. Moreover, the higher correlation between the main claims and by-claims also leads to higher finite-time ruin probabilities. Lastly, the premium adjustment principles based on settled claims experience (aggregate settled claims or settled claims number) account for higher finite-time ruin probabilities, compared with the principles based on the reported claims experience, given all other factors are the same. This difference is more remarkable when the probability of by-claim delays is high. According to these main findings in our study, the insurers should remain on high alert if a high correlation between the main claims and their associated by-claims is evident or the chance of getting delays in claim settlements is low because both situations could lead to increased insolvency risk. Further, the premium adjustment principles based on the reported claims experience could be a safer choice than the principles based on settled claims experience, especially in the high probability of delayed by-claim settlement cases. However, there are some limitations in this study that could be addressed in future research. Firstly, the numerical results of this study only assumed a positive correlation between the main claims and by-claims, but in real life, the correlation can also be negative. Secondly, this paper assumed that by-claim settlements could only be delayed by one time period, which is not realistic in real practice. As an extension, a multiple-period delay could be taken into consideration. Finally, our study assumed that there were at most one main claim and one by-claim incurring in each period and the settled claim amounts are always equal to the reported ones. Some more realistic models allowing general main claim and/or by-claim counts, as well as unequal amounts in reporting and settlement might be worth studying in future research.",
        "keywords": ""
    },
    {
        "id": 14,
        "title": "Handling Numeric Expressions in Automatic Speech Recognition",
        "abstract": "AbstractThis paper addresses the problem of correctly formatting numeric expressions in automatic speech recognition (ASR) transcripts. This is challenging since the expected transcript format depends on the context, e.g., 1945 (year) vs. 19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize and format numeric expression, such as years, timestamps, currency amounts, and quantities. For the end-to-end approach we employed a data generation strategy using a large language model (LLM) together with a text to speech (TTS) model to generate adaptation data. The results on our test dataset show that while approaches based on LLMs perform well on recognizing formatted numeric expressions, adapted end-to-end models offer competitive performance with the advantage of lower latency and inference cost.",
        "corpus": "This paper addresses the problem of correctly formatting numeric expressions in automatic speech recognition (ASR) transcripts. This is challenging since the expected transcript format depends on the context, e.g., 1945 (year) vs. 19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize and format numeric expression, such as years, timestamps, currency amounts, and quantities. For the end-to-end approach we employed a data generation strategy using a large language model (LLM) together with a text to speech (TTS) model to generate adaptation data. The results on our test dataset show that while approaches based on LLMs perform well on recognizing formatted numeric expressions, adapted end-to-end models offer competitive performance with the advantage of lower latency and inference cost. Index Terms: numeric expression formatting, automatic speech recognition. In the last decade, ASR systems improved tremendously in terms of word error rate (WER) due to more data, more computation power and better architectures [1, 2, 3]. These systems are normally trained with labeled ASR data, i.e., human transcribed speech or human correct automatically transcribed speech. The way how numeric expressions are transcribed - using numeric literals, e.g., 1945, or number words, e.g., nineteen forty-five - can vary between different datasets of sometimes even within a dataset. Furthermore, dependent on the usage of the ASR system, different transcript formats might be preferred. For example when a video conference call is automatically subtitled using an ASR system, the readers might prefer numeric literals since they are shorter and easier to read. On the other hand, transcripts of an ASR system containing numeric expressions should be formatted dependent on the context the numeric expressions occur in. For example the number word nineteen forty-five should be formatted as 1945 if it represents a year, as 19:45 if it represents a timestamp, as 19.45. Third, we prompted the LLM to convert the number words to numeric literals in the wanted format (see table 1). This is done by a prompt like Convert the {numeric expression type} in the sentences to numeric literals. The output of this step is used a labels for the utterances. To get a high quality dataset we applied some filtering using simple rules, e.g., output sentences of the third step not containing numeric literals were ignored. The created data was then split in training, development and test sets. We noticed that the numeric expressions created by the LLM sometimes repeated. Therefore, we split the data such that the numeric expressions contained in the three sets were chosen to be pairwise disjoint (see table 2). Then, the test set was read by human annotators to collect real audio samples. We noticed that for the end-to-end approach (see section 4) the performance on the timestamps was worse than the cascaded approaches. Therefore, we evaluated if more data generation could help and created more training data (Training-larger) containing timestamps with gpt-4o. This is done by a prompt like Generate a [German (optional)] sentence containing the timestamp {timestamp} written down using number words. For {timestamp} we iterate over many possibilities, e.g. for English one oclock, quarter past one, half past one, quarter to one, two minutes past one, two minutes to one. For German we use equivalent translations. The seconds and third step of the data generation are the same as before. To evaluate the general performance of our model, we report the WER on the Common voice [13] test sets in English and German. We filtered the test sets by excluding utterances containing numeric expressions because the numeric expressions contained in the labels are written down using number words and we tuned our models to output numeric literals. The English and German test sets each contain 2,000 utterances and 3.3 hours of audio. We compare cascaded and end-to-end approaches for numeric expression formatting (see figure 2). For the cascaded approach we use a trained ASR model and reformat the output using a text segmentation model. For the end-to-end approach we adapt the trained ASR model by fine-tuning on the training set of our numeric expressions dataset (see section 3.1). We use Whisper [3] (whisper-large-v2) as our baseline ASR model and for the text segmentation model we compare using a mbart-based model [14] (mbart-large-50) and LLMs. We adapted the pretrained mbart model in two steps since we only have limited data for the second step. First, we fine-tuned it to predict the transcript labels of the Common voice training sets (excluding utterances containing numeric expressions similar to the test sets) given the ASR hypothesis generated by our baseline ASR model. This model we denote by mbart baseline. Second, we fine-tuned the model on the numeric expressions dataset to format numeric expressions correctly. This is done by using the sentences where numeric expressions are written down as number words as input and the corresponding sentences where numeric expressions are written down as numeric literals as labels. This model we denote by mbart numeric expressions. For both steps we froze the embedding of the model since this yielded better performance than not freezing it. For the LLM we use GPT3.5 (gpt-3.5-turbo) or GPT 4 [15] (gpt4-turbo and gpt-4o) with in-context learning using one example for each numeric expression type (9 examples in total). The results can be seen in table 3 (WER) and table 4 (accuracy of the different numeric expression types). We see that ASR + mbart baseline slightly improves the WER on the Commonvoice test sets due to the learned correction of the ASR hypothesis. However, the performance (WER and accuracy) on the numeric expressions test sets heavily decreased since the model was not trained to predict numeric literals. The model ASR + mbart numeric expressions performs better and outperforms the ASR only model on the numeric expressions test sets, while there is not much difference on the Common voice test sets. However, the model struggles to format the timestamps (and currency amounts) correctly, e.g., the ASR hypothesis ”The library opens at 10 oclock, but its best to arrive early.” is converted to ”The library opens at 17:00, but its best to arrive early.” This is probably due to the limited amount of numeric expressions data. Using more data did help a bit but the accuracy on e.g. timestamps is still less than 40%. Using a LLM as text segmentation model sometimes (gpt3.5-turbo: 9.7%, gpt4-turbo: 1.4%, gpt-4o: 2.7%) does not follow the prompt, e.g., when the input sentence is a question, it is answered. This leads to a completely different transcript and increases in the WER. To circumvent this problem we compute the WER between input and output of the LLM and if the WER is larger than a threshold (0.5) we ignore to LLM output and return the input instead. With this, the LLMs clearly outperform the mbart-based model both in terms of WER and accuracy. The advantage of the LLMs is that they got trained on a lot more data. Most errors are caused by the LLM not following the prompt, e.g., in the sentence ”The bus leaves at five past seven.” the timestamps is not changed to 7:05. Furthermore, the most recently published LLMs perform better. Using an LLM which follows the prompt better would probably yield a bit better scores. It is quite expensive to reformat each hypothesis using an LLM (≈ $15 for evaluating the ASR + gpt-4o approach on the 4.000 Common voice test sets sentences), especially if the goal is to provide the transcription to lots of customers. Therefore, we experimented adapting the ASR model end-to-end. The results show similar WER performance as the LLM-based approaches, however the WER on the German numeric expressions test set is a bit better. The improvement is due to the fact that for most of the timestamp data the baseline ASR model outputs e.g. for an audio containing ”Ich habe bis fünfzehn Uhr fünfundvierzig Zeit.” a transcript ”Ich habe bis 15.45 Uhr Zeit”. The conversion by the LLM does not remove the ”Uhr” which is counted as an error. The fine-tuned ASR model does not output this ”Uhr”. The accuracy of the fine-tuned ASR model with more data on the numeric expressions is better than the approaches ASR + gpt3.5-turbo / gpt4-turbo and only a bit worse than ASR + gpt-4o. The largest difference is on the timestamp numeric expressions. For example the audio containing ”The deadline is at thirty minutes past three.” was transcribed ”The deadline is 03:30.” While this could be correct, a deadline is more likely not to be within the night and the better world knowledge helps the LLMs here to output ”15:30”. With our data generation strategy it is easy to add more formatting rules, e.g., new currency symbols, by performing more augmenting data and adapting the model. The main limitation for the cascaded approach using an LLM is the ability of the LLM to follow the prompt correctly. This is expected to be handled even better for newer LLMs getting trained. For the fine-tuned ASR model the limitation is getting diverse data containing suitable numeric expression formatting. We also tried adapting the ASR model using batch weighting [16] and/or a factorization-based approach [2] together with the common voice training dataset. While the performance on the Common voice test sets improved, which is expected since the training and test datasets are more similar, the performance on the numeric expression formatting was slightly worse. Furthermore, we tried freezing the encoder or only adapting the final projection layer during the adaptation with the numeric expressions data. For both, the performance on the numeric expressions test data was slightly worse compared to not freezing any weights. In this paper, we tackled the problem of correctly formatting numeric expressions in ASR transcripts. Our experiments revealed that LLMs, particularly the latest models, deliver strong performance in recognizing and formatting numeric expressions. On the other hand, end-to-end models adapted with synthetic ASR data provide competitive performance. This research was supported in part by a grant from Zoom Video Communications, Inc. The authors gratefully acknowledge the support.",
        "keywords": ""
    },
    {
        "id": 15,
        "title": "Evaluating Explainable AI Methods in Deep Learning Models for Early Detection of Cerebral Palsy",
        "abstract": "AbstractEarly detection of Cerebral Palsy (CP) is crucial for effective intervention and monitoring. This paper tests the reliability and applicability of Explainable AI (XAI) methods using a deep learning method that predicts CP by analyzing skeletal data extracted from video recordings of infant movements. Specifically, we use XAI evaluation metrics ” namely faithfulness and stability ” to quantitatively assess the reliability of Class Activation Mapping (CAM) and Gradient-weighted Class Activation Mapping (Grad-CAM) in this specific medical application. We utilize a unique dataset of infant movements and apply skeleton data perturbations without distorting the original dynamics of the infant movements. Our CP prediction model utilizes an ensemble approach, so we evaluate the XAI metrics performances for both the overall ensemble and the individual models. Our findings indicate that both XAI methods effectively identify key body points influencing CP predictions and that the explanations are robust against minor data perturbations. Grad-CAM significantly outperforms CAM in the RISv metric, which measures stability in terms of velocity. In contrast, CAM performs better in the RISb metric, which relates to bone stability, and the RRS metric, which assesses internal representation robustness. Individual models within the ensemble show varied results, and neither CAM nor Grad-CAM consistently outperform the other, with the ensemble approach providing a representation of outcomes from its constituent models. Both CAM and Grad-CAM also perform significantly better than random attribution, supporting the robustness of these XAI methods. Our work demonstrates that XAI methods can offer reliable and stable explanations for CP prediction models. Future studies should further investigate how the explanations can enhance our understanding of specific movement patterns characterizing healthy and pathological development.",
        "corpus": "Early detection of Cerebral Palsy (CP) is crucial for effective intervention and monitoring. This paper tests the reliability and applicability of Explainable AI (XAI) methods using a deep learning method that predicts CP by analyzing skeletal data extracted from video recordings of infant movements. Specifically, we use XAI evaluation metrics ” namely faithfulness and stability ” to quantitatively assess the reliability of Class Activation Mapping (CAM) and Gradient-weighted Class Activation Mapping (Grad-CAM) in this specific medical application. We utilize a unique dataset of infant movements and apply skeleton data perturbations without distorting the original dynamics of the infant movements. Our CP prediction model utilizes an ensemble approach, so we evaluate the XAI metrics performances for both the overall ensemble and the individual models. Our findings indicate that both XAI methods effectively identify key body points influencing CP predictions and that the explanations are robust against minor data perturbations. Grad-CAM significantly outperforms CAM in the RISv metric, which measures stability in terms of velocity. In contrast, CAM performs better in the RISb metric, which relates to bone stability, and the RRS metric, which assesses internal representation robustness. Individual models within the ensemble show varied results, and neither CAM nor Grad-CAM consistently outperform the other, with the ensemble approach providing a representation of outcomes from its constituent models. Both CAM and Grad-CAM also perform significantly better than random attribution, supporting the robustness of these XAI methods. Our work demonstrates that XAI methods can offer reliable and stable explanations for CP prediction models. Future studies should further investigate how the explanations can enhance our understanding of specific movement patterns characterizing healthy and pathological development. Index Terms” explainable AI, CAM, Grad-CAM, skeleton data, Cerebral Palsy Cerebral Palsy (CP) is the most common motor disability in children, and it is essential to detect it early for effective early intervention and surveillance [1]. Machine learning based technologies are increasingly being explored in the early detection of CP due to its potential for more accurate, accessible, and timely diagnoses. Specifically, deep learning methods have shown great potential in medical diagnostics due to their ability to detect complex patterns in large sets of data. For instance, McCay et al. developed a deep learning framework that classifies infant movements from RGB videos using extracted pose-based features to identify Fidgety Movements (FMs) [2]. Similarly, Groos et al. introduced a method leveraging deep learning to predict CP from skeletal data captured in spontaneous infant movements, validated across a multi-center cohort [3]. Additionally, Zhang et al. designed CP-AGCN, a graph convolutional network (GCN) that uses skeletal data from RGB videos and a frequency-binning module to classify CP risks in infants [4]. Gao et al. implemented a deep learning model to automate early CP detection by analyzing FMs in video sequences [5]. There are several data modalities that can be analyzed for early CP prediction such as in the sensor fusion approach proposed by Kulvicius et al. [6], but this paper focuses on analyzing skeletal data extracted from video recordings via pose estimation, noting its broader applicability to areas such as abnormal gait detection [7], Parkinson™s disease gait assessment [8], fall detection [9], and other health-related applications. Despite its promising potential, the use of AI in medical diagnostics introduces new challenges, including the widely discussed problem of explainability and transparency. Deep learning models™ inherent lack of interpretability “ commonly referred to as the black box™ challenge “ is problematic in a medical setting, where clear explanations for diagnoses is a requirement. To build trust in AI-driven diagnostic tools among clinicians and patients, and facilitate possible implementation in clinical use, it is crucial to understand the predictions made by these tools. This need for transparency and trust aligns with the requirements of the EU™s recently implemented AI regulation, the AI Act, which classifies applications that could affect the life, safety, and health of people as high-risk [10], and thereby requires these applications to provide explanations before they are allowed for deployment. Prechtl™s General Movements Assessment (GMA) is a highly reliable diagnostic tool for early detection of CP and is based upon medical experts observing normal versus abnormal movement patterns [11], yet it faces several challenges. These include the requirement for training the clinicians performing GMA to achieve proficiency in assessment techniques, the subjective nature of visual analysis which can lead to variability in interpretations, the time-intensive process of manually analyzing movements, and long-term costs associated with training and qualifying medical experts [12]. Using AI can potentially address these issues by automating the detection process of abnormal movement patterns, thereby reducing the burden on the limited number of trained GMA experts, while providing objective and consistent analysis, and significantly reducing the time needed for assessments. However, it is crucial that this does not compromise the interpretability that clinicians value in Prechtl™s GMA, ensuring that the insights offered by AI systems are sufficiently explained and thus complementary and understandable from a clinician™s perspective. To aid healthcare providers in understanding AI diagnoses, Explainable AI (XAI) methods can be used. For instance, in medical imaging, XAI has facilitated cancer detection [13]. In the CP prediction space, Sakkos et al. introduced a deep learning framework using RGB videos, with visualization showing segmented body parts with movement abnormalities and their contribution to the classification result [14]. Reflecting on the boom of automated solutions for Prechtl™s GMA and the growing trend of incorporating XAI in diverse medical applications, and in anticipation of the legal requirements when deploying AI-assisted medical diagnostic tools, our study aims to bridge a crucial gap: despite the evident progress, the application of XAI in skeleton-based CP diagnosis remains understudied. To assess the trustworthiness of the explanations generated by these methods, we advocate for the use of metrics that objectively evaluate the reliability of the explanations. This is crucial because AI-based analysis of skeletal data could extend to several other previously mentioned high-stakes applications, beyond CP prediction. However, the application of domain-specific knowledge across such a broad spectrum of potential uses could become cumbersome. Therefore, we also suggest adopting the metrics evaluated in this study as foundational benchmarks for assessing XAI techniques, and then supplementing them with specialized domain knowledge to enhance and confirm their validity further. This study explores the applicability of Class Activation Mapping (CAM) and Gradient-Weighted Class Activation Mapping (Grad-CAM), which are widely used XAI methods in Convolutional Neural Network (CNN) models, to Graph Convolutional Network (GCN)-based models for Cerebral Palsy (CP) prediction. Specifically, we investigate whether these XAI methods can effectively differentiate between important and unimportant body points influencing CP predictions. Additionally, we assess the stability of explanations when the input data undergo minor perturbations. The contributions of our study are: An objective evaluation framework to assess the reliability of different XAI methods used in a specific medical application, which is skeleton-based early detection of CP from infants™ spontaneous movements. The comparative analysis of different XAI methods (CAM and Grad-CAM), providing insight into their effectiveness in the context of early CP detection, which can potentially be used in other high-stakes applications. Showing the possible use of XAI methods to guide further research in early CP diagnosis. For example, it can potentially be used to discover specific infant movement patterns that are correlated with later CP status that might be complementary information for GMA experts focusing on gestalt infant movements. The evaluation of these XAI methods for this specific application is the first step towards determining the most reliable explanations that can provide valuable insights into specific infant movements. The application of XAI evaluation metrics to an ensemble of models for CP prediction, providing insight into the collective robustness of the aggregated ensemble and individual model explanations against minor perturbations. The following section discusses the various XAI methods implemented for the skeleton-based CP prediction model. These methods will subsequently be assessed using the XAI evaluation metrics. CAM was originally introduced as a method for identifying important pixels in an image [15] as determined by a CNN model. It projects the model™s output layer weights onto a convolutional layer™s feature maps (usually from the final layer), creating a heatmap that highlights the areas influencing the network™s predictions, CAM can be generalized and applied to other convolution-based models if the weights after Global Average Pooling (GAP) for a specific class™ classc l a s s and the nth feature map Fnsuperscript¹F^{n}F ^ n are multiplied, as shown in Equation (1). For example, in human activity recognition (HAR) using 3D skeleton graphs as input to a GCN, important body points in the skeleton data can be highlighted, as shown by Song et al. with their work on EfficientGCN [16]. CAM has several weaknesses, such as the lack of flexibility in model architecture due to the need for a GAP layer and a Fully Connected (FC) layer for classification. Gradient-weighted Class Activation Mapping (Grad-CAM) [17] addresses this by using gradients entering the FC layer instead of weights, calculated via thus making it adaptable to various CNN architectures. Since the introduction of Grad-CAM, many other CAM-based methods have been introduced to further improve the original CAM. Similarly to CAM, Grad-CAM has been shown to be applicable to GCN-based HAR using skeleton data, as illustrated by Das et al. [18]. In our previous work [19], we explored various metrics proposed in [20] to evaluate explanations from different XAI methods within the context of skeleton-based HAR. Building on these foundations, this paper aims to extend these evaluation techniques to a specific medical application, which is skeleton-based CP prediction. The following subsections give a brief overview of the applied metrics, illustrating their applicability and relevance in a specific medical use-case. Consider X‹XX as the original input data, with its corresponding explanation eXsubscript‹e_{X}e _ X , and let f(‹…)“‹…f( ( ‹… ) denote the model™s output, representing CP risk. Then, X²superscript‹²X^{ ^ ² is the perturbed version of X‹XX, and eX²subscriptsuperscript²‹e^{ ^ ² _ X stands for its corresponding explanation following the perturbation. Top-kkk refers to the most important features in the input data, while non-top-kkk refers to the remaining, less important features. Fidelity or faithfulness [21, 22, 23] checks whether an explanation accurately identifies the features that influence a model™s predictions. The Prediction Gap on Important features (PGI) in Equation (3) quantifies the prediction change when key features are altered, while the Prediction Gap on Unimportant features (PGU) in Equation (4) measures the change when minor features are modified. Ideally, when important features are perturbed, there should be a significant change in the model prediction. Conversely, perturbing unimportant features should have little effect on the prediction. By this logic, a good XAI method should identify both important and unimportant features in a way that results in a high PGI and a low PGU. Stability or robustness [24, 25], measures how much an explanation changes relative to changes in the model™s input, output, or internal representation when the original input data undergo minor perturbations, as shown in Equations (5)“(7). Relative Input Stability (RIS) includes three components for each of the model™s multiple input streams: position, velocity, and bone, referred to as RISp, RISv, and RISb, respectively. ROS refers to Relative Output Stability and RRS refers to Relative Representation Stability. To better understand these concepts in relation to the variables in the model, please refer to Fig. 1(a), which illustrates the main architecture of the network. Xsubscript‹ _ X in Equation (7) denotes the model™s internal representation, which in our study is the logits from the layer preceding the softmax activation function. A stability score of zero is ideal, as it means that visually imperceptible perturbations do not change the explanation at all. The dataset used in this study originates from Groos et al. [3], and is made up of 557 infants at elevated risk of perinatal brain injury, gathered from 13 different hospitals in Norway, Belgium, India, and the US. In compliance with Prechtl™s GMA tool protocols, the infants were filmed during the FM period occurring between 9 and 18 weeks™ corrected age. Using the CP decision tree from the Surveillance of Cerebral Palsy in Europe [26], a pediatrician determined their CP statuses at 12 months corrected age or older. Of the 557 videos, 75% were allocated for model training and validation, while the remaining 25% formed the test set which is used in this paper. Of the 139 videos in the test set, 21 are true CP cases and 118 are true No CP cases. For the XAI metrics testing, extracted skeleton tracker data from the original videos were used instead of the videos themselves. Given the long testing times for computing XAI metrics (approximately 1 hour per 5-second window on an RTX3090 GPU), random 5-second window samples were taken from the tracker data, proportional to the video length. For example, only one 5-second window sample was taken from the shortest videos. Moreover, since testing for stability ¡™¡¦stabilitys t a b i l i t y metric requires that the true label matches the predicted label, only correctly predicted data were used, resulting in 15 CP and 111 No CP cases. From these, 24 random 5-second windows were selected from CP data and 136 from No CP data, maintaining the original CP to No-CP ratio. Additionally, windows at the beginning or end of the videos were avoided to reduce noise, and non-overlapping 5-second windows were chosen to avoid redundant data. The same deep learning-based CP prediction model from the Groos et al. study [3] was evaluated for the XAI metrics. This model uses a GCN architecture to process infants™ biomechanical movement properties, namely positions, velocities, and bones (distances between body keypoints). The GCN architecture was optimized through an automatic search, which created 70 distinct model instances, each trained on different sections of the dataset. Refinement of the model involved hyperparameter tuning and an automatic Neural Architecture Search (NAS) approach, exploring various architectural designs and configurations. The final Ensemble-NAS-GCN model combines predictions from the 70 GCN instances, as illustrated in Fig. 1(b), where the model number represents a unique architecture and the portion number denotes the specific training/validation fold used for that model. Details about each of the 10 GCN model architectures can be found in the Appendix Table 3. Each model instance analyzes 5-second windows, with the CP risk determined as the median prediction across the ensemble. Similarly, for each 5-second window, the XAI attribution scores (i.e. CAM or Grad-CAM) from all 70 GCN instances were collected and unified by calculating the median score for each body point. There were no alterations made to the CP prediction pipeline except for capturing intermediate variables, such as model weights and gradients needed for the XAI metric evaluation. As demonstrated in our previous work [19], skeleton joints can be perturbed by converting Cartesian coordinates to spherical coordinates. This is performed using the equations below, where dx¥dxd x and dy¦dyd y represent the perturbation magnitudes along the x¥xx- and y¦yy-axes respectively, PƒPP is the original point, P²superscriptƒ²P^{ ^ ² is the new perturbed point, x²superscript¥²x^{ ^ ² and y²superscript¦²y^{ ^ ² are the new coordinates, x¥xx and y¦yy are the original coordinates, and Îƒ is the randomly generated azimuthal angle: Equations (5), (6), (7) require that the perturbed input data X²superscript‹²X^{ ^ ² remains close to the original data X‹XX, which also maintains accurate human kinematics and preserving the integrity of model predictions. To achieve this, rrr is set to 1% of the median height of the infant across all frames in a 5-second window. The infant coordinate data are in pixels and height is calculated as the Euclidean distance between the head and left ankle. The values dx¥dxd x and dy¦dyd y are computed once for each joint and applied consistently across all video frames, producing a perturbed 2D point. The first step is to obtain the explanation for the original unperturbed data. The skeleton tracker data is fed into the CP prediction pipeline after Data Preprocess stage as shown in Fig. 1(a). Each model in the ensemble generates its own explanation according to Equations (1) and (2), as shown in Fig. 1(b)). These individual explanations are combined by calculating the median values, representing the overall explanation for the ensemble. Similarly, individual model output predictions are combined by taking the median value, representing the ensemble™s output prediction. This process yields the terms f(X)“‹f(X)f ( X ), eXsubscript‹e_{X}e _ X , and X‹XX. The term Xsubscript‹ _ X is derived by collecting internal representations from the final FC layer before the softmax function of each model and flattening them into a single array. Next, the body keypoints are ranked by importance from highest to lowest, and the top-k and non top-k joints are identified as determined by the ensemble™s explanations. A sample visualization of these attribution scores, translated into color-coded importance indicators, is shown in Fig. 2, where green indicates low scores, yellow indicates moderate scores, orange indicates high scores, and red indicates very high scores relative to a threshold of 0.3. Perturbed varieties of the original data are generated as outlined in the previous section. Each 5-second window undergoes n=5050n=50n = 50 perturbations at a specified magnitude rrr. The top-k body points (where k = 1 to 19) are systematically perturbed nnn times and then fed into the model to calculate PGI, then the remaining points are perturbed to compute PGU. Stability metrics are also computed using the explanations for the nnn perturbations and the resulting intermediate values in the ensemble. We use the Area Under the Curve (AUC) to combine the calculated metric scores across all k values for each video into a single score. We then compute the mean and standard deviation of these AUCs in all videos to obtain the overall metric values. Since the metrics are unitless, the results are also compared against a random method that assigns feature attribution scores randomly, which represents the worst performance for an XAI method when subjected to a perturbation magnitude rrr. To compare the ensemble™s XAI metrics performance with individual models, we conducted the same test on each model architecture. The ensemble consists of 70 model instances across 10 unique architectures, each trained on 7 different folds. Model 9, portion 5 achieved the highest AUC-ROC score in the test set, and thus portion 5 was used to represent each architecture in the tests. The same test pipeline is implemented as described above, except that the intermediate values for the metrics were derived from the individual models rather than the ensemble. Note that each model and the ensemble have unique sets of top-k and non top-k body points identified, resulting in distinct perturbed skeleton data, requiring separate tests. Using our hardware setup with RTX3090 GPU, the calculation for the ensemble took approximately 163 hours for CAM, 238 hours for Grad-CAM, and 120 hours for random method. The calculation for CAM, Grad-CAM, and random methods combined took approximately 24 hours for each model. The results for the individual models can be found in the Appendix Tables 1 and 2. Lastly, we conducted an unpaired t-test to determine the statistical significance of the differences in metric results between CAM and Grad-CAM. For completeness, we also performed t-tests comparing CAM versus random attribution and Grad-CAM versus random attribution, expecting significant statistical differences in both tests. A line plot is used to comparatively show the metrics performance of each XAI method in Fig. 3, which shows the relative performances for each metric based on their positions on the horizontal axis. The exact numerical results with confidence intervals are shown in the Appendix Tables 1 and 2. PGI results indicate that the ensemble™s predictions are influenced by key features identified by the two XAI methods (i.e. the change in output probability f(X)“‹f(X)f ( X ) increases when important nodes are perturbed), with a significant difference compared to the random attribution at p<0.0050.005p<0.005p < 0.005 for both CAM versus random and Grad-CAM versus random. There was no significant difference in PGI and PGU between CAM and Grad-CAM. Overall, the faithfulness tests indicate that XAI methods effectively distinguish important from unimportant body points influencing predictions. The plots of RISp, RISv, RISb, ROS, and RRS in Fig. 3 immediately show us that random explanations cause significant changes in the stability results, with p<0.0050.005p<0.005p < 0.005 for both CAM versus random and Grad-CAM versus random. This indicates that both XAI methods are less susceptible to large explanation changes with small input perturbations. Most notably, Grad-CAM offers a highly significant improvement (with p<8.507Ã1078.507superscript107p<8.507 10^{-7}p < 8.507 Ã 10 ^ - 7 ) of input stability compared to CAM for RISv of the ensamble model. In contrast, CAM offers a significantly better input stability (with p<0.050.05p<0.05p < 0.05) compared to Grad-CAM for RISb and RRS. The result for the stability and faithfullness metric for the different GCN models in ensemble can be found in the Appendix Tables 1 and 2. This study evaluated the reliability and robustness of two XAI techniques, CAM and Grad-CAM, within a deep learning ensemble model. The faithfulness metrics show both methods are effective in identifying important and unimportant body points influencing CP predictions, while stability tests demonstrate robustness against minor data perturbations. Specifically, CAM significantly outperformed Grad-CAM in RISb and RRS, while Grad-CAM excelled in RISv. The choice of XAI method depends on the specific application and key metrics. For instance, to identify potential movement biomarkers for CP related to joint velocity, Grad-CAM is recommended for its superior RISv performance. This also suggests exploring the velocity input branch for possible CP movement biomarkers, which aligns with previous studies [27, 28, 29, 30] where a velocity parameter is used in classical machine learning assessments of CP. If similar explanations for body points are found in multiple 5-second windows, Grad-CAM offers better insight into body point velocity due to its stable explanations despite input variations. Conversely, for quick explanations (i.e. for visual inspection purposes), CAM is preferable due to its faster calculation time. Individual models in the ensemble show varied results in metrics tests, while the ensemble provides a combined outcome from its constituent models. In our previous work [19], we applied metrics tests to a single GCN-based model, limiting generalizability. This study has evaluated XAI metrics in both an ensemble and the individual models composing it, revealing that neither CAM nor Grad-CAM consistently outperforms the other in all metrics. Overall, our findings demonstrate that both XAI methods can provide reliable and stable explanations in CP prediction models. By finding patterns and doing further studies into the explanations, these could potentially supplement GMA observers with specialized domain knowledge, contributing to more interpretable and trustworthy AI diagnostics. The study™s insights into the comparative performance of CAM and Grad-CAM can also guide future research in improving AI explainability in other high-stakes medical applications. As a next step, it is crucial to introduce more XAI metrics that incorporate domain-specific knowledge to further validate the relevance and accuracy of the explanations provided. It is also necessary to perform clinical interpretations on the explanations to enhance the practical utility of the models. Architectural choice 1 2 3 4 5 6 7 8 9 10 No. modules of input br. 3 3 2 2 2 2 3 3 2 2 Width of input br. 10 10 12 10 8 6 8 6 12 12 Block type in initial mod. Bottl. Basic Basic Basic Bottl. Basic Basic MBC. Bottl. Basic Residual type in initial mod. None Den. None Block Den. Den. Mod. Block Den. Den. No. tmp. scales in input br. 1 3 2 2 3 2 2 1 2 2 No. levels of main br. 3 1 3 2 2 2 2 2 2 1 No. modules of main br. levels 3 2 1 1 3 3 2 3 1 1 Width of first level of main br. 12 12 12 10 12 12 10 12 8 12 No. tmp. scales in main br. 1 2 2 3 2 1 2 1 1 1 Pooling layer type Gl. Gl. Gl. Sp. Sp. Gl. Gl. Gl. Gl. Sp. Graph convolution type DA 2 DA 4+2 SC DA 4 SC DA 4 DA 2 DA 2 DA 4 SC Block type Basic MBC. Basic Basic Bottl. Bottl. Basic Basic Bottl. Basic Bottl. factor 4 2 2 2 2 2 4 4 4 4 Residual type None Block Den. None None Block None Den. Block None SE type None Outer Inner None Outer None None Outer Outer None SE ratio - 2 2 4 2 4 4 4 4 4 SE ratio type - Abs. Abs. Abs. - Abs. - Abs. Abs. - Attention type Ch. - - - Ch. - Ch. - Ch. - Nonlinearity type ReLU Sw. ReLU Sw. Sw. ReLU ReLU Sw. ReLU Sw. Tmp. kernel size 9 7 9 7 9 7 9 7 9 7 AUC 0.949 0.942 0.938 0.943 0.937 0.956 0.953 0.953 0.932 0.947",
        "keywords": ""
    },
    {
        "id": 16,
        "title": "Spatio-Temporal Communication Compression for Distributed Prime-Dual Optimization",
        "abstract": "AbstractIn this paper, for the problem of distributed computing, we propose a general spatio-temporal compressor and discuss its compression methods. This compressor comprehensively considers both temporal and spatial information, encompassing many existing specific compressors. We use the average consensus algorithm as a starting point and further studies distributed optimization algorithms, the Prime-Dual algorithm as an example, in both continuous and discrete time forms. We find that under stronger additional assumptions, the spatio-temporal compressor can be directly applied to distributed computing algorithms, while its default form can also be successfully applied through observer-based differential compression methods, ensuring the linear convergence of the algorithm when the objective function is strongly convex. On this basis, we also discuss the acceleration of the algorithm, filter-based compression methods in the literature, and the addition of randomness to the spatio-temporal compressor. Finally, numerical simulations illustrate the generality of the spatio-temporal compressor, compare different compression methods, and verify the algorithm™s performance in the convex objective function scenario.",
        "corpus": "In this paper, for the problem of distributed computing, we propose a general spatio-temporal compressor and discuss its compression methods. This compressor comprehensively considers both temporal and spatial information, encompassing many existing specific compressors. We use the average consensus algorithm as a starting point and further studies distributed optimization algorithms, the Prime-Dual algorithm as an example, in both continuous and discrete time forms. We find that under stronger additional assumptions, the spatio-temporal compressor can be directly applied to distributed computing algorithms, while its default form can also be successfully applied through observer-based differential compression methods, ensuring the linear convergence of the algorithm when the objective function is strongly convex. On this basis, we also discuss the acceleration of the algorithm, filter-based compression methods in the literature, and the addition of randomness to the spatio-temporal compressor. Finally, numerical simulations illustrate the generality of the spatio-temporal compressor, compare different compression methods, and verify the algorithm™s performance in the convex objective function scenario. keywords Communication compression; distributed optimization; linear convergence; spatio-temporal compressors; average consensus. Distributed intelligent systems, such as drone swarms, smart grids, and cyber-physical systems, have been extensively researched across disciplines such as control, signal processing, and machine learning [1, 2, 3, 4]. The mathematical representation of a distributed system involves a network connecting multiple agents, where each node symbolizes an individual agent, and the edges depict communication lines between these nodes. When distributed systems are required to implement tasks such as cluster optimization and collaborative control, it requires the foundational functionality of distributed computing. In this process, each node stores localized information and communicates messages with connected nodes through the network, and collaboratively solves a mathematical problem [1]. This paper focuses on addressing distributed optimization problems, where each node possesses a function, aiming to identify solutions that collectively minimize the sum of network node functions through constant communication across the network. Extensive research has been devoted to the study of distributed optimization algorithms, primarily rooted in the consensus algorithm. The goal of this algorithm is to foster consistency in the states across nodes within the network. A combination of the consensus algorithm with the classical gradient descent method in optimization problems, coupled with stability tactics, results in the distributed subgradient algorithm (DSG), achieving sublinear convergence under a strongly convex global cost function [5, 6]. To address distributed optimization problems with faster rate requirements, more sophisticated algorithms have been introduced. The distributed gradient tracking algorithm (DGT) incorporates an additional state to trace the gradient of the objective function [7, 8], akin to integral action [9]. For diverse equivalent forms of distributed optimization problems, various Lagrangian functions have been proposed, giving rise to multiple algorithms based on the saddle point dynamic method. Examples include the Wang-Eila algorithm in [10] and primal dual algorithm in [11], distinct in communication states. In practical implementation, the network bandwidth for communication in distributed systems is limited and numerous communication compression strategies have been developed to handle such issues. In [12, 13, 14, 15], the idea of quantifying the communication is combined with DSG and DGT algorithms. Specifically, [12] introduced adaptive quantization and [13] applied random quantization, where the quantizer codebook changes when approaching the solution. In [15], the authors developed a dynamic encoding and decoding scheme for quantization. In addition to quantization, there are also several other types of compressors capable of reducing communication bits by synthesizing concepts from quantization, sparsity, and randomization [16, 17, 18, 19]. These compressors exclusively focus on the spatial dimension, encompassing the information within transmitted messages. Notably, the compressor in [20] incorporates temporal dimensions, utilizing information across time. Other research aims to identify commonalities among specific compressors, thereby proposing a generalized definition of compressors [21, 22, 23]. This definition allows any function that satisfies these properties to be considered as a compressor and applied to algorithms. Our goal is to propose such a definition, characterized by properties that simultaneously consider both temporal and spatial dimensions. In addition, how to combine the compressors with distributed optimization algorithms has become a noteworthy area of study. This is because refining the application method can facilitate the successful integration of more general compressors and enhance the overall effectiveness of the algorithm. Beyond the direct application of compressors to the communication state, there exist intriguing techniques, as direct application often poses challenges to stability [24, 25]. For instance, [12, 19] incorporate a weighted sum of the updated value and the original value into the original value, while [13, 26] compress the difference between iterations rather than the original value. In the work of [15, 27], the difference is scaled and then compressed, with the results communicated after a reverse reduction, further ensuring the convergence of the algorithm. [16, 28] adopts a differential compression method based on filtering, and through additional equivalent transformations, ensures that only compressed values are exchanged between nodes. The proposal of a compressor application method based on observers is also a main focus of this paper. Our research in this paper contributes in two aspects: proposing a general compressor definition and exploring its application methods. Firstly, we propose an unified spatio-tempral compressor theory for communication compression in distributed optimization. Such ST compressor uses a constructive exponential stability perspective from nonlinear systems, and various static communication compressors in the literature are categorized into it. We discuss the compression method of direct compression and our proposed observer-based compression, and establish convergence conditions for a class of distributed Prime-Dual optimization algorithms with explicit convergence rates. Our results and analysis are presented to the large class of ST communication compression, without replying on the specific form of a particular compressor. We also discuss the extensions to accelerated algorithm and stochastic compressor. Finally, we validate the above conclusions through simulation experiments. The paper is structured as follows. Section 2 formulates the distributed optimization problem of interest and proposes the spatio-temporal compressor of both original form and stronger form for message communication. In Section 3, we start from the distributed consensus to illustrate the stronger conditions required when applying this compressor directly, as well as the successful combination with observer-based compression methods. In Section 4, we respectively discuss the applicability of these two compression methods to the Prime-Dual flow and discuss several extension. In Section 5, we discretize the Flows based on Euler method, discuss its acceleration method, and introduce randomness to our compressor. Numerical simulations are presented to show the effectiveness of the proposed approaches in Section 6. Finally, a brief conclusion is made in Section 7. All technical proofs are collected in the Appendices. Notation. In this paper, ¥‹…¥ ‹… ¥ denotes Euclidean norm. The notation n(n)subscript1subscript0 _ n ( 0 _ n ), nsubscript _ n and {1,¦,m}subscript1¦subscript e}_{1},...,{ e}_{m} e _ 1 , ¦ , e _ m } denote the column one (zero) vector, identity matrix and base vectors in dsuperscript ^ d , respectively. The expression diag(x1,¦,xn)diagsubscript¥1¦subscript¥ ( x _ 1 , ¦ , x _ n ) is a diagonal matrix with the iii-th diagonal element being xisubscript¥x_{i}x _ i . The symbol Štensor-product denotes the Kronecker product. For differential function, (‹…)‹… ( ‹… ) denotes its gradient. denotes Hadamard product. In this paper, we consider a network of agents indexed by V={1,2¦n}V12¦ = { 1 , 2 ¦ n }, where each agent iVVi roman_V holds a cost function fi:d†:subscript“†superscriptf_{i}: _ i : R ^ d † R, and aims to solve the following distributed optimization problem Particularly, each local cost function fisubscript“f_{i}f _ i is assumed to fulfill the following requirements. The following properties are satisfied. The global cost function f(±):=i=1nfi(±)assign“±superscriptsubscript1subscript“±f( ( x ) := _ i = 1 ^ n f _ i ( x ) is strongly convex, i.e., there exists Î¼>00 > 0 such that f(²)¥f(±)+f(±)T(²±)+Î¼2²±2“²“±“superscript±²±2superscriptnorm²±2f( f( f+ ( y ) ¥ f ( x ) + f ( x ) ^ T ( y - x ) + / Î¼ 2 ¥ y - x ¥ ^ 2 for all ±,²d±²superscript , y R ^ d . Each local gradient fisubscript“ f_{i} f _ i is globally Lipschitz continuous, i.e., there exists Lf>0subscript¿“0L_{f}>0L _ f > 0 such that fi(±)fi(²)¤Lf±²normsubscript“±subscript“²subscript¿“norm±² f_{i}( f_{i}( L_{f} f _ i ( x ) - f _ i ( y ) ¥ ¤ L _ f ¥ x - y ¥ for all ±,²d±²superscript , y R ^ d . ¡¡ If Assumption 1 holds, then the considered optimization problem (1) turns out a strongly convex optimization problem, allowing an optimal solution sŠ†dsuperscript superscripts^{ ^ Š† R ^ d such that f(s)=0“superscript 0 f(s^{ f ( s ^ ) = 0 and f(s)=f“superscript superscript“f(s^{ ( s ^ ) = f ^ , where fsuperscript“f^{ ^ is the optimal value. As each agent has the information of only local cost function, to solve such a distributed optimization problem (1), a communication network is usually required for transmitting messages. Denote the communication graph G=(V,E)GVE = ( roman_V , roman_E ), where EE denotes the set of edges. Let [aij]RnÃndelimited-[]subscriptsuperscriptR[a_{ij}] n}[ a _ i j ] roman_R ^ n Ã n denote the weight matrix complying with graph GG i.e., aij>0subscript0a_{ij}>0a _ i j > 0 if (j,i)EE(j,i) j , i ) roman_E and aij=0subscript0a_{ij}=0a _ i j = 0 if (j,i)EE(j,i) j , i ) roman_E. Then denote by ‹‹ the Laplacian matrix of graph GG satisfying [‹]ij=aijsubscriptdelimited-[]‹subscript[ L ] _ i j = - a _ i j for all i ji ji j, and [‹]ii=j=1naijsubscriptdelimited-[]‹superscriptsubscript1subscript[ L ] _ i i = _ j = 1 ^ n a _ i j for all iVVi roman_V. Denote the neighbor set of agent iii as NisubscriptN _ i , satisfying jNisubscriptNj roman_N _ i if and only if [‹]ij 0subscriptdelimited-[]‹0[ 0[ L ] _ i j 0 for all i,jVVi,j , j roman_V. For simplicity, we make the following assumption on the communication graph. The graph GG is undirected, connected and time-invariant. The above Assumption 2 indicates that the Laplacian matrix ‹‹ is symmetric positive semi-definite, with [‹]ij=[‹]jisubscriptdelimited-[]‹subscriptdelimited-[]‹[ L ] _ i j = [ L ] _ j i , ‹n=nsubscript‹subscript0 _ n = 0 _ n and its eigenvalues Î»isubscript† _ i , iVVi roman_V in an ascending order satisfying 0=Î»1<Î»2¤¦¤Î»n0subscript†1subscript†2¦subscript†0= = Î» _ 1 < Î» _ 2 ¤ ¦ ¤ Î» _ n by [1]. We let nÃ(n1)superscript1 R ^ n Ã ( n - 1 ) be a matrix whose rows are eigenvectors corresponding to nonzero eigenvalues of ‹‹ satisfying With the communication graph GG several distributed optimization algorithms have been developed in the literature [7, 8, 9, 10, 11] to compute the solution ssuperscript s^{ ^ for (1). In this paper, we mainly focus on the distributed Prime-Dual algorithm, which enables to achieve exponential convergence and further generalizations to the case with optimization constraints [29, 30]. A common distributed Prime-Dual flow for (1) takes the form [11] where Î²,Î·>0½‚0 , Î· > 0 are parameters to be fixed and the initial condition i=1n¯i(0)=dsuperscriptsubscript1subscript¯0subscript0 _ i = 1 ^ n v _ i ( 0 ) = 0 _ d . In this paper, we are particularly interested in the Spatio-Temporal (ST) compressors. Given a uniformly linearly bounded mapping ‚:dÃ+†d:‚†superscriptsubscriptsuperscript : R ^ d Ã R _ + † R ^ d , i.e., there exists a Lc>0subscript¿0L_{c}>0L _ c > 0 such that ‚(±e,t)¤Lc±enorm‚subscript±¡subscript¿normsubscript± L_{c} C ( x _ e , t ) ¥ ¤ L _ c ¥ x _ e ¥ for all ±edsubscript±superscript _ e R ^ d and any t+¡subscriptt R _ + . Then we have the following statements. The mapping ‚‚ is said to be a ST compressor in continuous time, if the induced continuous-time non-autonomous system ±Ë™e=‚(±e,t)subscriptË™±‚subscript±¡ x _ e = - C ( x _ e , t ) is uniformly globally exponentially stable at the origin. The mapping ‚‚ is said to be a ST compressor in discrete time, if the induced discrete time non-autonomous system ±e(t+1)=±e(t)Îº0‚(±e,t)subscript±¡1subscript±¡subscript…0‚subscript±¡ _ e ( t + 1 ) = x _ e ( t ) - Îº _ 0 C ( x _ e , t ) is uniformly globally exponentially stable at the origin for some stepsize Îº0>0subscript…00 _ 0 > 0. ¡¡ In the following, a stronger version of the spatio-temporal compressors, i.e., the Strongly Spatio-Temporal (SST) compressors, is introduced. Given a uniformly globally Lipschitz mapping ‚:dÃ+†d:‚†superscriptsubscriptsuperscript : R ^ d Ã R _ + † R ^ d , i.e., there exists a Lc>0subscript¿0L_{c}>0L _ c > 0 such that ‚(±e,t)‚(±e²,t)¤Lc±e±e²norm‚subscript±¡‚superscriptsubscript±²¡subscript¿normsubscript±superscriptsubscript±² L_{% c} C ( x _ e , t ) - C ( x _ e ^ ² , t ) ¥ ¤ L _ c ¥ x _ e - x _ e ^ ² ¥ for all ±e,±e²dsubscript±superscriptsubscript±²superscript _ e , x _ e ^ ² R ^ d and any t+¡subscriptt R _ + . Then we have the following statements. The mapping ‚‚ is said to be a SST compressor in continuous time, if the induced continuous-time non-autonomous system ±Ë™e=k‚(±e,t)subscriptË™±‚subscript±¡ x _ e = - k C ( x _ e , t ) is uniformly globally exponentially stable at the origin for any k>00k>0k > 0. The mapping ‚‚ is said to be a SST compressor in discrete time, if there exists Îº0>0superscriptsubscript…00 _ 0 ^ > 0 such that the induced discrete time non-autonomous system ±e(t+1)=±e(t)Îº0‚(±e,t)subscript±¡1subscript±¡subscript…0‚subscript±¡ _ e ( t + 1 ) = x _ e ( t ) - Îº _ 0 C ( x _ e , t ) is uniformly globally exponentially stable at the origin for any stepsize Îº0(0,Îº0)subscript…00superscriptsubscript…0 _ 0 ( 0 , Îº _ 0 ^ ). ¡¡ It is clear that the mapping ‚‚ satisfying either of both Definitions 1 and 2, needs to vanish at the origin, i.e., ‚(0,t)¡0‚0¡0 0C ( 0 , t ) ¡ 0 uniformly in t¡tt. This immediately shows that the uniformly globally Lipschitz property in Definition 2 implies the uniformly linearly bounded property. Besides, the exponential stability of the induced non-autonomous ±esubscript± _ e -systems in Definition 2 is more restrictive than Definition 1. In view of both points, the notion of the SST compressor in Definition 2, is stronger than that of Definition 1. In the literature there are also some other classes of compressors, that are indeed special cases of our (strong) ST compressors. The scalarized compressor ‚1:dÃ+†d:subscript‚1†superscriptsubscriptsuperscript _ 1 : R ^ d Ã R _ + † R ^ d satisfies ‚1(±e,t)=™(t)™(t)T±esubscript‚1subscript±¡™¡™superscript¡subscript± _ 1 ( x _ e , t ) = ( t ) ( t ) ^ T x _ e , where the compression vector ™:+†d:™†subscriptsuperscript : R _ + † R ^ d is uniformly bounded and persistently excited, i.e., either of the followings holds For continuous time, For discrete time, for some constants Î±1,Î±2,T1>0subscript¼1subscript¼2subscript10 _ 1 , Î± _ 2 , T _ 1 > 0 (see [20]). ¡¡ The contractive compressor ‚2:d†d:subscript‚2†superscriptsuperscript _ 2 : R ^ d † R ^ d satisfies for some †(0,1]01 ( 0 , 1 ] and p>00p>0p > 0 (see [16, 19, 31], with the expectation operator removed). By [16], the following ‚2asubscript‚2 _ 2 a and ‚2bsubscript‚2 _ 2 b are specific examples of ‚2subscript‚2 _ 2 , and ‚2csubscript‚2 _ 2 c is a specific example of ‚2subscript‚2 _ 2 with p=11p=1p = 1 and †=3434 = / 3 4 : Greedy (Top-k) sparsifier [32] ‚2a(±e)=s=1k[±e]isissubscript‚2subscript±superscriptsubscript 1subscriptdelimited-[]subscript±subscript subscriptsubscript e}_% {i_{s}}C _ 2 a ( x _ e ) = _ s = 1 ^ k [ x _ e ] _ i _ s e _ i _ s where i1,¦,iksubscript1¦subscripti_{1},...,i_{k}i _ 1 , ¦ , i _ k are the indices of largest t¡tt coordinates in the absolute value of ±esubscript± _ e . Standard uniform quantizer [16] ‚2b(±e)=±e2sgn(±e),subscript‚2subscript±subscriptnormsubscript±2sgnsubscript± sgn}( _ 2 b ( x _ e ) = / ¥ x _ e ¥ _ 2 roman_sgn ( x _ e ) , where sgn(‹…)sgn‹… ( ‹… ) denotes the element-wise sign. Saturated quantizer where i=1,2¦,d12¦i=1,2...,di = 1 , 2 ¦ , d, Î”Î” R denotes the quantization precision and Š‹…‹‹… ‹… ‹ denotes the the floor sign. ¡¡ The following statements are true. The scalarized compressor ‚1subscript‚1 _ 1 belongs to the SST compressor. The contractive compressor ‚2subscript‚2 _ 2 belongs to the ST compressor. ¡¡ A specific example of discrete time cases of ‚1subscript‚1 _ 1 , denoted by ‚1asubscript‚1 _ 1 a , can be derived by letting ™(t)=i™¡subscript ( t ) = e _ i with i=1+(tmodd)1¡modi=1+(t d)i = 1 + ( t roman_mod d ) for t•¡•t N. In addition to the above mentioned compressors, there are also some other forms of compressors that satisfy Definition 2. For example, ‚1(±e,t)=Î(t)™(±e,t)subscript‚1subscript±¡ƒ¡™subscript±¡ _ 1 ( x _ e , t ) = Î ( t ) ( x _ e , t ) where ™(±e,t)™subscript±¡ ( x _ e , t ) is a scalarized mapping and Î(t)™(±e,t)ƒ¡™subscript±¡ ( t ) ( x _ e , t ) is strongly PƒPP-monotonic (see [33]). ¡¡ We stress that when the compressor ‚(±e,t)‚subscript±¡ ( x _ e , t ) is used, we do not mean to using ‚(±e,t)‚subscript±¡ ( x _ e , t ) to encode ±esubscript± _ e for communication and then transmitting the whole vector of ‚‚ directly. Instead, the ‚‚ represents the communication information, whose transmission can be implemented requiring less bandwidths than directly transmitting ±esubscript± _ e of ddd dimensions, leading to the so-called communication compression. For example, if the scalarized compressor ‚1subscript‚1 _ 1 is adopted, the actual communication message at each round is a scalar ™(t)Š¤±e(t)™superscript¡topsubscript±¡ ( t ) ^ Š¤ x _ e ( t ) with each agent holding a common ™(t)™¡ ( t ), while for the standard uniform quantizer ‚2bsubscript‚2 _ 2 b , the actual communication message consists of a scalar ±esubscriptnormsubscript± x _ e ¥ _ and a vector sgn(±e)sgnsubscript± ( x _ e ). In view of this, with a bit abuse of notation, we insist on saying the mapping ‚‚ to be a compressor throughout the paper. ¡¡ In contrast with the conventional compressors, e.g., the contractive compressor, the ST compressor exhibits two distinctive features. Firstly, it synthesizes information from both the time and space domains, broadening its applicability and expanding the design possibilities. Secondly, its key characteristic is elucidated through a non-autonomous system, which can simplify the design procedure while providing the flexibility to incorporate control-related tools into distributed optimization. Distributed consensus is a fundamental algorithm that acts as a subroutine in numerous distributed optimization problems. In view of this, in this section we investigate how to combine the ST compressors with the consensus algorithm, which motivates the subsequent developments of distributed optimization algorithms with ST compressors. Moreover, due to its convenience of analysis, we focus on the continuous-time distributed consensus, taking the form where ±i,cdsubscript±superscript _ i , c R ^ d denotes the message transmitted by agent iVVi roman_V. It is clear that over the graph GG under Assumption 2, each node state exponentially reaches consensus at the average ±:=1nj=0n±j(0)assignsuperscript±1superscriptsubscript0subscript±0{ ^ := / 1 n _ j = 0 ^ n x _ j ( 0 ). An intuitive design of compressed consensus algorithm is to directly replace the transmitted message ±isubscript± _ i by the compressed one, i.e., ±i,c=‚(±i,t)subscript±‚subscript±¡ _ i , c = C ( x _ i , t ) in (4), leading to the following distributed consensus flow with direct compression (DC-DC) as Then a natural question arises: given a ST compressor ‚‚ whether or when the DC-DC flow (5) maintains the exponential convergence to the average. Before we answer such a question, we make the following observation on the SST compressor. Given a SST compressor ‚‚ in continuous time, it is clear that the system where ²e(n1)dsubscript²superscript1{ _ e R ^ ( n - 1 ) d , Î:=diag(Î»2,¦,Î»n)ŠdassignÎtensor-productdiagsubscript†2¦subscript†subscript := roman_diag ( Î» _ 2 , ¦ , Î» _ n ) Š I _ d and (²,t):=[‚T(²1,t),¦,‚T(²n1,t)]Tassignsuperscript²¡superscriptsuperscript‚subscript²1¡¦superscript‚subscript²1¡ }^{T}( ^ - ( y , t ) := [ C ^ T ( y _ 1 , t ) , ¦ , C ^ T ( y _ n - 1 , t ) ] ^ T , is uniformly globally exponentially stable at the zero equilibrium. By the converse Lyapunov Theorem for exponential stability [34, Theorem 4.14], this implies the existence of a Lyapunov function Ve:(n1)dÃ+†+:subscript†superscript1subscriptsubscriptV_{e}: _ e : R ^ ( n - 1 ) d Ã R _ + † R _ + such that for some c1,c2,c3,c4>0subscript1subscript2subscript3subscript40c_{1},c_{2},c_{3},c_{4}>0c _ 1 , c _ 2 , c _ 3 , c _ 4 > 0. With this in mind, and defining •Š:=Šdassign•Štensor-productsubscript := S Š I _ d and (±,t):=[‚T(±1,t),¦,‚T(±n,t)]Tassign±¡superscriptsuperscript‚subscript±1¡¦superscript‚subscript±¡ }( ( x , t ) := [ C ^ T ( x _ 1 , t ) , ¦ , C ^ T ( x _ n , t ) ] ^ T , we are ready to propose the following theorem for Flow (5), answering the question by showing that an extra condition on the communication network GG and the SST compressor ‚‚ is still required to maintain an exponential convergence to the average. Let Assumption 2 hold, then for the DC-DC Flow (5) with a SST compressor in continuous time ‚‚ if there holds for Î<c3c4Î»n¿subscript3subscript4subscript† < / c _ 3 c _ 4 Î» _ n , then there holds for some Î³>0¾0 > 0. ¡¡ From the extra condition (7), it can be seen that not only the SST compressor ‚‚ but also the network graph (see •Š•Š play a role of determining the exponential convergence property of the DC-DC flow (5) in general. Moreover, by taking a linear form of SST compressor ‚(±e,t)=M(t)±e‚subscript±¡¡subscript± ( x _ e , t ) = M ( t ) x _ e , e.g. the scalarized compressor ‚1subscript‚1 _ 1 , we note that the extra condition (7) reduces to which holds for all (±,t)ndÃ+±¡superscriptsubscript( x , t ) R ^ n d Ã R _ + , since (n1ŠM(t))•ŠT•ŠT(n1ŠM(t))=0tensor-productsubscript1¡superscript•Šsuperscript•Štensor-productsubscript1¡0( M(t)) M(t))=0( I _ n - 1 Š M ( t ) ) S ^ T - S ^ T ( I _ n - 1 Š M ( t ) ) = 0. This immediately implies that the linear SST compressor, e.g. the scalarized compressor ‚1subscript‚1 _ 1 , is applicable to the DC-DC flow (5) with no need of any extra condition. In the previous subsection, it has been shown that the SST compressor can be directly applied, subject to an extra condition (7) which poses limitations on the range of feasible compressors and communication graphs. In this subsection, such limitations will be removed by proposing a new distributed compressed consensus, taking the form where Î±>0¼0 > 0 is a gain parameter, and ±j,ci(0)=±i,cj²(0),j,j²Niformulae-sequencesubscriptsuperscript±0subscriptsuperscript±superscript²0for-allsuperscript²subscriptN j,j^{ ^ i _ j , c ( 0 ) = x ^ j ^ ² _ i , c ( 0 ) , j , j ^ ² roman_N _ i , iVVi roman_V. The proposed compressed consensus flow (8) is comprised of two sets of states for each agent iii. The state ±isubscript± _ i denotes the estimate of consensus solution as in (4), while the states ±j,cisubscriptsuperscript±{ ^ i _ j , c are introduced to each agent iii to estimate its neighboring solution state ±jsubscript± _ j , jNisubscriptNj roman_N _ i . To have a better view of this, let first ignore the compressor and have ±i,c=±i±i,cisubscript±subscript±superscriptsubscript±{ _ i , c = x _ i - x _ i , c ^ i in (8). Then it is clear that the ±j,cisubscriptsuperscript±{ ^ i _ j , c acts as an observer to estimate ±jsubscript± _ j . Our proposed compression strategy is thus established by compressing the observation error ±i±i,cisubscript±superscriptsubscript±{ _ i - x _ i , c ^ i between the solution state ±isubscript±{ _ i and the corresponding observer state ±i,cisuperscriptsubscript± _ i , c ^ i of agent iii, as message for communication with neighbors. We are ready to propose the following theorem for Flow (8). Let Assumption 2 hold, then for the DC-OC flow (8) with the ST compressor ‚‚ in continuous time, there exists Î±>0superscript¼0 ^ > 0 such that for all Î±¤Î±¼superscript¼ ¤ Î± ^ , there holds for some Î³>0¾0 > 0. ¡¡ A rigorous proof of Theorem 2 is presented in Appendix C. Intuitively, from the perspective of control system, we stress that the corresponding system (8) can be regarded as an interconnection of two subsystems: ±isubscript± _ i -subsystem and ±j,cisubscriptsuperscript±{ ^ i _ j , c -subsystem, with Î±¼ a low gain that is tuned to be small such that the supply functions of the two interconnected subsystems satisfy some small-gain condition for closed-loop exponential stability [34, Theorem 5.6]. In this subsection, we aim to present distributed compressed Prime-Dual optimization flow for the problem (1) by applying the SST compressor to directly compress the communication message in the conventional Prime-Dual optimization algorithms, as in Subsection 3.1. The proposed distributed Prime-Dual flow with direct compression takes the form where the initial condition i=1n¯i(0)=dsuperscriptsubscript1subscript¯0subscript0 _ i = 1 ^ n v _ i ( 0 ) = 0 _ d . We are ready to propose the following theorem for Flow (9). Let Assumption 1 and 2 hold, and ‚‚ be a SST compressor in continuous time, who also satisfies (7) with some Î>0¿0 > 0. Then there exists some Î²,Î·>0½‚0 , Î· > 0 such that the flow (9) converges to the optimal solution ssuperscript s^{ ^ exponentially, i.e., for some Î³>0¾0 > 0. ¡¡ In this subsection, we propose compressed distributed Prime-Dual optimization flow by applying the ST compressor to compress the communication message in the conventional Prime-Dual optimization algorithms, based on distributed observer-based compressed consensus (8) in Subsection 3.2. The proposed distributed Prime-Dual flow in continuos form with observer-based compression takes the form where the initial condition is i=1n¯i(0)=dsuperscriptsubscript1subscript¯0subscript0 _ i = 1 ^ n v _ i ( 0 ) = 0 _ d and for every iii, ±j,ci(0)=±i,cj²(0),j,j²Vformulae-sequencesubscriptsuperscript±0subscriptsuperscript±superscript²0for-allsuperscript²V j,j^{ ^ i _ j , c ( 0 ) = x ^ j ^ ² _ i , c ( 0 ) , j , j ^ ² roman_V.. We are ready to propose the following theorem for Flow (10). Let Assumptions 1 and 2 hold, and ‚‚ be a ST compressor in continuous time. Then there exist Î±,Î²,Î·>0¼½‚0 , Î² , Î· > 0 such that the flow (10) converges to the optimal solution ssuperscript s^{ ^ exponentially, i.e., for some Î³>0¾0 > 0. ¡¡ The theorems in this section only discuss the case of strongly convex object functions and demonstrate the exponential convergence of the flows. Since the convergence of the system is composed of the convergence of the Prime-Dual flow and the convergence to optimal solution of the compressor, and the Prime-Dual flow achieves asymptotic convergence to optimal solution for convex functions, it is not difficult to infer that both (9) and (10), and their discretization forms in next section, can achieve asymptotic convergence to optimal solution for convex functions. In practice, algorithms are always implemented in a discrete time form. In the following, we discretize Flow (9) based on Euler method, and derive the following discrete-time solver where the initial condition i=1n¯i(t)=dsuperscriptsubscript1subscript¯¡subscript0 _ i = 1 ^ n v _ i ( t ) = 0 _ d . Let Assumptions 1 and 2 hold, and ‚‚ be a SST compressor in discrete time, who also satisfies (7) with some Î>0¿0 > 0. Then there exists some Îº,Îº0,Î²,Î·>0…subscript…0½‚0 , Îº _ 0 , Î² , Î· > 0 such that DPD-DC (11) converges to the optimal solution ssuperscript s^{ ^ linearly, i.e., for some Î³(0,1)¾01 ( 0 , 1 ). ¡¡ Next, we discretize Flow (10) based on Euler method, yielding the following discrete time algorithm where the initial condition is i=1n¯i(t)=dsuperscriptsubscript1subscript¯¡subscript0 _ i = 1 ^ n v _ i ( t ) = 0 _ d and for every iii, ±i,cj(0)=±i,cj²(0),j,j²Vformulae-sequencesubscriptsuperscript±0subscriptsuperscript±superscript²0for-allsuperscript²V j,j^{ ^ j _ i , c ( 0 ) = x ^ j ^ ² _ i , c ( 0 ) , j , j ^ ² roman_V. Let Assumption 1 and 2 hold, and ‚‚ be a ST compressor in discrete time with some Îº0>0subscript…00 _ 0 > 0. Then there exists some Îº,Î²,Î·>0…½‚0 , Î² , Î· > 0 such that DPD-OC (12) converges to the optimal solution ssuperscript s^{ ^ linearly. ¡¡ When the ST compressor is enhanced to a SST compressor, it can be directly applied to distributed optimization algorithms as shown in DPD-DC (11), where the j=1n‹ij‚(±i,t)subscriptsuperscript1subscript‹‚subscript±¡- ^ n _ j = 1 L _ i j C ( x _ i , t ) term ensures the convergence of system consensus. However, for the ST compressor, we need to ensure the convergence of the system through the method of compressing differences by observer, as shown in DPD-OC (12). Therefore, we consider the case of using the SST compressor for DPD-OC (12) and add the j=1n‹ij‚(±i,t)subscriptsuperscript1subscript‹‚subscript±¡- ^ n _ j = 1 L _ i j C ( x _ i , t ) term to accelerate the convergence of the system consensus. We obtain the following solver where kp¥0subscript0k_{p} 0k _ p ¥ 0 is the proportional gain and the initial condition is i=1n¯i(t)=dsuperscriptsubscript1subscript¯¡subscript0 _ i = 1 ^ n v _ i ( t ) = 0 _ d and for every iii, ±i,c1j(0)=±i,c1j²(0),j,j²Vformulae-sequencesubscriptsuperscript±10subscriptsuperscript±superscript²10for-allsuperscript²V j,j^{ } ^ j _ i , c 1 ( 0 ) = x ^ j ^ ² _ i , c 1 ( 0 ) , j , j ^ ² roman_V. Let Assumption 1 and 2 hold, and ‚‚ be a SST compressor in discrete time, who also satisfies (7) with some Î¿ Then there exists some kp,Îº,Îº0,Î²,Î·>0subscript…subscript…0½‚0k_{p}, _ p , Îº , Îº _ 0 , Î² , Î· > 0 such that A-DPD-DC (13) converges to the optimal solution ssuperscript s^{ ^ linearly. ¡¡ The proof of Lemma 1 can be obtained with the proof of Theorem 5 in Appendix F and Theorem 6 in Appendix G. In numerical simulations, we can observe that under certain parameter conditions, this algorithm indeed converges noticeably faster than other algorithms. Observe (9) and (10), and we can see that they apply different compression methods to (2). There are many different compression methods documented in the literature. For example, let us discuss the combination of our ST compressor with a commonly used method [16]. This method introduces a distributed filter and a distributed integrator and compresses the state errors. The proposed distributed Prime-Dual flow with error state compression (DPD-ESC) takes the form. where Îº… Îº0subscript…0 _ 0 , Î²,Î·>0½‚0 , Î· > 0 are parameters to be fixed. Similar theorems as those in previous context can be proved. For interested readers, the validity of the continuous form of the Algorithm above is proved in previous work [35]. It should be noticed that many literature on compressor assumption take into account the presence of randomness. Therefore, we extend the ST compressor to randomness and research its effectiveness in applications. In this section, we study the randomization of the ST compressor and the application of DPD-OC as a example. Introduce randomness to Definition 1, we obtain the definition of Stochastic Spatio-Temporal (StST) Compressor, with focus on discrete time. Given a linearly mean-square bounded mapping ‚:dÃ+†d:‚†superscriptsubscriptsuperscript : R ^ d Ã R _ + † R ^ d , i.e., there exists a Lc>0subscript¿0L_{c}>0L _ c > 0 such that ”¼‚(±e,t)2¤Lc2±e2”¼superscriptnorm‚subscript±¡2subscriptsuperscript¿2superscriptnormsubscript±2 L^{2}_{c} ^{2}E ¥ C ( x _ e , t ) ¥ ^ 2 ¤ L ^ 2 _ c ¥ x _ e ¥ ^ 2 for all ±edsubscript±superscript _ e R ^ d and any t+¡subscriptt R _ + . Then, ‚‚ is said to be a StST compressor, if the induced non-autonomous system ±e(t+1)=±e(t)Îº0‚(±e,t)subscript±¡1subscript±¡subscript…0‚subscript±¡ _ e ( t + 1 ) = x _ e ( t ) - Îº _ 0 C ( x _ e , t ) is uniformly globally exponentially stable at the origin in the mean-square sense111See [36] for the definition of mean square convergence, for some stepsize Îº0>0subscript…00 _ 0 > 0. ¡¡ The ST compressor is a special case of the StST compressor. Moreover, some compressor assumptions in literature belongs to the StST compressor. The stochastic contractive compressor ‚3:d†d:subscript‚3†superscriptsuperscript _ 3 : R ^ d † R ^ d satisfies for some †(0,1]01 ( 0 , 1 ] and p>00p>0p > 0. By [16], the followings are specific examples of ‚3subscript‚3 _ 3 : Unbiased l™ll-bits quantizer [37] where ¯¯” is a random perturbation vector uniformly sampled from [0,1]dsuperscript01[0,1]^{d}[ 0 , 1 ] ^ d . Compressor ‚3subscript‚3 _ 3 belongs to the StST compressor. ¡¡ The proof of Proposition 2 is similar to that of Proposition b). in Appendix A and is omitted for simplicity. We apply the StST compressor to Algorithm (12) and propose the following theorem for DPD-OC. Let Assumption 1 and 2 hold, and ‚‚ be a StST compressor with some Îº0>0subscript…00 _ 0 > 0. Then for Îº,Î²,Î·>0…½‚0 , Î² , Î· > 0, the mean square of ±i(t)subscript±¡ _ i ( t ) in the DPD-OC algorithm (12) converges to the optimal solution ssuperscript s^{ ^ linearly. ¡¡ In this subsection, we will verify that the compressors mentioned in this paper, ‚1asubscript‚1 _ 1 a , ‚2asubscript‚2 _ 2 a , ‚2bsubscript‚2 _ 2 b , ‚2csubscript‚2 _ 2 c , ‚3asubscript‚3 _ 3 a , satisfy the core property, i.e. the exponential stability of induced system, of ST compressor. Specifically, we let d=55d=5d = 5, k=22k=2k = 2 for ‚2asubscript‚2 _ 2 a , Î”=1Î”1 = 1 for ‚2csubscript‚2 _ 2 c and l=4™4l=4l = 4 for ‚3asubscript‚3 _ 3 a . The figures respectively demonstrate the exponential convergence system ±Ë™e=‚(±e,t)subscriptË™±‚subscript±¡ x _ e = - C ( x _ e , t ) with different compressors, validating our conclusions. Other properties of the ST compressor can also be easily verified through theoretical analysis. Additionally, the satisfaction of these compressors with the conditions of the ST compressor in discrete form can also be verified through simulation, but it is omitted here. In this subsection, we consider a network of nnn nodes over a circle communication graph and dimension of local state is ddd, where each edge is assigned with the same unit weight and each node holds a local function fi(±i)=12iT±ibi2subscript“subscript±12superscriptnormsubscriptsuperscriptsubscript±subscript2f_{i}( _ i ( x _ i ) = / 1 2 ¥ H ^ T _ i x _ i - b _ i ¥ ^ 2 with some randomly generated idsubscriptsuperscript _ i R ^ d and bisubscriptb_{i} _ i R. Assume that the linear equation x=¥ x = b has a unique solution ssuperscript s^{ ^ , where =[1¦n]Š¤nÃdsuperscriptdelimited-[]subscript1¦subscripttopsuperscript d}H = [ H _ 1 ¦ H _ n ] ^ Š¤ R ^ n Ã d and =[b1¦bn]Š¤dsuperscriptdelimited-[]subscript1¦subscripttopsuperscript = [ b _ 1 ¦ b _ n ] ^ Š¤ R ^ d , then we can conclude that the functions fi(±i)subscript“subscript±f_{i}( _ i ( x _ i ) satisfy Assumption 1 with Î¼>00 > 0 and optimal solution ssuperscript s^{ ^ . Specifically, we let n=1010n=10n = 10, d=55d=5d = 5 and s=[2,4,4,2,3]Tsuperscript superscript24423s^{ ^ = [ 2 , - 4 , - 4 , 2 , - 3 ] ^ T . Next, we will apply different compression methods to the compressors and compare their effects. We use the scalar compressor ‚1asubscript‚1 _ 1 a and greedy sparsifier compressor ‚2asubscript‚2 _ 2 a as examples. In this application, we integrate DPD-DC (11), A-DPD-OC (13), DPD-ESC (14) with ‚1asubscript‚1 _ 1 a , and integrate A-DPD-OC (13), DPD-ESC (14), with ‚2asubscript‚2 _ 2 a . Specifically, we let the parameters kp=10subscript10k_{p}=10k _ p = 10, Îº0=1subscript…01 _ 0 = 1, Îº=0.002…0.002 = 0.002, Î±=1¼1 = 1, Î²=10½10 = 10, Î·=5‚5 = 5. The plot illustrates the sum of squared distances from the current ±i(t)subscript±¡ _ i ( t ) to ssuperscript s^{ ^ , denoted as i=1n±i(t)s2superscriptsubscript1superscriptnormsubscript±¡superscript 2 _ i = 1 ^ n ¥ x _ i ( t ) - s ^ ¥ ^ 2 over time. We also simulated the Prime-Dual algorithm in discrete time without compressor under the same parameters for comparison. Notably, the algorithms exhibits exponential convergence to the optimal solution, verifying the theorems. Furthermore, we can observe that observer-based compression method has significant advantages on convergence rate under the same parameters. Next, we discuss the case where the objective function is convex but not strongly convex, while other settings is same as that in the previous subsection. We take the objective function from [38] as whose optimal solution is s=dsuperscript subscript1s^{ ^ = 1 _ d . The simulate results of DPD-DC (11) and DPD-OC (12) with ‚1asubscript‚1 _ 1 a , as an example, are shown. From the figure, we can see that the above algorithm can achieve asymptotic convergence to the optimal solution for convex function In this paper, we have introduced a type of spatio-temporal compressor that integrates both spatial and temporal characteristics, effectively compresses information by leveraging information from both the time and space domains. This class of compressors has covered several assumptions in literature on compressors. Our proposed compressor has been implemented in the Prime-Dual algorithm by direct compression and observer-based compression. In the future, we will investigate a broader spectrum of compressor types or enhanced algorithms tailored to the characteristics of this compressor, and to have extended its application to more classical distributed optimization algorithms, examining its universality across different algorithms. Proof of a). The proof of the statement in Definition 2 is obvious by recalling [39] that system ±Ë™e=k(t)(t)T±esubscriptË™±¡superscript¡subscript± x _ e = - k ( t ) ( t ) ^ T x _ e is globally exponentially stable at the zero equilibrium for any k>00k>0k > 0 under the condition in continuous time cases, and ±e(t+1)=±e(t)Îº0(t)(t)T±e(t)subscript±¡1subscript±¡subscript…0¡superscript¡subscript±¡{ _ e ( t + 1 ) = x _ e ( t ) - Îº _ 0 ( t ) ( t ) ^ T x _ e ( t ) is globally exponentially stable at the zero equilibrium for any Îº0¤Îº0subscript…0superscriptsubscript…0 _ 0 ¤ Îº _ 0 ^ with some Îº0>0superscriptsubscript…00 _ 0 ^ > 0 under the condition in discrete time cases. The proof of linearly boundedness property can be shown by noting that (t)¡ ( t ) is uniformly bounded. Proof of b). We proceed to show the compressor ‚2(±e)subscript‚2subscript± _ 2 ( x _ e ) satisfying properties Definition 1. Note that the contractive compressor (3) is equivalent to First, we prove that ‚2subscript‚2 _ 2 is the ST compressor in continuous time by proving the system xË™e=‚2(±e,t)subscriptË™¥subscript‚2subscript±¡ x _ e = - C _ 2 ( x _ e , t ) is exponentially stable at the zero equilibrium. By choosing the Lyapunov function Ve(±e)=±e2/psubscriptsubscript±superscriptnormsubscript±2V_{e}( _ e ( x _ e ) = ¥ x _ e ¥ ^ 2 / p and using (16), we have Thus ±esubscript± _ e -system is globally exponentially stable at the zero equilibrium with †>00 > 0. Next, we prove that ‚2subscript‚2 _ 2 is the ST compressor in discrete time by proving the system ±e(t+1)=±e(t)Îº0‚2(±e(t))subscript±¡1subscript±¡subscript…0subscript‚2subscript±¡ t))x _ e ( t + 1 ) = x _ e ( t ) - Îº _ 0 C _ 2 ( x _ e ( t ) ) is exponentially stable at the zero equilibrium with Îº0=1psubscript…01 _ 0 = / 1 p . By (16), Thus ±esubscript± _ e -system is exponentially stable at the zero equilibrium with †(0,1]) ( 0 , 1 ] ). Then, by (16) and using the Young™s inequality, we have where the last inequality is obtained by †(0,1]01 ( 0 , 1 ]. Thus the linearly boundedness property is proved with Lc=2p>0subscript¿20L_{c}=2p>0L _ c = 2 p > 0. This completes the proof. Flow (5) can be written in tight form as where ±:=[±1T,¦,±nT]Tassign±superscriptsuperscriptsubscript±1¦superscriptsubscript± := [ x _ 1 ^ T , ¦ , x _ n ^ T ] ^ T , ±c:=[±1,cT,¦,±n,cT]Tassignsubscript±superscriptsuperscriptsubscript±1¦superscriptsubscript± _ c := [ x _ 1 , c ^ T , ¦ , x _ n , c ^ T ] ^ T and :=‹Šdassigntensor-product‹subscript := L Š I _ d . We decompose ±± by defining ±‚:=•ŠT±=[±‚,1T,¦,±‚,n1T]Tassignsubscript±perpendicular-tosuperscript•Š±superscriptsubscriptsuperscript±perpendicular-to1¦subscriptsuperscript±perpendicular-to1 _ ‚ := S ^ T x = [ x ^ T _ ‚ , 1 , ¦ , x ^ T _ ‚ , n - 1 ] ^ T and ±¥:=•T±assignsubscript±parallel-tosuperscript•± _ ¥ := I ^ T x, where •:=1nnŠdassign•tensor-product1subscript1subscript := / 1 square-root n 1 _ n Š I _ d . We can obtain that ±Ë™¥=dsubscriptË™±parallel-tosubscript0 x _ ¥ = 0 _ d in (18) by the fact Then with the fact it can be noticed that we can prove ±isubscript± _ i converges to average consensus by proving ±‚subscript±perpendicular-to _ ‚ converges to zero equilibrium. We can obtain the derivation of ±‚subscript±perpendicular-to _ ‚ along with time . Define the Lypanuov function of (21) V(±‚,t):=Ve(±‚,t)assignsubscript±perpendicular-to¡subscriptsubscript±perpendicular-to¡V( ( x _ ‚ , t ) := V _ e ( x _ ‚ , t ). which is defined in (6), then we have where the inequality is obtained by the fact If Î¤c3c4Î»n¿subscript3subscript4subscript† ¤ / c _ 3 c _ 4 Î» _ n , VË™Ë™ V is negative definite. With (6), we have then ±‚(t)2=ª(eÎ³t)superscriptnormsubscript±perpendicular-to¡2ªsuperscript¾¡ t})¥ x _ ‚ ( t ) ¥ ^ 2 = caligraphic_O ( e ^ - Î³ t ), where Î³=c3c4ÎÎ»nc1¾subscript3subscript4¿subscript†subscript1 = / c _ 3 - c _ 4 Î Î» _ n c _ 1 . Thus the theorem is proved. From Flow (8) and its initial condition, we can obtain that for every iVVi roman_V, ±j,ci(0)=±i,cj²(0),j,j²Vformulae-sequencesubscriptsuperscript±0subscriptsuperscript±superscript²0for-allsuperscript²V j,j^{ ^ i _ j , c ( 0 ) = x ^ j ^ ² _ i , c ( 0 ) , j , j ^ ² roman_V, i.e. the stored value of ±isubscript± _ i is same in every node. Thus the stored value of each node can be written as ±c:=[±1,cT,¦,±n,cT]Tassignsubscript±superscriptsuperscriptsubscript±1¦superscriptsubscript± _ c := [ x _ 1 , c ^ T , ¦ , x _ n , c ^ T ] ^ T . Then Flow (8) can be written in a tight form as where (±±c,t):=[‚T(±1±1,c,t)¦‚T(±n±n,c,t)]T.assign±subscript±¡superscriptdelimited-[]superscript‚subscript±1subscript±1¡¦superscript‚subscript±subscript±¡ ( x - x _ c , t ) := [ C ^ T ( x _ 1 - x _ 1 , c , t ) ¦ C ^ T ( x _ n - x _ n , c , t ) ] ^ T . Similarly, we decompose ±± by defining ±‚:=•ŠT±assignsubscript±perpendicular-tosuperscript•Š± _ ‚ := S ^ T x and ±¥:=•T±assignsubscript±parallel-tosuperscript•± _ ¥ := I ^ T x, still ±Ë™¥=dsubscriptË™±parallel-tosubscript0 x _ ¥ = 0 _ d and we will prove the convergence of ±‚subscript±perpendicular-to _ ‚ . By (23), we have We choose V1(±‚):=12±‚2assignsubscript1subscript±perpendicular-to12superscriptnormsubscript±perpendicular-to2V_{1}( _ 1 ( x _ ‚ ) := / 1 2 ¥ x _ ‚ ¥ ^ 2 , then we have As ±Ë™e=‚(±e,t)subscriptË™±‚subscript±¡ x _ e = - C ( x _ e , t ) is exponentially convergent at the zero equilibrium, where ±edsubscript±superscript _ e R ^ d , then there exists a Lyapunov function Ve(±e,t):dÃ+†:subscriptsubscript±¡†superscriptsubscriptV_{e}( _ e ( x _ e , t ) : R ^ d Ã R _ + † R which satisfies for some c1,c2,c3,c4>0subscript1subscript2subscript3subscript40c_{1},c_{2},c_{3},c_{4}>0c _ 1 , c _ 2 , c _ 3 , c _ 4 > 0. We choose V2(±±c,t):=i=1nVe(±i±i,c,t)assignsubscript2±subscript±¡superscriptsubscript1subscriptsubscript±subscript±¡V_{2}( {x}_{i,c},t)V _ 2 ( x - x _ c , t ) := _ i = 1 ^ n V _ e ( x _ i - x _ i , c , t ), then we have where the first inequality is obtained by (26) and the second inequality is obtained by the fact Define the Lypanuov function of (24) V:=0V1+V2assignsubscript0subscript1subscript2V:= := _ 0 V _ 1 + V _ 2 with 0=4Î»nc4nÎ»2subscript04subscript†subscript4subscript†2 _ 0 = / 4 Î» _ n c _ 4 square-root n Î» _ 2 . If we let Î±¤min{2c39Î»nc4n,2c33Î»n}¼min2subscript39subscript†subscript42subscript33subscript† {3 ¤ roman_min { / 2 c _ 3 9 Î» _ n c _ 4 square-root n , / 2 c _ 3 3 Î» _ n } then with (25) and (27), we have With (26), we have then ±‚(t)2=ª(eÎ³t)superscriptnormsubscript±perpendicular-to¡2ªsuperscript¾¡ t})¥ x _ ‚ ( t ) ¥ ^ 2 = caligraphic_O ( e ^ - Î³ t ), where Î³=min{Î±Î»22,c33c1}¾min¼subscript†22subscript33subscript1 = roman_min { / Î± Î» _ 2 2 , / c _ 3 3 c _ 1 }. The theorem is proved. Flow (9) can be written in a tight form as where ¯:=[¯1T,¦¯nT]Tassign¯superscriptsuperscriptsubscript¯1¦superscriptsubscript¯ := [ v _ 1 ^ T , ¦ v _ n ^ T ] ^ T and …(±):=[f1T(±1)¦fnT(±n)]Tassign…±superscriptdelimited-[]superscriptsubscript“1subscript±1¦superscriptsubscript“subscript± f_{1}^{T}( f_{n}^{T}(% ( x ) := [ f _ 1 ^ T ( x _ 1 ) ¦ f _ n ^ T ( x _ n ) ] ^ T . As f(x)“¥f(x)f ( x ) is strongly convex, there exists a unique sdsuperscript superscripts^{ ^ R ^ d that f(s)=d“superscript subscript0 f(s^{ f ( s ^ ) = 0 _ d , i.e. •T…(¬)=dsuperscript•…¬subscript0 ^ T F ( s ) = 0 _ d , where ¬:=n•sassign¬•superscript := square-root n I s ^ . It can be noticed that ±=¬superscript±¬ ^ = s, and ¯=Î·…(±)Î²superscript¯‚…superscript±½ ^ = - / Î· F ( x ^ ) Î² is the equilibrium point of system (30). We introduce the state error by defining ±¯:=±¬assign¯±±¬ x := x - s, ¯¯:=¯¯assign¯¯¯superscript¯ v := v - v ^ . Taking the time derivative of the state errors along (30) yields where …~(±¯):=…(±¯+¬)…(¬)assign~…¯±…¯±¬…¬ F ( over¯ x ) := F ( over¯ x + s ) - F ( s ). We decompose ±¯¯± x and ¯¯¯¯ v by defining ±¯‚:=•ŠT±¯assignsubscript¯±perpendicular-tosuperscript•Š¯± x _ ‚ := S ^ T over¯ x , ±¯¥:=•T±¯assignsubscript¯±parallel-tosuperscript•¯± x _ ¥ := I ^ T over¯ x , ¯¯‚:=•ŠT¯¯assignsubscript¯¯perpendicular-tosuperscript•Š¯¯ v _ ‚ := S ^ T over¯ v and ¯¯¥:=•T¯¯assignsubscript¯¯parallel-tosuperscript•¯¯ v _ ¥ := I ^ T over¯ v . It can be noticed that the convergence of ±¯¯± x and ¯¯¯¯ v can be shown as if ±¯¥subscript¯±parallel-to x _ ¥ , ±¯‚subscript¯±perpendicular-to x _ ‚ , ¯¯‚subscript¯¯perpendicular-to v _ ‚ and ¯¯‚subscript¯¯perpendicular-to v _ ‚ converge to the zero equilibrium by (20). By the initial condition we know that •T¯(0)=dsuperscript•¯0subscript0 ^ T v ( 0 ) = 0 _ d . With system (30) and (19), we conclude that •T¯(t)=dsuperscript•¯¡subscript0 ^ T v ( t ) = 0 _ d , then By (19) and (32), the system (31) becomes where the fact (•±¯+¬,t)=nd•¯±¬¡subscript0 }_{nd}caligraphic_L caligraphic_C ( I over¯ x + s , t ) = 0 _ n d is used. Let ³:=1Î²¯¯‚+±¯‚assign³1½subscript¯¯perpendicular-tosubscript¯±perpendicular-to _{ := / 1 Î² over¯ v _ ‚ + over¯ x _ ‚ . The system (33) then becomes It can be noticed that the exponential stability of the system (33)italic-(33italic-) ) and the system (34)italic-(34italic-) ) are equal. Define V1(±¯‚,³)=12(±¯‚2+³2)subscript1subscript¯±perpendicular-to³12superscriptnormsubscript¯±perpendicular-to2superscriptnorm³2V_{1}( _ 1 ( over¯ x _ ‚ , z ) = / 1 2 ( ¥ over¯ x _ ‚ ¥ ^ 2 + ¥ z ¥ ^ 2 ), then we have where the second inequality is obtained by the fact from property of ‚‚ and . Define V2(±¯‚,t):=Ve(±¯‚,t)assignsubscript2subscript¯±perpendicular-to¡subscriptsubscript¯±perpendicular-to¡V_{2}( _ 2 ( over¯ x _ ‚ , t ) := V _ e ( over¯ x _ ‚ , t ), where VesubscriptV_{e}V _ e is defined in (6). then we have where we let Î¤c3c4Î»n¿subscript3subscript4subscript† ¤ / c _ 3 c _ 4 Î» _ n and then c3²:=c3c4ÎÎ»n>0assignsubscriptsuperscript²3subscript3subscript4¿subscript†0c^{ ^ ² _ 3 := c _ 3 - c _ 4 Î Î» _ n > 0, and the last inequality is obtained by (22) with the fact •ŠT±=•ŠT±¯=±¯‚superscript•Š±superscript•Š¯±subscript¯±perpendicular-to {x}}_{ ^ T x = S ^ T over¯ x = over¯ x _ ‚ , (37) and Young™s Inequality, where r>00r>0r > 0 is a parameter which will be determined later. Define V3(±¯¥):=12±¯¥2assignsubscript3subscript¯±parallel-to12superscriptnormsubscript¯±parallel-to2V_{3}( _ 3 ( over¯ x _ ¥ ) := / 1 2 ¥ over¯ x _ ¥ ¥ ^ 2 , as f(x)“¥f(x)f ( x ) is Î¼ convex, we have where the last inequality is obtained by property i). and ii). of f(x)“¥f(x)f ( x ) with Î¼n:=Î¼nassignsubscript _ n := / Î¼ n . We introduce some positive parameters that have nothing to do with Î²½ rrr and Î·‚ We define the Lyapunov functions of system (34) V:=V1+0V2+1V3assignsubscript1subscript0subscript2subscript1subscript3V:=V_{1}+ := V _ 1 + _ 0 V _ 2 + _ 1 V _ 3 . It is easy to prove that VVV is positive definite. In fact First, let™s impose some prime limit, By (35), (38), (39), (41), we can derive VË™Ë™ V is negative when we choose r=min{14Î¾3,1}min14subscript31r= = roman_min { / 1 4 Î¾ _ 3 , 1 }, Î²2¤min{0c3²8Î¾1,0c3²r8Î¾2}superscript½2minsubscript0subscriptsuperscript²38subscript1subscript0subscriptsuperscript²38subscript2 {0}c^{ ^ 2 ¤ roman_min { / _ 0 c ^ ² _ 3 8 Î¾ _ 1 , / _ 0 c ^ ² _ 3 r 8 Î¾ _ 2 }, Î·¤min{Î²2,Î²24Î¾4}‚minsuperscript½2superscript½24subscript4 ¤ roman_min { Î² ^ 2 , / Î² ^ 2 4 Î¾ _ 4 }. With (40), we have With the definition of VVV, we derive ±¯(t)2=ª(eÎ³t)superscriptnorm¯±¡2ªsuperscript¾¡ t})¥ over¯ x ( t ) ¥ ^ 2 = caligraphic_O ( e ^ - Î³ t ). With the definition ±¯=±¬¯±±¬ x = x - s before, we know that ±i(t)subscript±¡ _ i ( t ) in Flow (9) converge exponentially to the optimal solution ssuperscript s^{ ^ with the SST compressor. As analyzed in Appendix C, Flow (10) satisfies that for every iVVi roman_V, ±i,cj(0)=±i,cj²(0),j,j²Vformulae-sequencesubscriptsuperscript±0subscriptsuperscript±superscript²0for-allsuperscript²V j,j^{ ^ j _ i , c ( 0 ) = x ^ j ^ ² _ i , c ( 0 ) , j , j ^ ² roman_V. Then Flow (10) can be written as We carry out a similar analysis process as in Appendix D with ³:=Î±Î²¯¯‚+±¯‚assign³¼½subscript¯¯perpendicular-tosubscript¯±perpendicular-to := / Î± Î² over¯ v _ ‚ + over¯ x _ ‚ , and the system (42) becomes where ±¯c:=±c¬assignsubscript¯±subscript±¬ x _ c := x _ c - s and Î²Î±2:=Î²2/Î±assignsubscriptsuperscript½2¼superscript½2¼ ^ 2 _ Î± := Î² ^ 2 / Î±. Define V1(±¯‚,³)=12(±¯‚2+³2)subscript1subscript¯±perpendicular-to³12superscriptnormsubscript¯±perpendicular-to2superscriptnorm³2V_{1}( _ 1 ( over¯ x _ ‚ , z ) = / 1 2 ( ¥ over¯ x _ ‚ ¥ ^ 2 + ¥ z ¥ ^ 2 ), we have where the second inequality is obtained by the fact For V2(±¯±¯c,t)subscript2¯±subscript¯±¡V_{2}( _ 2 ( over¯ x - over¯ x _ c , t ), which is defined in Appendix C. then we have where the first inequality is obtained by ¯¯=•Š¯¯‚¯¯•Šsubscript¯¯perpendicular-to v = S over¯ v _ ‚ and (26), and the last inequality is obtained by (37), the fact and Young™s Inequality, where r>00r>0r > 0 is a parameter which will be determined later. Define V3(±¯¥):=12±¯¥2assignsubscript3subscript¯±parallel-to12superscriptnormsubscript¯±parallel-to2V_{3}( _ 3 ( over¯ x _ ¥ ) := / 1 2 ¥ over¯ x _ ¥ ¥ ^ 2 , then (39) still holds. We introduce some positive parameters that have nothing to do with Î±¼ Î²½ rrr and Î·‚ We define the Lyapunov functions of system (43) V:=V1+V2+1V3assignsubscript1subscript2subscript1subscript3V:=V_{1}+V_{2}+ := V _ 1 + V _ 2 + _ 1 V _ 3 . It is easy to prove that VVV is positive definite. In fact First, let™s impose some prime limit, By (44), (46), (39) (49), we can derive VË™Ë™ V is negative when we choose r=min[Î¾14Î¾3,14Î¾5,1]minsubscript14subscript314subscript51r= = roman_min [ / Î¾ _ 1 4 Î¾ _ 3 , / 1 4 Î¾ _ 5 , 1 ], Î±¤min[c3r4Î¾6,c34Î¾7]¼minsubscript34subscript6subscript34subscript7 ¤ roman_min [ / c _ 3 r 4 Î¾ _ 6 , / c _ 3 4 Î¾ _ 7 ], Î²2¤min[Î±2,Î¾1Î±24Î¾2]superscript½2minsuperscript¼2subscript1superscript¼24subscript2 ^ 2 ¤ roman_min [ Î± ^ 2 , / Î¾ _ 1 Î± ^ 2 4 Î¾ _ 2 ], Î·¤min[Î²Î±2,14Î¾4]‚minsuperscriptsubscript½¼214subscript4 ¤ roman_min [ Î² _ Î± ^ 2 , / 1 4 Î¾ _ 4 ]. With (48), we have With the definition of VVV, we derive ±¯(t)2=ª(eÎ³t)superscriptnorm¯±¡2ªsuperscript¾¡ t})¥ over¯ x ( t ) ¥ ^ 2 = caligraphic_O ( e ^ - Î³ t ). With the definition ±¯=±¬¯±±¬ x = x - s before, we know that ±i(t)subscript±¡ _ i ( t ) in Flow (10) converge exponentially to the optimal solution ssuperscript s^{ ^ with the ST compressor. Flow (11) can be written in a tight form as Similar to the proof of continuous time form in Appendix D. We introduce the state error by defining ±¯:=±¬assign¯±±¬ x := x - s, ¯¯:=¯¯assign¯¯¯superscript¯ v := v - v ^ . Then decompose ±¯¯± x and ¯¯¯¯ v by defining ±¯‚:=•ŠT±¯assignsubscript¯±perpendicular-tosuperscript•Š¯± x _ ‚ := S ^ T over¯ x , ±¯¥:=•T±¯assignsubscript¯±parallel-tosuperscript•¯± x _ ¥ := I ^ T over¯ x , ¯¯‚:=•ŠT¯¯assignsubscript¯¯perpendicular-tosuperscript•Š¯¯ v _ ‚ := S ^ T over¯ v and ¯¯¥:=•T¯¯assignsubscript¯¯parallel-tosuperscript•¯¯ v _ ¥ := I ^ T over¯ v . The convergence of ±¯¯± x and ¯¯¯¯ v can be shown as if ±¯¥subscript¯±parallel-to x _ ¥ , ±¯‚subscript¯±perpendicular-to x _ ‚ , ¯¯‚subscript¯¯perpendicular-to v _ ‚ and ¯¯‚subscript¯¯perpendicular-to v _ ‚ converge to the zero equilibrium. Also we can conclude (32). Let ³:=1Î²¯¯‚+±¯‚assign³1½subscript¯¯perpendicular-tosubscript¯±perpendicular-to _{ := / 1 Î² over¯ v _ ‚ + over¯ x _ ‚ , with (34), the system (50) becomes It can be noticed that the exponential stability of the system (50)italic-(50italic-) ) and the system (51)italic-(51italic-) ) are equal. Define V1,t(±¯‚,³)=12(±¯‚2+³2)subscript1¡subscript¯±perpendicular-to³12superscriptnormsubscript¯±perpendicular-to2superscriptnorm³2V_{1,t}( _ 1 , t ( over¯ x _ ‚ , z ) = / 1 2 ( ¥ over¯ x _ ‚ ¥ ^ 2 + ¥ z ¥ ^ 2 ), then where the second inequality is obtained by (36) and (37). Before we introduce the second Lyapunov function, we will show that the following system where ²e(n1)dsubscript²superscript1 _ e R ^ ( n - 1 ) d , ±endsubscript±superscript _ e R ^ n d and ²e=•ŠT±esubscript²superscript•Šsubscript±{ _ e = S ^ T x _ e , achieves exponential convergence at the zero equilibrium for some positive Îº0,Îsubscript…0¿ _ 0 , Î in Theorem 5. By definition of ‚(±e,t)‚subscript±¡ ( x _ e , t ), it is easy to find the following system exponential convergence at the zero equilibrium if Îº0¤Îº0/Î»nsubscript…0superscriptsubscript…0subscript† _ 0 ¤ Îº _ 0 ^ / Î» _ n , Then there exists positive constants C¶CC, Î³D<1subscript¾·1 _ D < 1, for any t¡tt and N•+subscript•N N _ + , the solution satisfies Assume •tt+T(²e(t))superscriptsubscriptitalic-•¡¡subscript²¡ _ t ^ t + T ( y _ e ( t ) ) is the state of system ²e(t+1)=²e(t)Îº0Î(²e(t),t)subscript²¡1subscript²¡subscript…0Îsuperscriptsubscript²¡¡ {y}_{e}(t),t)y _ e ( t + 1 ) = y _ e ( t ) - Îº _ 0 roman_Î caligraphic_C ^ - ( y _ e ( t ) , t ) in t+N¡t+Nt + N moment with the state in t¡tt moment is ²e(t)subscript²¡ _ e ( t ). It is easy to verified that for any ²(n1)d²superscript1 R ^ ( n - 1 ) d and some L•>0subscript¿italic-•0L_{ _ • > 0 by property of compressor ‚‚ We find Lyapunov function Ve(²e,t)=j=0N1•tt+j(²e)2subscriptsubscript²¡superscriptsubscript01superscriptnormsuperscriptsubscriptitalic-•¡¡subscript²2V_{e}( _ e ( y _ e , t ) = _ j = 0 ^ N - 1 ¥ • _ t ^ t + j ( y _ e ) ¥ ^ 2 satisfies for c1=1,c2=NL•formulae-sequencesubscript11subscript2subscript¿italic-•c_{1}=1,c_{2}=NL_{ _ 1 = 1 , c _ 2 = N L _ • . Moreover, we have We choose a N•+subscript•N N _ + large enough and then c3=1CÎ³DN>0subscript31¶superscriptsubscript¾·0c_{3}=1-C _ 3 = 1 - C Î³ _ D ^ N > 0, i. e. Besides, for Î=2+2Lc2Îº02Î»n2>0ƒ22superscriptsubscript¿2superscriptsubscript…02superscriptsubscript†20 = 2 + 2 L _ c ^ 2 Îº _ 0 ^ 2 Î» _ n ^ 2 > 0 by property of ‚‚ For system (53), we apply Lyapunov function Ve(²e,t)subscriptsubscript²¡V_{e}( _ e ( y _ e , t ) and obtain the difference of Ve(²e,t)subscriptsubscript²¡V_{e}( _ e ( y _ e , t ) for c4:=NL•Îassignsubscript4subscript¿italic-•ƒc_{4}:=NL_{ _ 4 := N L _ • Î, where the first inequality is obtained by (LABEL:eq:DPD.a.Vec3) and the second inequality is obtained by (7). It is obvious that for that the difference of Ve(²e,t)subscriptsubscript²¡V_{e}( _ e ( y _ e , t ) is negative definite with c3²:=c32c4Îº0Î»nÎc4Îº02Î»n2Î2>0assignsuperscriptsubscript3²subscript32subscript4subscript…0subscript†¿subscript4superscriptsubscript…02superscriptsubscript†2superscript¿20c_{3}^{ _ 3 ^ ² := c _ 3 - 2 c _ 4 Îº _ 0 Î» _ n Î - c _ 4 Îº _ 0 ^ 2 Î» _ n ^ 2 Î ^ 2 > 0, thus system (53) achieves exponential convergence at the zero equilibrium. Next we continue to choose Lyapunov function by defining V2,t:=Ve(±¯‚,t)assignsubscript2¡subscriptsubscript¯±perpendicular-to¡V_{2,t}:=V_{e}( _ 2 , t := V _ e ( over¯ x _ ‚ , t ), then where the second inequality is obtained by (56) and (LABEL:eq:DPD.a.Ve'), and the last inequality is obtained by (37) and Young™s Inequality, where r>00r>0r > 0 is a parameter which will be determined later. Define V3,t(±¯¥):=12±¯¥2assignsubscript3¡subscript¯±parallel-to12superscriptnormsubscript¯±parallel-to2V_{3,t}( _{ _ 3 , t ( over¯ x _ ¥ ) := / 1 2 ¥ over¯ x _ ¥ ¥ ^ 2 , with (39), we have where the second equality is obtained by (37) and the first inequality is obtained by (39). We introduce some parameters 0subscript0 _ 0 , 1subscript1 _ 1 , Î¾1subscript1 _ 1 , Î¾2subscript2 _ 2 ¦>0absent0>0> 0 that have nothing to do with Î²½ rrr and Î·‚ and some parameters Î¶1subscript1 _ 1 , Î¶2subscript2 _ 2 ¦>0absent0>0> 0. We define the Lyapunov functions of system (51) Vt:=V1,t+0V2,t+1V3,tassignsubscript¡subscript1¡subscript0subscript2¡subscript1subscript3¡V_{t}:=V_{1,t}+ _ t := V _ 1 , t + _ 0 V _ 2 , t + _ 1 V _ 3 , t . It is easy to prove that VVV is positive definite. In fact First, let™s impose prime limit (41). By (52), (58), (59), (41) we can derive Î”VtÎ”subscript¡ V_{t}roman_Î” V _ t is negative definite when we choose r=min{14Î¾3,1}min14subscript31r= = roman_min { / 1 4 Î¾ _ 3 , 1 }, Î²2¤min{0c3²8Î¾1,0c3²r8Î¾2}superscript½2minsubscript0subscriptsuperscript²38subscript1subscript0subscriptsuperscript²38subscript2 {0}c^{ ^ 2 ¤ roman_min { / _ 0 c ^ ² _ 3 8 Î¾ _ 1 , / _ 0 c ^ ² _ 3 r 8 Î¾ _ 2 }, Î·¤min{Î²2,Î²24Î¾4}‚minsuperscript½2superscript½24subscript4 ¤ roman_min { Î² ^ 2 , / Î² ^ 2 4 Î¾ _ 4 } and Îº¤Îº1:=12min{0c3²4Î¶1,Î²22Î¶2,Î·Î¼n14Î¶3,1}…subscript…1assign12minsubscript0subscriptsuperscript²34subscript1superscript½22subscript2‚subscriptsubscript14subscript31 ,1 ¤ Îº _ 1 := / 1 2 roman_min { / _ 0 c ^ ² _ 3 4 Î¶ _ 1 , / Î² ^ 2 2 Î¶ _ 2 , Î· / Î¼ _ n _ 1 4 Î¶ _ 3 , 1 }. With (60), then Let Îº2:=2/min{0c3²2(1+20c1),Î²2,Î·Î¼n2}assignsubscript…22minsubscript0subscriptsuperscript²3212subscript0subscript1superscript½2‚subscript2 , _ 2 := 2 / roman_min { / _ 0 c ^ ² _ 3 2 ( 1 + 2 _ 0 c _ 1 ) , Î² ^ 2 , Î· / Î¼ _ n 2 }. When Îº¤min{Îº1,Îº2}…minsubscript…1subscript…2 ¤ roman_min { Îº _ 1 , Îº _ 2 }, we can derive for Î³(0,1)¾01 ( 0 , 1 ) , Vt=ª((1Î³)t)subscript¡ªsuperscript1¾¡V_{t}= _ t = caligraphic_O ( ( 1 - Î³ ) ^ t ). With the definition of Vtsubscript¡V_{t}V _ t , we derive ±¯(t)2=ª((1Î³)t)superscriptnorm¯±¡2ªsuperscript1¾¡ over¯ x ( t ) ¥ ^ 2 = caligraphic_O ( ( 1 - Î³ ) ^ t ). With the definition ±¯=±¬¯±±¬ x = x - s before, we know that ±i(t)subscript±¡ _ i ( t ) in Algorithm (12) converge exponentially to the optimal solution ssuperscript s^{ ^ with the SST compressor. Based on the proof of Theorem 5 and Theorem 4, with (42) in mind, the system (12) becomes Define V1,t(±¯‚,³)=12(±¯‚2+³2)subscript1¡subscript¯±perpendicular-to³12superscriptnormsubscript¯±perpendicular-to2superscriptnorm³2V_{1,t}( _ 1 , t ( over¯ x _ ‚ , z ) = / 1 2 ( ¥ over¯ x _ ‚ ¥ ^ 2 + ¥ z ¥ ^ 2 ), then where the second inequality is obtained by (37) and (45). Now that ±e(t+1)±e(t)=Îº0‚(±e(t),t)subscript±¡1subscript±¡subscript…0‚subscript±¡¡ _ e ( t + 1 ) - x _ e ( t ) = - Îº _ 0 C ( x _ e ( t ) , t ), where ±edsubscript±superscript _ e R ^ d , achieves exponential convergence at the zero equilibrium, then clearly ²e(t+1)²e(t)=Îº0(²e(t),t)subscript²¡1subscript²¡subscript…0subscript²¡¡ ,t)y _ e ( t + 1 ) - y _ e ( t ) = - Îº _ 0 caligraphic_C ( y _ e ( t ) , t ), where ²endsubscript²superscript _ e R ^ n d , achieves also. Then there exists positive constants C¶CC, Î³D<1subscript¾·1 _ D < 1, for any t¡tt and N•+subscript•N N _ + , the solution satisfies Assume •tt+N(²e(t))superscriptsubscriptitalic-•¡¡subscript²¡ _ t ^ t + N ( y _ e ( t ) ) is the state of system ²e(t+1)²e(t)=Îº0(²e(t),t)subscript²¡1subscript²¡subscript…0subscript²¡¡ ,t)y _ e ( t + 1 ) - y _ e ( t ) = - Îº _ 0 caligraphic_C ( y _ e ( t ) , t ) in t+N¡t+Nt + N moment with the state in t¡tt moment is ²e(t)subscript²¡ _ e ( t ). It is easy to verified that for any ²nd²superscript R ^ n d and some L•>0subscript¿italic-•0L_{ _ • > 0 by property of compressor ‚‚ With (LABEL:eq:DPD.a.Vec3) in mind, we can proof Lyapunov function Ve(²e,t)=j=0N1•tt+j(²e)2subscriptsubscript²¡superscriptsubscript01superscriptnormsuperscriptsubscriptitalic-•¡¡subscript²2V_{e}( _ e ( y _ e , t ) = _ j = 0 ^ N - 1 ¥ • _ t ^ t + j ( y _ e ) ¥ ^ 2 with some N>00N>0N > 0 satisfies for c1=1,c2=NL•,c3>0formulae-sequencesubscript11formulae-sequencesubscript2subscript¿italic-•subscript30c_{1}=1,c_{2}=NL_{ _ 1 = 1 , c _ 2 = N L _ • , c _ 3 > 0. Besides, for Î=2+2Lc2Îº02>0ƒ22superscriptsubscript¿2superscriptsubscript…020 = 2 + 2 L _ c ^ 2 Îº _ 0 ^ 2 > 0 by property of ‚‚ Moreover, (47) still holds. Define V2(±¯±¯c,t):=Ve(±¯±¯c,t)assignsubscript2¯±subscript¯±¡subscript¯±subscript¯±¡V_{2}( _ 2 ( over¯ x - over¯ x _ c , t ) := V _ e ( over¯ x - over¯ x _ c , t ), we can derive for c4:=NL•Îassignsubscript4subscript¿italic-•ƒc_{4}:=NL_{ _ 4 := N L _ • Î, where the first inequality is obtained by ¯¯=•Š¯¯‚¯¯•Šsubscript¯¯perpendicular-to v = S over¯ v _ ‚ , (63) and (64), and the last inequality is obtained by (37), (47) and Young™s Inequality, where r>00r>0r > 0 is a parameter which will be determined later. Define V3,t(±¯¥):=12±¯¥2assignsubscript3¡subscript¯±parallel-to12superscriptnormsubscript¯±parallel-to2V_{3,t}( _{ _ 3 , t ( over¯ x _ ¥ ) := / 1 2 ¥ over¯ x _ ¥ ¥ ^ 2 , then (59) still holds. We introduce some parameters 1subscript1 _ 1 , Î¾1subscript1 _ 1 ¦>0absent0>0> 0 that have nothing to do with Î±¼ Î²½ rrr and Î·‚ and some parameters Î¶1subscript1 _ 1 , Î¶2subscript2 _ 2 ¦>0absent0>0> 0. We define the Lyapunov functions of system (43) Vt:=V1,t+V2,t+1V3,tassignsubscript¡subscript1¡subscript2¡subscript1subscript3¡V_{t}:=V_{1,t}+V_{2,t}+ _ t := V _ 1 , t + V _ 2 , t + _ 1 V _ 3 , t . It is easy to prove that Vtsubscript¡V_{t}V _ t is positive definite. In fact First, let™s impose some prime limit (49). By (62), (65), (59) (49), we can derive Î”VtÎ”subscript¡ V _ t is negative when we choose r=min{Î¾14Î¾3,14Î¾5,1}minsubscript14subscript314subscript51r= = roman_min { / Î¾ _ 1 4 Î¾ _ 3 , / 1 4 Î¾ _ 5 , 1 }, Îº¤Îº1=c32Î¾6r+2Î¾7…subscript…1subscript32subscript62subscript7 ¤ Îº _ 1 = / c _ 3 2 Î¾ _ 6 r + 2 Î¾ _ 7 , Î²2¤min{1,Î¾14Î¾2}superscript½2min1subscript14subscript2 ^ 2 ¤ roman_min { 1 , / Î¾ _ 1 4 Î¾ _ 2 }, Î·¤min{Î²2,14Î¾4}‚minsuperscript½214subscript4 ¤ roman_min { Î² ^ 2 , / 1 4 Î¾ _ 4 } and Îº¤Îº2:=12min{Î¾12Î¶1,Î²22Î¶2,Î·1Î¼n4Î¶3,c32Î¶4}…subscript…2assign12minsubscript12subscript1superscript½22subscript2‚subscript1subscript4subscript3subscript32subscript4 }}{2 ¤ Îº _ 2 := / 1 2 roman_min { / Î¾ _ 1 2 Î¶ _ 1 , / Î² ^ 2 2 Î¶ _ 2 , Î· / _ 1 Î¼ _ n 4 Î¶ _ 3 , / c _ 3 2 Î¶ _ 4 }. With (66), then Let Îº3:=2/min{Î¾1,Î²2,Î·Î¼n2,c32c1}assignsubscript…32minsubscript1superscript½2‚subscript2subscript32subscript1 c_{3}}{2c_{1}} _ 3 := 2 / roman_min { Î¾ _ 1 , Î² ^ 2 , Î· / Î¼ _ n 2 , / c _ 3 2 c _ 1 }, When Îº¤min{Îº1,Îº2,Îº3}…minsubscript…1subscript…2subscript…3 ¤ roman_min { Îº _ 1 , Îº _ 2 , Îº _ 3 }, we can derive for Î³(0,1)¾01 ( 0 , 1 ) , Vt=ª((1Î³)t)subscript¡ªsuperscript1¾¡V_{t}= _ t = caligraphic_O ( ( 1 - Î³ ) ^ t ). With the definition of Vtsubscript¡V_{t}V _ t , we derive ±¯(t)2=ª((1Î³)t)superscriptnorm¯±¡2ªsuperscript1¾¡ over¯ x ( t ) ¥ ^ 2 = caligraphic_O ( ( 1 - Î³ ) ^ t ). With the definition ±¯=±¬¯±±¬ x = x - s before, we know that ±i(t)subscript±¡ _ i ( t ) in Algorithm (12) converge exponentially to the optimal solution ssuperscript s^{ ^ with the ST compressor. The idea of proof is quite similar to that in Appendix G. We just recalculate Î”V2,tÎ”subscript2¡ V_{2,t}roman_Î” V _ 2 , t with stochastic impact while the other proof process is the same. Now that ±e(t+1)±e(t)=Îº0‚(±e(t),t)subscript±¡1subscript±¡subscript…0‚subscript±¡¡ _ e ( t + 1 ) - x _ e ( t ) = - Îº _ 0 C ( x _ e ( t ) , t ), where ±e(t)dsubscript±¡superscript _ e ( t ) R ^ d , achieves mean square exponential convergence at the zero equilibrium, then clearly ²e(t+1)²e=Îº0(²e,t)subscript²¡1subscript²subscript…0subscript²¡ _ e ( t + 1 ) - y _ e = - Îº _ 0 caligraphic_C ( y _ e , t ), where ²endsubscript²superscript _ e R ^ n d , achieves also. Then there exists positive constants C¶CC, Î³<1¾1 < 1, for any t¡tt and T•+subscript•T N _ + , the solution satisfies Assume •tt+N(²e)superscriptsubscriptitalic-•¡¡subscript² _ t ^ t + N ( y _ e ) is the state of system ²e(t+1)²e(t)=Îº0(²e(t),t)subscript²¡1subscript²¡subscript…0subscript²¡¡ ,t)y _ e ( t + 1 ) - y _ e ( t ) = - Îº _ 0 caligraphic_C ( y _ e ( t ) , t ) in t+N¡t+Nt + N moment with the state in t¡tt moment is ²e(t)subscript²¡ _ e ( t ). It is easy to verified that for any ²(n1)d²superscript1 R ^ ( n - 1 ) d and some L•>0subscript¿italic-•0L_{ _ • > 0 by property of compressor ‚‚ With (LABEL:eq:DPD.a.Vec3) in mind, we can proof Lyapunov function Ve(²e,t)=j=0N1•tt+j(²e)2subscriptsubscript²¡superscriptsubscript01superscriptnormsuperscriptsubscriptitalic-•¡¡subscript²2V_{e}( _ e ( y _ e , t ) = _ j = 0 ^ N - 1 ¥ • _ t ^ t + j ( y _ e ) ¥ ^ 2 with some N>00N>0N > 0 satisfies for c1=1,c2=NL•,c3>0formulae-sequencesubscript11formulae-sequencesubscript2subscript¿italic-•subscript30c_{1}=1,c_{2}=NL_{ _ 1 = 1 , c _ 2 = N L _ • , c _ 3 > 0. Besides, for Î=2+2Lc2Îº02>0ƒ22subscriptsuperscript¿2superscriptsubscript…020 = 2 + 2 L ^ 2 _ c Îº _ 0 ^ 2 > 0 by property of ‚‚ Define V2(±¯±¯c,t):=Ve(±¯±¯c,t)assignsubscript2¯±subscript¯±¡subscript¯±subscript¯±¡V_{2}( _ 2 ( over¯ x - over¯ x _ c , t ) := V _ e ( over¯ x - over¯ x _ c , t ), with (65) in mind, we can derive for c4:=NL•Îassignsubscript4subscript¿italic-•ƒc_{4}:=NL_{ _ 4 := N L _ • Î, where the first inequality is obtained (67) and (68), and r>00r>0r > 0 is a parameter which will be determined later. We define the same Vtsubscript¡V_{t}V _ t as that in Appendix G, then (66) holds and we have with the same parameters. Then we can derive ”¼±¯(t)2=ª((1Î³)t)”¼superscriptnorm¯±¡2ªsuperscript1¾¡ ¥ over¯ x ( t ) ¥ ^ 2 = caligraphic_O ( ( 1 - Î³ ) ^ t ). With the definition ±¯=±¬¯±±¬ x = x - s before, we know that the mean square of ±i(t)subscript±¡ _ i ( t ) in Algorithm (12) converge exponentially to the optimal solution ssuperscript s^{ ^ with the StST compressor.",
        "keywords": ""
    },
    {
        "id": 17,
        "title": "Enhancing Trustworthiness and Minimising Bias Issues in Leveraging Social Media Data for Disaster Management Response",
        "abstract": "AbstractDisaster events often unfold rapidly, necessitating a swift and effective response. Developing plans of action, resource allocation, and resolution of help requests in disaster scenarios is a time-consuming and complex process since the disaster-relevant information is often uncertain. Leveraging real-time data can significantly deal with data uncertainty and enhance disaster response efforts. To deal with uncertainty in data in real-time, social media appeared as an alternative effective source of real-time data as there has been extensive use of social media during and after the disasters. However, it also brings forth challenges regarding trustworthiness and bias in these data. To fully leverage social media data for disaster management, it becomes crucial to mitigate biases that may arise due to specific disaster types or regional contexts. Additionally, the presence of misinformation within social media data raises concerns about the reliability of data sources, potentially impeding actionable insights and leading to improper resource utilization. To overcome these challenges, our research aimed to investigate how to ensure trustworthiness and address biases in social media data. We aim to investigate to identify the factors that can be used to enhance trustworthiness and minimize bias to make an efficient and scalable disaster management system utilizing real-time social media posts, identify disaster-related keywords, and assess the severity of the disaster. By doing so, the integration of real-time social data can improve the speed and accuracy of disaster management systems.",
        "corpus": "Disaster events often unfold rapidly, necessitating a swift and effective response. Developing plans of action, resource allocation, and resolution of help requests in disaster scenarios is a time-consuming and complex process since the disaster-relevant information is often uncertain. Leveraging real-time data can significantly deal with data uncertainty and enhance disaster response efforts. To deal with uncertainty in data in real-time, social media appeared as an alternative effective source of real-time data as there has been extensive use of social media during and after the disasters. However, it also brings forth challenges regarding trustworthiness and bias in these data. To fully leverage social media data for disaster management, it becomes crucial to mitigate biases that may arise due to specific disaster types or regional contexts. Additionally, the presence of misinformation within social media data raises concerns about the reliability of data sources, potentially impeding actionable insights and leading to improper resource utilization. To overcome these challenges, our research aimed to investigate how to ensure trustworthiness and address biases in social media data. We aim to investigate to identify the factors that can be used to enhance trustworthiness and minimize bias to make an efficient and scalable disaster management system utilizing real-time social media posts, identify disaster-related keywords, and assess the severity of the disaster. By doing so, the integration of real-time social data can improve the speed and accuracy of disaster management systems. Disasters, whether natural or human-induced, have profound impacts on society. Its consequences include the human toll, environmental degradation, economic loss, psychological disruption, and infrastructure damage [1]. Unfortunate events like Hurricane Sandy (2012), the Nepal Earthquake (2015), Hurricane Harvey (2017), Cyclone Idai (2019), the ongoing COVID-19 pandemic [2], and the Palestine-Israel conflict, etc have resulted in the loss of millions of lives, disrupted the economies, and strained healthcare systems. In such uncertain scenarios, effective disaster management and decision-making systems are central to mitigating and minimizing the effects of future calamities. However, not all emergency stakeholders possess specialized expertise in emergencies [3]. The rise of social networks plays an important role in bridging the gap between stakeholders and those with more informed decision-making capabilities. Among various communication mediums, social media platforms have proven to be effective in disseminating real-time situational awareness, and safety instructions and facilitating rapid response. It has been reported that 5.04 billion people worldwide now use social media, a significant increase from the 3.6 billion users recorded in 2020 [4]. Due to the accessibility of platforms like Facebook, Twitter, and Instagram, social media have been recognized as a powerful data source for studying disastrous events over the last decade. Compared to traditional methods of data collection like cameras [5], RFID readers [6], and GPS information [7], social media data has promising merits: 1) scrapping posts from social media is economical, 2) the availability of user-generated content that can only be acquired through traditional data sources and government and regulatory organizations 3) real-time disaster updates, and 4) swift data acquisition. A massive amount and variety of user-generated content is shared on social media. These platforms enable a human-centric approach where the public can share rich footprints of disastrous events that can be used to enhance the effectiveness of disaster management systems. With potential benefits, several challenges are associated with the vast unstructured data coming from indeterminant sources [8]. First, determining the veracity and quality of data sources is a significant issue. A reliable disaster management system demands trust in data for responsible disaster management. Moreover, to fully leverage social media data for disaster management, it is essential to aim at the conscious elimination of biases that may arise due to specific disaster types or regional contexts. Otherwise, these biases will be encoded into the latent representation of the disaster management system which in turn will result in unfair responses. This research work is dedicated to mitigating biases and ensuring the trustworthiness and quality of social media data for a fair and responsible disaster management and response system. We investigate the approaches that can enhance the veracity and quality of social media data. The approaches explored in this work can ensure the integrity of a scalable disaster management system. The rest of the paper is organized as follows: Section II highlights the significant biases present in disaster-related social media data and provides effective mitigation strategies to address them. Section III focuses on trustworthiness concerns and the corresponding methods to tackle those issues. While Section IV concludes the presented work. Literature has witnessed the utilization of social media data in disaster management, contributing towards real-time crisis and public sentiments. Table I presents datasets used in the literature to advance research in disaster management and response. The CrisisLexT26 dataset has been utilized for early detection of crisis-related events [9]. CrisisNLP has been employed in research to develop a natural language processing tool to detect crisis-relevant information for humanitarian aid [10]. Moreover, CrisisMMD has facilitated multimodal analysis of crises, exploring the integration of text, image, and video data for enhanced situational awareness [11]. Similarly, the UnifiedCEHMET Dataset has demonstrated high precision in severity classification using deep learning methods [12]. These studies exhibit the versatility and significance of these datasets in advancing understanding and improving response strategies in disaster management. However, the human-driven nature of these social media datasets can exhibit socio-demographics, language preferences, content, and spatial biases. The datasets contain inherent biases that hinder precise evaluation and response to crisis events, while the representation of disasters affects response strategies and resource allocation. Recently, this dilemma has caught the attention of researchers, and various biases have been identified. The identified biases are presented in 1. To mitigate their impact several approaches are discussed in the subsequent subsections. The geographic bias in social media data can hinder disaster response efforts due to the skewed geographic distribution of data. Recent studies have highlighted that the uneven representation of certain locations or regions leads to disparities in data coverage during disaster situations [13]. To address this issue, spatial sampling helps to enhance geographic representativeness by collecting data from diverse geographic regions. To quantify and visualize geographic biases in the dataset, sampbias has been developed as a tool to promote the usefulness of data in several research areas [14]. Moreover, sampling techniques like the synthetic minority over-sampling technique (SMOTE) can improve the representativeness of the skewed instances from the majority regions [15]. By mitigating the geographic concentration of data, the machine-learning models will be more effective at capturing situations from social media in disasters. Language biases can limit the inclusivity and comprehensiveness of the data. It arises in social media-based datasets from the prevalent utilization of specific languages to post help and rescue requests in crises. According to [16], the language of posts shared on social media data is categorized into four types, including, i) global language, ii) local language, iii) mixed language, and iv) mixed script. Being the global language, most social media posts are shared in English. Potential countermeasures to demographic language bias are multilingual data collection [17] and utilization of translation services. The inception of large language models (LLMs) has advanced the field of natural language processing. Multilingual LLMs (MLLMs) address lingual biases because these language models are trained on multiple languages [18]. Notable examples of MLLMs include mBERT [19], XLM-R [20], PaLM [21], mT5 [22], Falcon [23], BLOOM [24], and LLaMA [25]. These MLLMs are trained on multiple languages and provide reasonably fair language processing capabilities. BLOOM is an outstanding MLLM due to its ability to support 46 languages, including French, English, African, Indonesian, Mandarin, etc. However, despite these advancements certain challenges like corpora, misalignment of MLLMs, and inherent biases in corpora still persist. To exemplify, ChatGPT is trained on 92.099% of English Corpora and only 0.16% of the corpora account for the Chinese language. True multilingualism can be achieved by creating a high-quality multilingual dataset. This area is undervalued and needs the attention of researchers. For a fair and unbiased representation of the training data, de-biasing the crisis-relevant data while preserving the important information is of paramount significance. To maintain the representation integrity, the learning fair representation (LFR) algorithm [26] is designed to transform the training data while minimizing the loss of non-sensitive instances. Additionally, the prejudice-free representations (PFR) algorithm [27] is proposed to identify and remove the features causing discrimination in the dataset, particularly those relevant to the sensitive attributes. By utilizing representation methods like LFR and PFR, the fairness of models in de-biasing disaster-relevant data can be ensured. The trustworthiness of social media-based user-generated content is of paramount importance. The human-driven and diverse nature of the content and the spread of misinformation pose significant threats. It requires attention to assess the trustworthiness of the information provided on social media before using it for crisis-relevant applications, since the content may have been generated to spread false information. To combat these concerns and entrust social-media information relevant to sensitive events, various approaches have been proposed in the existing research (presented in Fig 2. In this section, we delve into existing techniques and mitigation strategies to ensure the quality and authenticity of social media data during disasters. It is essential to understand the dynamics of disaster-relevant social media users that contribute to data quality and trustworthiness. The main actors that spread information in crises are non-governmental organizations, government agencies, research/academic bodies, and public profiles [16]. Information disseminated through government organizations accounts is more trusted than that of a personal account. However, government authorities are noticed to be showing reduced participation in informing communities of the crises. Public users, on the other hand, have promising engagement on social media platforms. Enhancing Data Verification: The verification of social media data emphasizes the development of mechanisms to assure trust. Algorithms such as Naïve Bayes and Feature Tree (FT) classifiers are used to detect spam and fake posts on social media during crises [28]. These algorithms are utilized to distinguish between legitimate and false tweets with significance. Moreover, factors such as verified accounts, premium subscription services, and engagement metrics play an important role in ensuring trust in social media content and combatting misinformation propagation. In Twitter, factors like verified users, blue tick subscriptions, and retweet metrics enhance the trust level of their shared content. Transfer Learning: Advanced transformer-based language models like BERT or RoBERTa leveraging transfer learning facilitate the detection of misinformation conveyed on social media platforms [29]. Fine-tuning these models on misinformation datasets can help the system identify misleading information. Thereby, the reliability of the information disseminated on social media platforms can be enhanced. Deepfake Detection Techniques: Researchers are exploring various approaches utilizing Deepfake detection techniques to discern real from manipulated media [30]. Computer vision methodologies and deep neural networks are among the approaches that can be employed to mitigate potential implications leading to distrust. Early Detection Using Linguistic Patterns: Early identification of fraudulent information on social media platforms is made possible by linguistic cue analysis linked to misinformation and user attribute inference based on linguistic traits. By understanding various patterns like how the information spreads and recognizing linguistic traits indicative of misinformation, it is possible to mitigate the spread of false information. For instance, authors in [31] address the spread of misinformation on Twitter during the COVID-19 pandemic. The authors analyzed textual and non-textual cues to understand their influence on retweeting behavior and the spread of false information on social media. By analyzing 4923 tweets featuring disaster-relevant hashtags in May 2020, the study aims to discern retweet probability and volume. For this purpose, they have focused on employing logistic regression and machine learning techniques. These approaches can be leveraged in disaster management and response applications to enhance trust in social media data. These methods facilitate effective management for resource allocation and rescue during disastrous events. Disaster, either natural or human-induced, necessitates swift response to the situations for effective allocation of resources to mitigate the loss and save lives. The prevalent utilization of social media data has made it possible due to rich user-generated content which can help disaster management systems locate the victims and volunteers. However, social media data has challenges in terms of certain biases and trustworthiness. To encounter these issues, we have identified three types of biases that need to be mitigated before the utilization of social media data for sensitive crisis-relevant decision-making systems. We also highlight the importance of verifying the authenticity and quality of information posted on these platforms to ensure trustworthiness. For this purpose, we have explored several techniques from the literature to ensure trust in social media data by identifying misleading content. Incorporating the factors that can be used to enhance trustworthiness and minimize bias can help to develop an efficient and scalable disaster management system utilizing real-time social media posts. In the future, we intend to conduct rigorous analysis by considering case studies to provide empirical evidence. Furthermore, we will delve into the challenges and limitations of the mitigation strategies discussed in this research by focusing on concerns regarding implementation, resource constraints, and adaptability. It will help in the successful adoption of the proposed strategies and highlight the need for future research in this area.",
        "keywords": "Index Terms: \nTrustworthiness, Bias Minimization, Social Media Data, Disaster Management"
    },
    {
        "id": 18,
        "title": "iSurgARy: A mobile augmented reality solution for ventriculostomy in resource-limited settings",
        "abstract": "AbstractGlobal disparities in neurosurgical care necessitate innovations addressing affordability and accuracy, particularly for critical procedures like ventriculostomy. This intervention, vital for managing life-threatening intracranial pressure increases, is associated with catheter misplacement rates exceeding 30% when using a freehand technique. Such misplacements hold severe consequences including haemorrhage, infection, prolonged hospital stays, and even morbidity and mortality. To address this issue, we present a novel, stand-alone mobile-based augmented reality system (iSurgARy) aimed at significantly improving ventriculostomy accuracy, particularly in resource-limited settings such as those in low- and middle-income countries. iSurgARy uses landmark based registration by taking advantage of Light Detection and Ranging (LiDaR) to allow for accurate surgical guidance. To evaluate iSurgARy, we conducted a two-phase user study. Initially, we assessed usability and learnability with novice participants using the System Usability Scale (SUS), incorporating their feedback to refine the application. In the second phase, we engaged human-computer interaction (HCI) and clinical domain experts to evaluate our application, measuring Root Mean Square Error (RMSE), System Usability Scale (SUS) and NASA Task Load Index (TLX) metrics to assess accuracy usability, and cognitive workload, respectively.",
        "corpus": "Global disparities in neurosurgical care necessitate innovations addressing affordability and accuracy, particularly for critical procedures like ventriculostomy. This intervention, vital for managing life-threatening intracranial pressure increases, is associated with catheter misplacement rates exceeding 30% when using a freehand technique. Such misplacements hold severe consequences including haemorrhage, infection, prolonged hospital stays, and even morbidity and mortality. To address this issue, we present a novel, stand-alone mobile-based augmented reality system (iSurgARy) aimed at significantly improving ventriculostomy accuracy, particularly in resource-limited settings such as those in low- and middle-income countries. iSurgARy uses landmark based registration by taking advantage of Light Detection and Ranging (LiDaR) to allow for accurate surgical guidance. To evaluate iSurgARy, we conducted a two-phase user study. Initially, we assessed usability and learnability with novice participants using the System Usability Scale (SUS), incorporating their feedback to refine the application. In the second phase, we engaged human-computer interaction (HCI) and clinical domain experts to evaluate our application, measuring Root Mean Square Error (RMSE), System Usability Scale (SUS) and NASA Task Load Index (TLX) metrics to assess accuracy usability, and cognitive workload, respectively. Ventriculostomy, a common neurosurgical procedure, establishes a drainage pathway for cerebrospinal fluid (CSF) from the brain ventricles to a collection and monitoring system at bedside. This intervention aims to relieve excessive intracranial pressure within the skull caused by obstructed CSF flow. The need for ventriculostomy arises when various pathological processes such as hemorrhages (e.g., aneurysms and vascular malformations), head trauma, tumours, spina bifida, hydrocephalus, or congenital issues block or impede normal CSF flow, CSF production, or its absorption. A recent global study found that traumatic brain injury (TBI) and the presence of hydrocephalus accounts for 45% and 7%, respectively, of all cases admitted for acute neurosurgical care. [21]. Placement of an external ventricular catheter (ventriculostomy) involves drilling a small hole in the skull and carefully inserting a thin, flexible catheter into the ventricle. Pre-operative CT scans along with well-known external cranial landmarks are typically used to guide placement for optimal accuracy. Relying solely on external landmarks in emergency situations can lead to misplacement in over 30% of cases, and can potentially cause unwanted bleeding, inaccurate pressure readings, and ineffective drainage. These complications can also lead to longer hospital stays and increase in mortality [23, 26]. Image-guided neurosurgery (IGNS) improves any ventriculostomy procedure accuracy through use of pre-operative CT and MRI scans to provide real-time reference and spatial guidance, but its widespread use faces several challenges. The cost of acquiring these systems, ranging from USD 650,000 to over 900,000, can be a major barrier for many institutions [34].Furthermore, effectively operating IGNS systems requires the expertise of specialized technicians skilled in planning and systems setup. Additionally, the bulky nature of the equipment, including the workstation and tracking camera, limits its use to large operating rooms, making it difficult to use these systems in other tighter settings like emergency rooms, wards, and ICUs (Intensive Care Units). These challenges limit IGNS use in low- and middle-income countries (LMICs) and remote communities. Additionally, a technician-intensive system is limited to elective and not emergency procedures. Indeed, geographic disparities for timely and inexpensive EVD (External Ventricular Drain) placement exist. For example, Sub-Saharan Africa reports a much higher rate of infant hydrocephalus compared to other regions, with 750 new cases per 100,000 births compared to approximately 110 cases in Europe and the USA [20]. This disparity in disease prevalence and access to advanced technology creates a concerning disparity in the quality of care between the resource-constrained settings with those high-resource settings. Our aim is to narrow this gap by developing a solution that is affordable, easily deployable, has a small footprint, and requires minimal expertise, making it suitable for use in LMICs, remote communities, and other resource-limited settings. To address the challenges faced in translating surgical innovations into clinical practice [37], we focus on designing a ventriculostomy guidance system through co-design with neurosurgeons and a focus on user experience. By applying user-centered design practices, we aim to create a system that aligns with both technological advances and the practical needs of surgeons, ensuring its usability and successful integration into daily surgical practice in resource-limited settings. Specifically, we present iSurgARy, a mobile system designed to operate exclusively on iPhones or iPads. iSurgARy leverages LiDAR for precise patient tracking, complemented by augmented reality (AR) for procedural guidance. Neurosurgical procedures are constantly evolving with the integration of innovative technologies such as robotics, mixed and virtual reality, 3D printing and intraoperative imaging methods. Hong et al. [27] introduced a mobile AR navigation system (MARNS) for precise transverse-sigmoid sinus junction location determination during retrosigmoid craniotomy. It demonstrates efficacy with a mean matching error of 2.88 mm (SD ± 0.69 mm), a positioning time of 279.71 seconds (SD ± 27.29 seconds), and was found to maintain bone flap integrity in 86.7% of cases. In another study, de Almeida et al. [19] addressed the cost and complexity of traditional neuronavigation systems. Their research presents a mobile-based AR solution for localizing points or landmarks on the scalp surface. In laboratory testing with a 3D phantom under optimal conditions, the system achieved an accuracy of 2.6 mm (SD ± 1.6 mm). Gorkem et al. [25] investigated a 3D-printed marker-based AR system for intracranial tumor segmentation. It demonstrated high precision (0.5 to 3.5 mm targeting error), clinical feasibility, and cost-effectiveness, highlighting its potential for real-world application. In a separate study, LÃ©ger et al. proposed MARIN, an iPad-based AR neuronavigation system [32], as well as NousNav [31], a low-cost and an open-source neuronagivation system, as a solution for use in low-resource settings. NousNav uses low-cost off-the-shelf components, can be easily built for 6k USD, features an intuitive interface and easy intraoperative control, and is robust and modular, making it a promising candidate for wider accessibility. For ventriculostomy in particular, several mixed reality systems have been proposed to improve accuracy. Azimi et al., [16] developed an automated registration and trajectory planning system for ventriculostomy where a pointer equipped with a Vuforia marker was used for landmark-based registration. They achieved a 37% improvement in accuracy for tip placement compared to manual registration methods, with a pointer tip-to-target distance of 10.96mm10.9610.96mm10.96 m m. In a similar study by Schneider et al. [38], the authors used AR for ventriculostomy. They attached a Vuforia marker to the patient™s head to guide the projection of 3D ventricle models onto the skull. A game controller was employed to align the holograms with the patient. Their system achieved a success rate of 68.2% for ventriculostomy, with an average deviation of 5.2 mm from the planned trajectory. A subgroup showed significant improvement in results and precision after repeated attempts, suggesting a learning curve for using the AR system. The use of a rigid needle is assumed in AR-assisted medical procedures. Lin et al. [33] explored the limitations of this approach. They developed a system utilizing the HoloLens to display segmented ventricles and the desired catheter trajectory, achieving a target registration error of 4.34±1.63plus-or-minus4.341.634.34 1.634.34 ± 1.63 mm and reducing catheter passes from 2.33±0.98plus-or-minus2.330.982.33 0.982.33 ± 0.98 to 1.07±0.258plus-or-minus1.070.2581.07 0.2581.07 ± 0.258 times. However, this method does not account for needle deflection, which can lead to inaccuracies. As highlighted in their study, incorporating real-time deflection data into AR systems could further improve precision in needle procedures. To improve freehand ventriculostomy accuracy, Ansari et al. [35] developed VentroAR, a HoloLens-based AR system. This system guides surgeons in locating ventricles, aiming to reduce the risk of complications associated with misplaced catheters. In a user study with 15 novices, VentroAR achieved an average gesture-based registration accuracy of 10.75mm10.7510.75mm10.75 m m and a targeting accuracy of 10.64mm10.6410.64mm10.64 m m. While promising in terms of workflow and ease of use, the authors acknowledge the need for further accuracy improvements before clinical adoption. For a more comprehensive examination of mixed reality in ventriculostomy, readers are directed to the 2024 review by Alizadeh et al. [14]. Although tablet and smartphone systems have been developed for neurosurgery, to the best of our knowledge, they all either require external tracking systems, markers, or tags. Dogan et al. [22] (2024) used a smartphone for neurosurgery but had issues with repeated manual failed registrations. Santos et al. [24] developed a smartphone app to be used with a compass for neurosurgery procedures, yet their system required skin makers. De Almeida et al [19] developed a smartphone system, however their focus was on looking at the impact of lighting on registration. Furthermore, they used non-pro iPhones and thus were unable to take advantage of LiDAR for registration. MARIN [32] uses an iPad yet requires the use of an external optical tracker. In comparison to these works, iSurgARy is novel as it requires the use of only a smartphone, uses LiDAR, and does not require additional trackers or markers on the patient. This simplifies the workflow in emergency and low-resource settings. iSurgARy was developed to be a small-footprint, low-cost image-guided neurosurgery (IGNS) system suitable for use in resource-limited and emergency settings. To achieve this, iSurgARy features a simple user interface (UI) designed to allow clinicians to easily select anatomical landmarks on the patient using the touch screen of mobile devices. These selected landmarks are then used in a rigid registration process to map the patient to their pre-operative scans. Once the registration process is complete, the clinician can visualize the ventricles (in the case of ventriculostomy) or other relevant 3D models (such as the cerebral vacsculature) in AR. This visualization aids in guiding the clinician to optimal EVD placement. Additionally, the catheter tracking tool enhances the clinician™s spatial understanding of the distance between the catheter tip and the ventricles (see Figure 1). The current iSurgARy prototype utilizes an iPhone 15 Pro (MTUA3VC/A) running iOS 17.4 as the primary device. However, the application is also compatible with iPad Pro tablets that house the LiDAR sensor, offering greater flexibility in its use. Development is conducted within XCode Version 15.2 (15C500b) on a MacBook Air 15-inch, M2, 2023 with MacOS 14.3 (23D56), 16GB memory, and 1TB SSD. Additionally, a 3D-printed head phantom, designed for MRN system development and testing in neurosurgery, is used for alignment with corresponding digital ventricle and artery segments[36]. iSurgARy was built for iOS using the Swift [11] and C++[6] programming languages. It leverages Apple™s ARKit[28] and RealityKit[10] frameworks to track and assign anatomical landmarks to 3D anchors and visualize the AR 3D model. The UI was implemented with SwiftUI[12], while the asynchronous events are managed by the Combine framework[7]. For unit conversion compatible with ARKit and transform matrix computation, Accelerate framework is utilized. On the C++ side, the iterative closest point algorithm (ItCP) and its helper functions rely on the Eigen library[8] and standard libraries like cmath, algorithm, and limits. In the following section, we describe the user experience and workflow of using iSurgARy. Technical details about the tracking and registration implementation follow in the next sections. The application features a user-friendly interface designed for effortless registration and navigation. Users will load patient CT scans and select specific points on the scan for both landmark registration and surgical targeting. For instance, users can choose Koscher™s point for burr hole localization and catheter targeting and may move the target entry point as deemed appropriate if there is cortical and ventricular distortion from blood or other space occupying lesions. The app guides clinicians to place seven key anatomical landmarks on the patient, commonly used for patient-to-image registration in IGNS: the right tragus, the right outer canthus, the right inner canthus, the nose bridge, the left inner canthus, the left outer canthus, and the left tragus. During the registration process, the interface displays a text field at the top of the screen indicating the current landmark to target. A target cursor is centrally displayed on screen to aid in precise targeting. The right side of the interface contains an acquire™ button to confirm landmark placement relative to the center target cursor. When the user presses the acquire™ button, a red spherical AR object is rendered on the device™s camera feed at the aimed landmark. Once landmark placement is made, the user can either use the delete™ button to remove the AR landmark reference or choose a different anatomical target by using the next™ navigation buttons. The process of landmark placement for each target, guided by the text field at the top of the screen, is repeated until all seven key anatomical landmarks have been placed. The user can also navigate backwards and choose a previous anatomical target by using the back™ button. Technical details on landmark placement are discussed in Section 3.4. Once all landmarks are placed, the register™ button aligns the landmarks to the AR model using an iterative closest point algorithm (ItCP), described in Section LABEL:registration. When the program completes registration, an AR view of the patient™s internal structures for guidance is generated and the RMSE of the alignment is displayed at the top of the screen. If the user is dissatisfied with the alignment, they can easily re-select some or all of the anatomical landmarks for a more precise registration. If satisfied, the confirm™ button proceeds to the next phase. In the EVD entry point placement phase, an entry point button enables the placement of a target for burr hole localization. If an entry point target is misplaced, a delete™ button allows the user to remove the saved entry point location and re-select an entry-point. After saving the entry point location and navigating to the next phase, using the next™ button, the target registration error (TRE) is displayed in the text field at the top of the screen (if the entry point was chosen on the pre-operative CT). Catheter tracking is done using 2D image detection. Specifically, when the catheter tracking tool is within the camera frame, an AR view of a straight line is overlaid on the catheter from the QR code to the catheter tip, aiding in depth assessment during catheter insertion into the patient. Lastly, a reset™ button is available to remove all seven landmark targets placed by the user, providing a fast and convenient way to redo all landmarks, and reset the interface to the initial target stage where the user is asked to place the first landmark, the right tragus. The interface allows clinicians to move around the patient from different angles, improving their 3D spatial awareness of internal structures by providing a better understanding of distance and direction between the catheter tip and the ventricle. For registration, i.e. mapping of patient space to image space, iterative closest point (ItCP) [39] was used. ItCP aligns the landmarks placed in AR and the predefined landmarks from the patient™s CT scan. Our ItCP implementation processes coordinates of both AR-placed landmarks and predefined landmarks, producing a scale factor, a rotation matrix, and the RMSE. To visualize the aligned internal structures in AR, the 3D model is anchored at the centroid of the AR landmarks, with its origin as the mean of the predefined landmarks. The scale factor derived from the ItCP process is applied to the loaded 3D model. The rotation matrix is then used to adjust the orientation of the 3D model ensuring optimal alignment of the landmarks. Traditional IGNS systems present navigation information on a computer outside the sterile operating region. This requires the surgeon to shift their gaze away from the patient and surgical field for guidance information and mentally map it to the patient™s anatomy. This process can be time-consuming, cognitively demanding, and prone to error [29, 15]. Furthermore, it results in a disconnect between where the surgeon is working and where they are looking for guidance. This constant shifting of attention can negatively influence the successful completion of a surgical task [30]. To address these issues, we use AR to superimpose the patient™s ventricular anatomy onto the head and track the tip of the catheter. In iSurgARy, ARKit provides world tracking capabilities, utilizing the devices™ camera and motion sensors to monitor its position and orientation in the real world[4]. In an ARKit ARSession[5], feature points are detected within the camera™s image space and used to represent key details of the real-world environment. These feature points are then compiled into an ARPointCloud, which is a collection of 3D coordinates in the world space that ARKit uses for various tracking tasks. This point cloud is crucial for continuously updating fixed anchor points in the environment that serve as reference frames for placing and tracking virtual objects in real-time as the camera moves. ARKit uses this data to ensure that virtual content remains accurately positioned and aligned with the physical world across each frame. Additionally, the housed LiDAR scanner enhances depth tracking and anchor placement precision, we access the depth data using ARDepthData[2]. When the user presses the acquire™ button for placing a landmark, we use ARCamera[1] to capture the position and orientation of the camera, and we use ARDepthData to determine the distance between the device and the subject being targeted. From this, we compute the spatial coordinates of the landmarks from each capture and pass it into the ItCP algorithm in order to align our 3D model to the real-world environment. RealityKit is employed to render the 3D content, such as the virtual 3D spheres for the placed landmarks and the 3D model of the ventricles. To track the tip of the catheter, we pass a QR code image as a reference image to the detectionImages property of the world-tracking configuration[9]. When the ARsession recognizes the QR code reference image, an ARImageAnchor[3] is added to the detected image. Once detected, a rectangular AR object is rendered from the detected image extending to the tip of the catheter as shown in rightmost image of Figure 1. To evaluate the iSurgARy app in a laboratory setting, we conducted a two-phase user study. The first phase involved a preliminary study with novice participants, focusing on assessing the usability and learnability of the app. Feedback from this phase was used to refine the application. In the second phase, we engaged clinical and HCI experts to evaluate usability and performance. This phase included measuring RMSE to assess accuracy, SUS for system usability and the NASA TLX to evaluate cognitive workload. For the novice study, we had 10 university participants (5 female and 5 male) who were graduate students in engineering and science fields. They performed landmark registration and patient tracking and completed the SUS[17] to provide feedback on the app™s usability. We started with a brief introduction of the app to each participant. We explained that the accuracy of task completion was not the primary concern. Instead, the focus was on how easily the participants could follow the app instructions and navigate the user interface to complete the tasks. This approach ensured that we received meaningful feedback on the usability and learnability of the app, allowing us to identify and address any issues. In the second phase of our evaluation, we focused on assessing both the usability and learnability of iSurgARy, as well as its accuracy and cognitive load when used by domain knowledge users. This study involved 11 participants, including nurses, clinicians, and HCI experts. The demographics and backgrounds of these domain experts are presented in Table 1. We had a gender split of almost 50%, with the following backgrounds: A neurosurgeon with over 25 years of experience A clinician with a Master™s degree focused on IGNS Three nurses working at a neurological institute, including two with OR/ER experience and one nurse manager Three HCI researchers One HCI and mobile app developer One IGNS researcher/developer All participants had relevant or related experience, ensuring the feedback we received was well-informed and applicable to real-world scenarios. The study concluded by collecting qualitative feedback on the app™s usability. This feedback guided further improvements in the application™s design. For system usability, we used the SUS, a questionnaire employed to evaluate the usability of products and services. The results of the SUS are presented in Table 2. The average SUS was 81.52% indicating an easy-to-use system (Note: The average SUS score is 68% and scores up to 70% are considered good). Furthermore, comments from the participants indicated that they felt the system was easy to learn, understand and use. For evaluating system usability in our second study, we used SUS, the results of which are summarized in Table 3. The average SUS was 80.95%, indicating that the system was perceived as easy to use by our expert domain knowledge participants. The NASA Task Load Index (NASA TLX) was used to assess the subjective workload experienced by participants. The mean scores (with standard deviations) for the dimensions were as follows: Mental Demand, 7.17 (±4.04); Physical Demand, 7.5 (±4.19); Temporal Demand, 5.67 (±3.60); Performance, 16.33 (±3.17); Effort, 7.33 (±4.38); and Frustration, 5.33 (±4.03) (see Figure 4). Among these dimensions, performance had the highest mean score, suggesting that participants felt they had to work hard to get a good level of performance, in this case a low RMSE. The overall workload, based on the NASA TLX scores is 41.1 on a 100-point scale suggesting a moderate workload. The variability in standard deviations across the dimensions reflects differing levels of consensus among participants regarding their experience. In terms of accuracy, on average, we found an RMSE of 2.54mm±0.46plus-or-minus2.540.462.54mm 0.462.54 m m ± 0.46. These findings align with previous works [19, 27]. Furthermore, according to our experts as the ventricles are a large target, an RMSE of < 5 mm is generally acceptable. To ensure the application™s scope aligned with the needs of neurosurgeons, we collaborated closely with them during the development process. One of the neurosurgeons who participated in our expert user study provided particularly encouraging feedback. Comments included: \"Wow. I think you have the gist of it. Simple and quick registration and very responsive tracking. Sign me up. When would you like to test this in a clinical setting?\", \"I would like to use the iPad, but I think the residents will prefer the iPhone form factor as they can just pull it out of their pocket.\" and \"Superb user-friendly and easy-to-use tool for the task at hand. It will be a valuable adjunct towards the safe and timely placement of EVDs.\" Other comments included: \"Another anchor point that isn™t co-planar with rigid points to better account for out of plane rotations\" and \"Try using multiple selections of the same point to both assess intra-user variability in point selection and to use centroid of chosen points as anchor point for registration\", suggesting that improvements could be made on the registration procedure. Based on the various feedback, we have determined an ergonomic solution that would work in resource-limited emergency and ICU settings. First, an iPhone/iPad holder that would be clamped to the bed during the procedure would allow the user to have both hands available for catheter placement (see Figure 5). Next, simple solutions for stabalizing the patient™s head were discussed. Lastly, in future work, to ensure that the catheter can be tracked without the need for the QR code, we will focus on developing computer vision techniques. iSurgARy prioritizes affordability, aiming to make advanced navigation technology accessible to a wider range of medical institutions while maintaining accuracy. Unlike traditional IGNS systems, which can cost thousands of dollars due to sophisticated tracking cameras and dedicated computers, iSurgARy utilizes commercially available and affordable mobile devices. While there is an initial investment associated with these devices, it represents a significantly more economical option. The cost-effectiveness, combined with the portability and ease of use of our system makes it a much better navigation option for resource-constrained settings. iSurgARy combines both human expertise and machine capabilities, and in doing so, this introduces potential sources of error for which the user should be aware. The initial placement of virtual landmarks is a key step for accurate guidance, but it relies on the user™s judgment and precision. This can lead to inaccuracies that may significantly affect subsequent stages, particularly alignment. Errors can also arise from the ItCP algorithm and the ARKit framework. The ItCP algorithm, while robust in aligning 3D structures, can struggle with sparse data, uneven distributions of landmarks, or large initial misalignment that exceed its convergence criteria. The ARKit framework, used for real-world environment tracking, has its own limitations due to sensor accuracy and environmental factors. The iPhone/iPad uses an accelerometer and a gyroscope, and their precision limitations can manifest as inaccuracies in landmark tracking, potentially causing misalignment between the real world and the augmented overlay. In future work, we will consider capturing a point cloud of the patient with LiDAR and performing a more dense point-based registration. Despite a modest user study sample size, we believe our findings provide reliable results about the usability and applicability of the system. Our second study aimed at using domain experts (mobile app developers, HCI experts and clinicians) to determine the usability of the system as human-computer interaction (HCI) research suggests that a sample of even just 3-5 system evaluators can identify 75% of usability issues. Furthermore, the inclusion of double experts (in this case a clinician who was also an IGNS researcher) may identify more issues [13, 18]. Feedback from the neurosurgeon, clinician with IGNS research background and an IGNS researcher and developer, as well as our HCI experts not only validated our approach but also helped us identify areas for improvement in the next iteration of our application. This collaboration underscores the importance of iterative development and expert input in refining and enhancing the usability and functionality of medical applications. The next step of this work will be to further test iSurgARy with neurosurgeons and residents. This testing will be important in evaluating the robustness and accuracy of our system in real-world surgical settings, determining its practical viability and clinical effectiveness in achieving precise EVD placement. Furthermore, we plan to compare our smartphone/tablet system accuracy, usability, and ergonomics with head-mounted displays (HMDs) like the Apple Vision Pro or Microsoft HoloLens. These devices potentially offer a hands-free experience and potentially better immersion, which could be advantageous for complex procedures. However, weight, battery life, and maintaining sterility remain challenges. Our future research will explore these trade-offs to identify the most effective AR solution for surgical applications. In this work, we aimed to develop a practical system with an intuitive interface that could be used in an ICU, emergency room and/or resource-limited settings. By working closely with neurosurgeons in defining the app™s scope that aligns with their needs, iSurgARy addresses a critical need for a low-cost, portable, and user-friendly IGNS system in resource-limited settings and emergency situations. Ventriculostomy, a common neurosurgical procedure, is particularly suited to such a system. Our initial prototype demonstrates promising accuracy and sets the stage for continued refinement and clinical evaluation. Successful implementation of iSurgARy has the potential to significantly improve the accuracy of EVD placement, leading to reduced healthcare costs, mortality and morbidity rates. The work described in this paper was funded by XXX Conflict of interest: None declared.",
        "keywords": "keywords: \nVentriculostomyLow-CostAugmented Reality Neurosurgery Mobile ComputingResource-limited Settings"
    },
    {
        "id": 19,
        "title": "Language-centered Human Activity Recognition",
        "abstract": "Abstract.Human Activity Recognition (HAR) using Inertial Measurement Unit (IMU) sensors is critical for applications in healthcare, safety, and industrial production. However, variations in activity patterns, device types, and sensor placements create distribution gaps across datasets, reducing the performance of HAR models. To address this, we propose LanHAR, a novel system that leverages Large Language Models (LLMs) to generate semantic interpretations of sensor readings and activity labels for cross-dataset HAR. This approach not only mitigates cross-dataset heterogeneity but also enhances the recognition of new activities. LanHAR employs an iterative re-generation method to produce high-quality semantic interpretations with LLMs and a two-stage training framework that bridges the semantic interpretations of sensor readings and activity labels. This ultimately leads to a lightweight sensor encoder suitable for mobile deployment, enabling any sensor reading to be mapped into the semantic interpretation space. Experiments on four public datasets demonstrate that our approach significantly outperforms state-of-the-art methods in both cross-dataset HAR and new activity recognition. The source code will be made publicly available.",
        "corpus": "Human Activity Recognition (HAR) using Inertial Measurement Unit (IMU) sensors is critical for applications in healthcare, safety, and industrial production. However, variations in activity patterns, device types, and sensor placements create distribution gaps across datasets, reducing the performance of HAR models. To address this, we propose LanHAR, a novel system that leverages Large Language Models (LLMs) to generate semantic interpretations of sensor readings and activity labels for cross-dataset HAR. This approach not only mitigates cross-dataset heterogeneity but also enhances the recognition of new activities. LanHAR employs an iterative re-generation method to produce high-quality semantic interpretations with LLMs and a two-stage training framework that bridges the semantic interpretations of sensor readings and activity labels. This ultimately leads to a lightweight sensor encoder suitable for mobile deployment, enabling any sensor reading to be mapped into the semantic interpretation space. Experiments on four public datasets demonstrate that our approach significantly outperforms state-of-the-art methods in both cross-dataset HAR and new activity recognition. The source code will be made publicly available. Human Activity Recognition (HAR) based on data collected from Inertial Measurement Unit (IMU) sensors on mobile platforms such as smartphones and wearable devices is one of the key problems in mobile computing due to its important applications in fields such as healthcare (Subasi et al., 2018, 2020), safety (Sun and Chen, 2022; Liagkou et al., 2022), and industrial production (Niemann et al., 2021; Zheng et al., 2018). However, variations in activity patterns, device types, and sensor placements across different individuals result in significant distributional differences between datasets, even when capturing the same activity (Xu et al., 2023; Liu et al., 2022). Consequently, the performance of HAR models deteriorates considerably in cross-dataset human activity recognition scenarios (Xu et al., 2023). Therefore, developing HAR models capable of generalizing across different datasets remains a critical research problem that needs solutions. Existing work on cross-dataset HAR can be broadly categorized into two approaches: domain adaptation and data augmentation. Domain adaptation methods (Chang et al., 2020; Qin et al., 2019; Khan et al., 2018; Zhou et al., 2020) aim to bridge the distribution gap between source and target datasets to enhance model performance in training. Data augmentation methods (Um et al., 2017; Xu et al., 2023; Qian et al., 2022; Saeed et al., 2019) seek to increase the diversity of training data to improve the model™s generalization ability. While both approaches are valuable, they still suffer from two limitations: (i) Most existing work overlooks the physical meanings of activities. This can exacerbate the impact of classification errors on downstream applications. For instance, misclassifying a walking activity as running may have a relatively minor effect compared to misclassifying it as a significantly different activity such as biking (see quantitative results in Evaluation). (ii) Moreover, cross-dataset scenarios often introduce new activities that these methods struggle to handle, as these new activities are typically missing from the training phase. We argue that semantic interpretations of sensor readings and activity labels can play a crucial role in addressing the aforementioned limitations as shown in Figure 1. (i) For cross-dataset heterogeneity, interpreting sensor readings based on its physical meanings can help identify patterns associated with specific activities (e.g., jogging often exhibits regular periodicity). The underlying intuition is that, despite the variability across datasets, activities may still reveal common underlying patterns. By identifying these patterns, we can map the heterogeneous data into a common space, thereby mitigating the impact of dataset variability. (ii) When dealing with new activities, semantic interpretations of activity labels can enhance their meaning by generating detailed descriptions. Combining semantic interpretations of both sensor readings and activity labels creates the potential to map new data to new activity labels. However, manual semantic interpretation is resource-intensive. The recent success of large language models (LLMs) offers a promising solution, enabling the generation of semantic interpretations from both sensor readings and activity labels through natural language (Jiang et al., 2024; Zhang et al., 2023). Research has shown that LLMs have the ability to perceive aspects of the physical world (Xu et al., 2024), providing an opportunity to leverage these capabilities to bridge the gap in cross-dataset HAR through automated semantic interpretation. Despite the promise of semantic interpretations using LLMs, several challenges remain to be addressed: (i) Generation Challenge: The semantic interpretations generated by LLMs can suffer from issues like hallucinations and randomness. Ensuring consistent and high-quality semantic interpretations remains a significant challenge. (ii) Alignment Challenge: Once LLMs generate semantic interpretations of sensor readings and labels, the next step is to align them for Human Activity Recognition (HAR), i.e., aligning sensor reading and label based on their interpretation. While language models can be used to align these interpretations, general LLMs lack specific knowledge of IMU data and human activity recognition, limiting their ability to accurately understand and reason about activities. (iii) Deployment Challenge: Frequent access to cloud-based LLMs or deploying them locally on mobile devices is impractical due to high latency and resource constraints. A lightweight framework is needed to effectively leverage LLMs for on-device HAR. To address these challenges, we design LanHAR a novel system that leverages LLMs to generate semantic interpretations of sensor readings and activity labels, aimed at improving cross-dataset human activity recognition. LanHAR consists of three key components. (i) Generation: we carefully design prompts to guide LLMs in generating semantic interpretations and develop an iterative re-generation method to ensure high-quality semantic outputs. (ii) Alignment: we design a semantic interpretation alignment module to enhance understanding of semantic interpretations based on a pre-trained text encoder to encode interpretations and two contrastive learning tasks to improve the alignment between activities and sensor readings. (iii) Deployment: We introduce a lightweight, two-stage training and inference framework. First, we train the semantic interpretation alignment module, then design and train a sensor reading encoder to map sensor data to the semantic interpretation space. For inference on mobile devices, we use the trained encoder to obtain sensor reading encodings and measure their similarity to activity label semantics to determine the HAR results. In particular, our main contributions are as follows. To our knowledge, we are the first to leverage LLMs to generate semantic interpretations for cross-dataset human activity recognition. Our approach introduces a novel perspective by utilizing semantic interpretations from both sensor readings and activity labels, which helps mitigate the impact of cross-dataset heterogeneity and opens the door to recognizing new activities. We design LanHAR, a novel system that leverages LLMs to generate semantic interpretations of sensor readings and activity labels, addressing the challenge of cross-dataset human activity recognition. The system features a semantic interpretation generation process with an iterative re-generation method to ensure high-quality outputs, alongside a two-stage training framework that transfers the capabilities of large language models (LLMs) to mobile devices. We evaluate our system based on four public datasets. The experimental results demonstrate that it outperforms other state-of-the-art methods in cross-dataset activity recognition, achieving an improvement of 7.21% in accuracy and 13.31% in F1 score. Additionally, for new activity recognition, it shows further enhancement, with improvements of 74.65% in accuracy. Existing research (Xu et al., 2024; Ji et al., 2024) has demonstrated that LLMs possess the ability to perceive and interpret the physical world. For instance, LLMs can analyze various types of sensor data to infer a person™s location or activity. Building on this capability, we explore the use of LLMs to analyze sensor readings and describe activity labels, thereby generating semantic interpretations for both. Specifically, we design prompts that include data introduction, data analysis, relevant knowledge, and task instructions (detailed in Section 3.3) to guide LLMs in generating semantic interpretations of sensor readings. Similarly, we guide LLMs to produce descriptions related to activity labels. Figure 2 provides an example where GPT-4 generates its understanding of jogging alongside an analysis of sensor data sequences. We explain why LLM-generated semantic interpretation offers a valuable approach to addressing data heterogeneity in cross-dataset HAR. The key intuition is that, despite variations in how heterogeneous data manifests, it often has underlying common activity patterns. By identifying these shared patterns, we can map the original heterogeneous sensor data into a common shared space (i.e., a language space with encoded semantic interpretation), effectively reducing the impact of data heterogeneity. To understand how data heterogeneity is mitigated using generated semantic interpretations, we compare data distributions under three settings: raw data, data encoded by encoders, and generated semantic interpretations. Data encoding is a common approach used in existing research (Auge et al., 2021), aimed at learning better representations or embeddings of sensor readings. For data encoding, we employ a self-supervised learning model, BERT (Devlin et al., 2018), as a representative method, following prior work (Xu et al., 2021) that masks and predicts parts of the sensor readings. For semantic interpretation, we use GPT-4 to generate interpretations from raw IMU data based on a designed prompt (detailed in Section 3.3). These interpretations are then input into a pre-trained language model (BERT) to produce representations in the language space. We use two public datasets”UCI (Reyes-Ortiz et al., 2016) (D1) and Motion (Malekzadeh et al., 2019) (D2) as a cross-dataset example including four common activities: walking, sitting, going upstairs, and going downstairs. Qualitative results: We visualize the results from the three settings using t-SNE to project them onto a 2D surface. As an example, we plot two activities, sit (blue) and walk (orange), in Figure 3. Initially, in the raw data, the same activities from different datasets are far apart. After applying self-supervised learning, the distribution gap between the datasets decreases, but the activities also become less distinguishable. However, when using semantic interpretations, the distribution gap reduces further, and the same activities (same color) cluster more closely, demonstrating improved grouping of the same activities. Quantitative results: We quantitatively assess the impact of introducing semantic interpretations for sensor readings across all activities. To measure the distribution differences between the two datasets under the three settings, we use Kullback-Leibler (KL) divergence, a widely used metric for comparing distributions. Table 1 presents the KL divergence for all activities across the two datasets under each setting. The results show that introducing semantic interpretations significantly reduces the distribution gap for all activities. On average, the KL divergence is reduced by 56.89%. Recognizing new activities has been a longstanding challenge in HAR, particularly for machine learning-based methods, which are unable to identify activities that were not present during the training phase. The most relevant approach to address this challenge involves using predefined attributes of activities to establish connections between new and existing activities (Cheng et al., 2013). However, the performance of this method relies heavily on the quality of manually defined activity attributes, making it costly and difficult to generalize to new labels. We argue that combining the semantic interpretations of sensor readings and labels offers a new perspective for addressing this challenge. By converting all activity labels into a language space using their semantic interpretations, we can represent labels even without corresponding sensor readings (which is common for new labels). Similarly, with sensor readings mapped into the same language space, we can match each reading to a label, regardless of whether the label was previously encountered. Figure 4 illustrates this concept, where we assume biking and going upstairs are known activities from the training phase, while going downstairs is a new activity. In our approach, we first convert all activity labels into the same language space using their LLM-generated semantic interpretations. We then obtain semantic interpretations for the sensor readings. Even though going downstairs is unseen during training, we can still measure its distance to all activity labels in the language space, enabling the recognition of new activities. In our setting, we assume that we have the knowledge of possible activity labels in the inference stage. We consider LanHAR to be implemented in a cloud-client setting, where each client possesses locally collected, unlabeled IMU datasets. The cloud holds an initial source dataset, XssuperscriptX ^ s , collected from a subset of clients, with labels YssuperscriptY ^ s provided by clients or experts. Given an unlabeled target dataset XtsuperscriptX¡ ^ t from the clients, our goal is to achieve high activity recognition accuracy on XtsuperscriptX¡ ^ t , even when the label set YssuperscriptY ^ s differs from YtsuperscriptY¡ ^ t . We design LanHAR, a novel system that leverages LLM-generated semantic interpretations of sensor readings and activity labels for cross-dataset human activity recognition. As shown in Figure 5, LanHAR consists of four processes: (1) LLMs for semantic interpretations. We utilize LLMs to generate semantic interpretations of sensor readings XssuperscriptX ^ s and activity labels YssuperscriptY ^ s . We introduce an iterative re-generation process to filter out low-quality interpretations due to issues such as hallucinations. (2) Text encoder for semantic interpretations alignment. We train a text encoder (i.e., initialized by a pre-trained language model) to encode and align the semantic interpretations generated in step (1). This alignment enables us to leverage language for human activity recognition by matching the semantic interpretation of sensor readings to the most similar semantic interpretation of activity labels within the language space. (3) Sensor encoder for mapping sensor reading to language space. We train a sensor reading encoder, based on the text encoder from step (2), which is capable of mapping any sensor reading into the language space resulted from semantic interpretations. (4) Inference on the mobile device. For inference, we deploy the sensor encoder from (3) on mobile devices to generate sensor embeddings, which are then compared with pre-stored (new) activity label embeddings to determine the activity through similarity computation. We introduce how we guide LLMs to generate semantic interpretations of sensor readings and activity labels. We also discuss our approach to handling low-quality interpretations to ensure the accuracy and reliability of LLM responses. We carefully design a prompt to help LLMs better understand the problem setting and deliver more accurate and meaningful answers. An example is shown in Figure 6. The prompt consists of four key parts: data introduction, data analysis, knowledge, and task introduction. (i) Data introduction: We provide details about the data, including its source, the meaning of each dimension, the sampling frequency, and the context in which the data was collected during different activities. (ii) Data analysis: Intermediate steps (i.e., Chain-of-Thought (Wei et al., 2022)) can enhance LLMs™ reasoning abilities. Our offline testing revealed that while LLMs are capable of performing various data analyses, skipping intermediate steps often leads to less confident responses. To address this, we design multiple auxiliary analysis steps, including amplitude, frequency, time series analysis, and statistical measures like mean, standard deviation, maximum, and minimum values. These steps help LLMs generate more accurate and precise answers. (iii) Knowledge: We include relevant background knowledge to support the LLMs in making accurate judgments. In many cases, specialized knowledge is essential for accurate interpretation. For example, we describe typical patterns observed in accelerometer and gyroscope readings during activities like running, which aids the LLMs in focusing on the appropriate knowledge area. This knowledge can be sourced from public databases like Wikipedia or generated by LLMs. (iv) Task introduction: We outline the task clearly, specifying what the LLMs need to achieve and the format of the final output. This ensures that the LLMs have a clear understanding of the task requirements and can deliver standardized responses that meet our expectations. Similarly, we design a prompt to guide LLMs in generating semantic interpretations of activity labels. In this prompt, we outline the task for the LLMs, instructing them to generate a description for a given activity. The description should cover three key aspects: a general overview of the activity, the potential states or patterns detected by the accelerometer and gyroscope during the activity, and the body parts likely involved in performing the activity. An example of this prompt is shown in Figure 7. The quality of LLMs™ interpretations is critical for our approach. However, due to hallucinations or other incidental factors, LLMs may sometimes generate inaccurate or illogical responses. Through manual review, we identified many such cases when generating semantic interpretations. To improve the accuracy of LLM responses, we developed an iterative re-generation method to ensure high-quality outputs. The key intuition is that filtering out low-quality responses will result in a lower KL divergence within the same activity. As illustrated in Figure 8, the framework first identifies the kkk most problematic responses based on KL divergence. These data points are re-input into the LLM to regenerate the semantic interpretations. We then assess whether to incorporate the new interpretations based on their ability to reduce the overall KL divergence. The details are as follows. We show in the evaluation this process improves the data quality. (1) Filter out inaccurate semantic interpretations: We randomly / the generated semantic interpretations of the same activity into two parts, AAA and BµBB, and calculate the initial KL divergence between them. Next, we perform an iterative selection process: in each iteration, we identify and select the data sample from one part that most negatively impacts of KL divergence, adding it to a selection set. After selecting a data sample, it is removed from its original part. This process is repeated until the selection set contains kkk data samples. (2) Regenerate new semantic interpretations by LLMs: After identifying the kkk data samples, we update the task introduction in the prompt (Section 3.3.1) to inform the LLMs of the inaccurate responses. The revised prompt includes the following: This is your previous response to the task. Please analyze it step by step to identify any logical errors, inconsistencies with real-world knowledge, or discrepancies with the input data. Provide a corrected response according to the required format. (3) Incorporate the new semantic interpretation based on whether it can reduce the overall KL divergence: If the newly generated interpretations reduce the KL divergence, we accept the new semantic interpretation; otherwise, we retain the original one. We repeat this process until either the KL divergence no longer shows significant improvement for multiple continuous iterations or the specified number of iterations is reached. After generating the semantic interpretations of sensor readings and activity labels, we train a text encoder (initialized with a pre-trained language model) to encode and align these interpretations in a shared language space. To ensure the text encoder effectively captures the relationship between the semantic interpretations of sensor readings and activity labels, as well as their alignment, we design two subtasks: a contrastive learning task and a reconstruction task. We adopt a pre-trained language model as the text encoder due to its robust ability to understand semantic interpretations through language. Specifically, we use the text encoder to encode the semantic interpretations of sensor readings zsuperscript§ ^ z and activity labels lsuperscript™ ^ l , which are represented as ™=gen(z)™subscript”ensuperscript§ = g _ en ( S ^ z ) and =gen(l)subscript”ensuperscript™ = g _ en ( S ^ l ), where gen(‹…)subscript”en‹…g_{ _ en ( ‹… ) denotes the text encoder. In our work, we utilize the BERT architecture as the text encoder, initializing it with pre-trained BERT parameters (Devlin et al., 2018). BERT consists of multiple Transformer encoder layers, each incorporating multi-head self-attention mechanisms and feedforward neural networks, allowing it to generate word representations in a bidirectional context. We chose BERT because of its superior ability to understand text semantics and compare text similarity, as it captures word meanings in a bidirectional context, enabling nuanced comprehension and effective comparisons. However, this text encoder structure is flexible and can be substituted with other language models if needed. Inspired by multimodal alignment (e.g., vision and language) (Radford et al., 2021), we adopt contrastive learning to align the semantic representations of activity labels with those of sensor readings. This alignment enables the use of language to guide human activity recognition by matching the semantic interpretations of sensor readings to the most similar activity label interpretations in the language space. First, we normalize both the embeddings of the semantic interpretations of sensor readings and activity labels to have unit length. Then, we compute the similarity between the normalized embeddings using the dot product, defined as: The goal of training is to maximize the similarity between matching pairs of semantic interpretations and minimize the similarity between non-matching pairs. Given a batch of NNN matching pairs, the loss function is defined as: where is a temperature parameter that scales the similarities, and Hisubscript»H_{i}H _ i and ZisubscriptZ_{i}Z _ i represent the embeddings of the semantic interpretations of sensor readings and activity labels for the iii-th matching pair. We design two contrastive learning subtasks to enable the text encoder to fully understand the knowledge related to human activity recognition. Firstly, we use pre-defined category-level relations (details in Section 4.4) to guide the comparison between the semantic interpretations of sensor readings and activity labels. For semantic interpretations of activity labels, two activity labels in the same category have a higher similarity than two activity labels in the different categories. The loss function for this task is defined as where «a1subscript«subscript1 _ a _ 1 represents comparison pairs among the semantic interpretations of activity labels. Hicmsuperscriptsubscript»subscriptH_{i}^{c_{m}}H _ i ^ c _ m and Hjcmsuperscriptsubscript»subscriptH_{j}^{c_{m}}H _ j ^ c _ m denote the embeddings of two activity label samples iii and jjj, that belong to category cmsubscriptc_{m}c _ m . Similarly, we apply the same comparison method to the semantic interpretations of sensor reading. The loss function for this task is defined as where «a2subscript«subscript2 _ a _ 2 represents comparison pairs among the semantic interpretations of sensor readings. ZicmsuperscriptsubscriptsubscriptZ_{i}^{c_{m}}Z _ i ^ c _ m and ZjcmsuperscriptsubscriptsubscriptZ_{j}^{c_{m}}Z _ j ^ c _ m denote the embeddings of two semantic interpretations of sensor readings samples iii and jjj, that belong to category cmsubscriptc_{m}c _ m . Secondly, to enrich the semantic interpretations of activity labels and enable the model to adapt to diverse descriptions, we generate multiple descriptions for each activity label. These descriptions guide the comparison, ensuring that the similarity of descriptions for the same activity is higher than that for different activities. The loss function for this task is defined as Ca3=«a3(lnƒ(s(Hilm‹…Hjlm)s(Hilm‹…Hjln)))subscriptsubscript¶subscript3subscriptsubscript«subscript3™ ‹…subscriptsuperscript»subscript™subscriptsuperscript»subscript™ ‹…subscriptsuperscript»subscript™subscriptsuperscript»subscript™ H^{l_{m}}_{j}})-s({H^{l_{m}}_{i} H^{l_{n}}_{j}})))caligraphic_L _ C _ a _ 3 = - _ caligraphic_P _ a _ 3 ( l n ƒ ( s ( H ^ l _ m _ i ‹… H ^ l _ m _ j ) - s ( H ^ l _ m _ i ‹… H ^ l _ n _ j ) ) ), where «a3subscript«subscript3 _ a _ 3 represents comparison pairs among the semantic interpretations of activity labels. Hilmsubscriptsuperscript»subscript™H^{l_{m}}_{i}H ^ l _ m _ i and Hjlmsubscriptsuperscript»subscript™H^{l_{m}}_{j}H ^ l _ m _ j denote the embeddings of two different semantic interpretations of the activity label lmsubscript™l_{m}l _ m . Lastly, we design a reconstruction task to retain the characteristics of the language model, ensuring it can function as a language model to understand the new activity descriptions and new sensor pattern descriptions. We design a text decoder to reconstruct semantic interpretations of sensor reading and activity labels. The objective of the text decoder can be formulated as ^³=gde(™)superscript^³subscript”de™ S ^ z = g _ de ( Z ) and ^¥=gdesuperscript^¥subscript”de S ^ l = g _ de ( H ), where ™™ and denote embeddings of semantic interpretations of sensor readings and activity labels, respectively, and ^³superscript^³ S ^ z and ^¥superscript^¥ S ^ l denote reconstructed semantic interpretations of sensor reading and activity labels, respectively. The text decoder consists of multiple layers of the Transformer decoder, each containing multi-head attention mechanisms and feedforward neural networks. The loss function of the reconstruction task is defined as re=1NiNCE(S^iz,Siz)+1NiNCE(S^il,Sil)subscriptre1superscriptsubscript¶subscriptsuperscript^†§subscriptsuperscript†§1superscriptsubscript¶subscriptsuperscript^†™subscriptsuperscript†™ _ re = / 1 N _ i ^ N C E ( over^ S ^ z _ i , S ^ z _ i ) + / 1 N _ i ^ N C E ( over^ S ^ l _ i , S ^ l _ i ), where CE denotes Cross-Entropy (CE) loss and NNN denotes the number of training samples. In total, the loss function of training the text encoder is defined as where Î±¼ and Î²½ are weighting factors that control the contributions of the different tasks. To bridge IMU sensor readings with their semantic interpretations and enable mobile deployment, we introduce a sensor encoder that maps IMU sensor readings to the language space, aligning them with the semantic interpretation encodings of the same sensor readings. We first describe the structure of the sensor encoder, followed by an explanation of the training process. The objective of the sensor encoder is to capture key features and generate embeddings from IMU data, which can be formulated as: =fsensubscript“ = f _ s e n ( X ) where SdimÃLsuperscriptsubscript†dim¿ L}X R ^ S _ dim Ã L represents the IMU sensor readings, and denotes the corresponding embeddings. The structure of the sensor encoder is flexible, allowing for either a CNN or Transformer-based architecture. In our work, we adopt a Transformer-based sensor encoder. First, following (Xu et al., 2021), we normalize the IMU sensor readings to obtain the normalized readings, §¨«superscript§¨« ^ nor . Next, we project the normalized sensor readings through a linear layer and apply layer normalization to the projected data: ²=LayerNorm(Wp§¨«)superscript²LayerNormsubscriptŠsuperscript§¨« ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT = LayerNorm ( W _ p X ^ nor ) where WpsubscriptŠW_{p}W _ p is the weight matrix of the linear layer. Then, we add positional encoding to the normalized data:=²+PositionEncoding(²)superscript0superscript²PositionEncodingsuperscript² { ^ 0 = X ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT + PositionEncoding ( X ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ) The position-encoded data superscript0 ^ 0 is then fed through the Transformer encoder, which processes the data across multiple layers to capture complex relationships and dependencies. Each Transformer layer consists of two main sub-layers: Multi-Head Self-Attention and a Feed-Forward Network, both followed by layer normalization and residual connections. For the iii-th encoder layer, the operations are as follows: First, the Multi-Head Self-Attention mechanism calculates self-attention scores, producing the output: Next, the output from the Multi-Head Self-Attention sub-layer is passed through the Feed-Forward Network: This process is repeated across multiple encoder layers, with the output of each layer serving as the input for the next. Similar to the alignment task in Section 3.4.2, we adopt contrastive learning to align embeddings of sensor reading with the embeddings of semantic interpretation of sensor reading. First, the embeddings of both the sensor reading and its semantic interpretation are normalized to unit length. Their similarity is calculated using the dot product between the normalized embeddings, denoted as s(,™)=‹…™™. ™‹…norm™norm™s( }}{|| ( E , Z ) = / E | | E | | ‹… / Z | | Z | | . The goal of the training is to maximize the similarity between matching pairs while minimizing the similarity between non-matching pairs. Given a batch of N sensor reading and semantic interpretation of sensor reading pairs, the loss function is computed as: where is a temperature parameter. Training process: (i) Training the text encoder: We use the semantic interpretations of sensor readings and activity labels from the source dataset to train the text encoder, embedding human activity recognition-related knowledge and semantics. (ii) Training the sensor encoder for all datasets using raw IMU data and its semantic interpretations (i.e., unlabeled data): The sensor encoder is designed to map any sensor reading into the semantic interpretation space of sensor readings. Inference process: For inference, the sensor encoder and activity label embeddings are deployed on mobile devices. IMU sensor readings are input into the encoder to generate embeddings, which are then compared with the activity label embeddings to calculate similarity. The activity label with the highest similarity is identified as the recognized activity. This process is also used for recognizing new activities. We evaluate the performance of LanHAR using four public datasets that have been extensively employed in existing research (Xu et al., 2023). (1) HHAR (Stisen et al., 2015) includes individuals performing six activities (sitting, standing, walking, going downstairs, going upstairs, and biking) using six different mobile phone types. (2) UCI (Reyes-Ortiz et al., 2016) captures six activities (i.e., standing, sitting, lying, walking, going downstairs, and going upstairs). (3) Motion (Malekzadeh et al., 2019) involves 24 participants performing six activities (sitting, standing, walking, going upstairs, going downstairs, and jogging). (4) Shoaib (Shoaib et al., 2014) includes 10 participants engaged in seven activities (sitting, standing, walking, going upstairs, going downstairs, jogging, and biking). We preprocess all datasets to ensure consistent sampling rates and window sizes. The sampling rate is set to 20 Hz to simulate a resource-constrained scenario, following prior studies (Xu et al., 2023). For cross-dataset activity recognition, we focus on four common activities (walking, going upstairs, going downstairs, and sitting) across the four datasets. We select one dataset as the source and one of the remaining three as the target, resulting in a total of 12 combinations. Datasets are categorized based on their role as either source or target. For source datasets, each is split into a training set (80%) and a validation set (20%). For target datasets, the entire dataset is used as the test set to directly assess cross-dataset performance. For new activity recognition, we train the model on the four common activities from the source dataset. We then select activities from the target dataset that differ from the four common activities as new activities. Finally, we evaluate the model™s ability to recognize these new activities by determining which activity labels the new data corresponds to. We compare our method LanHAR with three categories of baselines including conventional HAR, cross-dataset HAR, and new activity baselines. (i) Conventional HAR baselines: DCNN (Yang et al., 2015) are widely used in the field of human activity recognition for their ability to automatically extract local features from time-series sensor data. Transformers (Vaswani, 2017) utilize self-attention mechanisms to capture long-range dependencies in sensor data. (ii) Cross-dataset HAR baselines: SDMix (Lu et al., 2022) It is a method for cross-domain activity recognition that uses the mixup method that addresses semantic inconsistencies caused by domain gaps. UDAHAR (Chang et al., 2020) is a method for cross-dataset activity recognition using domain adaptation. UniHAR (Xu et al., 2023) is a method for cross-dataset activity recognition. It designs physical-informed augmentation methods to improve the cross-dataset HAR performance. (iii) New activity baselines: DCNN (Yang et al., 2015) To adapt it for recognizing new activities, we fine-tune it using a small portion of data from the target dataset. Nuactiv (Cheng et al., 2013) is a method designed for new activities. It predefined activity label-related attributes: actions of a specific part of the body. (iv) Variants of our model: (1) LanHAR without Subtasks of Text Encoder (w/o STE). We remove the subtasks from the text encoder, focusing its training on alignment. (2) LanHAR without Training of Text Encoder (w/o TTE). We remove the training part of the text encoder and directly use a pre-trained BERT model to replace the text encoder. (3) LanHAR with LLAMA2 (w LLAMA2). We replace GPT-4 with LLAMA2 for obtaining semantic interpretations. (4) LanHAR with GPT-3 (w GPT-3). We replace GPT-4 with GPT-3 for obtaining semantic interpretations. (5) LanHAR with Different Language Model (w DLM). We use the transformer architecture to replace the BERT in LanHAR. (6) LanHAR without Iterative re-Generation (w/o IG). We remove iterative re-generation method. (7) LanHAR without Iterative re-Generation and designed Prompt (w/o IGP). We remove the iterative re-generation method and designed prompt. We directly use a simple prompt only containing data and task requirement. We evaluate our model using two metrics: accuracy and F1 score. For cross-dataset HAR performance, we use accuracy to measure the correctness of the classification results for the selected four activities. For new activities, we use accuracy to evaluate the correctness of the classification results only for the new activity. We implement our method and baselines using PyTorch 1.9.0 in a Python 3.7 environment and train them using two NVIDIA RTX A5000 GPUs, each with 24 GB of memory. For LLMs, we choose GPT4 as our LLMs to obtain semantic interpretations of sensor readings and activity labels. For the text encoder, we use the BERT-base architecture as its structure and initialize it with the pre-trained BERT-base parameters (Devlin et al., 2018). For the sensor encoder, we set the embedding dimension to 768768768768 for consistency with pre-trained BERT. The multi-head attention parameter is set to 2222 and the encoder layer consists of 3333 layers. We apply the AdamW optimizer with a learning rate of 1e-5 and set the batch size to 32. We compare our method with various baselines, with the results presented in Table 2. Overall, our method demonstrates superior average performance across 12 different source and target dataset combinations, leading in the majority of settings. Compared to conventional HAR baselines, our method consistently outperforms them. These traditional baselines are not specifically designed for cross-dataset HAR, and their strong performance typically depends on large, carefully collected datasets. Compared to cross-dataset HAR baselines, our method outperforms the cross-dataset HAR baselines (except in the setting from Shoaib to Motion). Although SDMix and UDAHAR are also designed for cross-dataset HAR, they have certain requirements. For instance, UDAHAR may be more suitable for datasets with smaller distribution gaps. This consistent performance indicates the robustness and reliability of our method in diverse conditions and datasets. We compare our method with two new activity recognition baselines, with the results presented in Table 3. Specifically, we train the model using four common activities from the source dataset and evaluate it on different activities from the target dataset that are not among the four common activities, treating them as new activities. The trained model is then used for recognizing these new activities, i.e., determining which activity new data belong to. For new activity HAR, our model outperforms the baseline. Our model™s performance is superior to that of a fine-tuned DCNN model, which indicates its advantage over simple fine-tuniing on small datasets. Additionally, our model performs better than Nuactiv, which relies on predefined attributes for activities. Defining these attributes is challenging and varies significantly across different activity contexts, directly impacting Nuactiv™s performance. This limitation hinders Nuactiv™s performance in cross-dataset settings. We further evaluate the category-level accuracy of HAR performance. An activity is considered correctly classified if it is categorized into a similar activity category, and incorrect if placed into a dissimilar category. Specifically, we group seven activities (walking, jogging, biking, sitting, standing, going upstairs, going downstairs, and lying) into three categories based on their similarity: Category 1 (walking, jogging, biking), Category 2 (sitting, standing, lying), and Category 3 (going upstairs, going downstairs). We compared our method with all the baselines. The average results are shown in Figure 10. As shown, our method improves category-level HAR accuracy compared to the baselines. This improvement has the potential to reduce the impact of classification errors on downstream applications. To assess the impact of the subtasks in the text encoder, we compare our method with a variant (i.e., w/o STE), as shown in Figure 10. In the w/o STE variant, we remove the subtasks (i.e., the two contrastive learning and reconstruction tasks) and focus only on aligning the semantic interpretations of sensor readings and activity labels. As shown in the figure, removing the subtasks leads to a decline in performance compared to our full model. This demonstrates that the subtasks we designed help the model better understand IMU sensor readings and HAR-related semantics. To further explore the effect of the text encoder for semantic interpretation alignment, we replace the carefully designed text encoder with a pre-trained BERT model and directly use it for training in stage 2. The results are shown in Figure 10. From the figure, it is clear that the performance of w/o TTE is lower than both w/o STE and our full model. This suggests that pre-trained language models without fine-tuning cannot fully capture HAR-related semantics or effectively align different HAR-related semantic interpretations. In our text encoder, we used a pre-trained BERT architecture and fine-tuned it with our designed tasks. To explore the impact of using different language model architectures on HAR performance, we replaced the BERT-based structure with GPT-2. As shown in Figure 12, this replacement resulted in decreased HAR performance. BERT is generally more suitable for tasks that require understanding sentence meaning and comparing text similarities, as its architecture and pre-training are optimized for capturing semantics and contextual relationships in a bidirectional manner. In contrast, GPT-2, with its unidirectional processing, is less effective at understanding sentence-level meaning and comparing text similarity, though it excels in generating coherent and contextually relevant text. This explains the performance drop when GPT-2 is used for HAR tasks. Given the varying capabilities of different LLMs, we explore the impact of three LLMs (GPT-4, GPT-3, and LLAMA2) on interpreting sensor readings and activity labels for HAR performance. As shown in Figure 12, the performance of GPT-3 and LLAMA2 is noticeably lower than that of GPT-4, highlighting the importance of the LLM™s capability in language-centered human activity recognition. To ensure the quality of the LLM™s interpretation of sensor readings, we (i) design specific prompts to guide the LLM™s responses and (ii) develop an iterative re-generation method to filter out low-quality semantic interpretations and generate improved ones. To demonstrate the effectiveness of the automatic iterative re-generation method, we compared the KL divergence of the same activities across different datasets before and after applying the method. Specifically, for each activity, we measure the KL divergence between any two datasets both with and without applying the iterative re-generation method (i.e., w iterative re-generation and w/o iterative re-generation), as shown in Figure 14. After applying the method, the average KL divergence for the same activity across different datasets decreased by 58.75%, indicating that the automatic iterative re-generation method significantly improves the quality of LLM responses and reduces the distribution gap in heterogeneous data. To verify the impact of these methods on HAR performance, we compare our method with two variants: one using only simple prompts (w/o IGP) and another using only specifically designed prompts (w/o IG). As shown in Figure 14, the performance when using only simple prompts is the lowest, indicating that LLMs require guidance to generate high-quality responses. The combination of specially designed prompts and the automatic iterative re-generation method performs the best, demonstrating that our approach effectively enhances the quality of LLM responses, leading to improved human activity recognition outcomes. In our model, selecting the appropriate parameters for the sensor encoder is critical, as it directly affects its suitability for deployment on mobile devices (e.g., ensuring the parameter size is sufficiently small). In this section, we primarily analyze the impact of the embedding dimensions and the number of encoding layers on performance. Figure 16 shows the performance comparison w.r.t.formulae-sequence¤¡w.r.t.w . r . t . the number of attention heads in the sensor encoder. Overall, the performance is relatively stable under different numbers of attention heads. We chose the number of attention heads of 2 because it provides good performance while reducing the model™s parameter size. Figure 16 shows the performance comparison with respect to different numbers of encoding layers in the sensor encoder. As observed, a smaller number of encoding layers (e.g., 2 layers) results in insufficient representation, while a larger number (e.g., 6 layers) increases model complexity. We selected 3 encoding layers as it strikes a balance, delivering good performance without significantly increasing model complexity. To ensure the sensor encoder is suitable for deployment on mobile devices, we analyze its parameter size and inference time. When deployed on a mobile device, the sensor encoder, along with the activity label embeddings (approximately 0.01 MB of memory), requires around 61 MB of total memory. For inference, the execution time for processing one IMU sample (i.e., recognizing an activity) on a Pixel 7 device is 23.71ms. These results confirm that the sensor encoder can operate efficiently on mobile devices. Lessons learned: We summarized the following lessons to benefit future studies: (1) Introducing semantic interpretation can mitigate heterogeneity in cross-dataset HAR and offers the potential for recognizing new activities, as indicated in Table 2 and Table 3. (2) The success of LLMs in cross-dataset HAR is partly due to their ability to summarize patterns and represent them using language. This leads to lower KL divergence between cross-dataset data distributions compared to raw data. (3) Improving the quality of LLM responses through iterative re-generation, guided by KL divergence, enhances HAR performance. The evaluation shows that generating high-quality responses has a direct positive impact on HAR accuracy. Comparison to other ways of utilizing LLMs: In addition to our approach, there are two other potential methods for utilizing LLMs in HAR: training a foundation model specifically for HAR or fine-tuning an existing open-source LLM, such as Llama. A key advantage of our approach lies in its low cost and flexibility. On one hand, our method does not require large amounts of data or computing resources for model training or fine-tuning. On the other hand, different LLMs can be flexibly integrated into our framework. As demonstrated in our evaluation, incorporating a high-quality LLM could further enhance the performance of our method. Other ways of improving LLMs responses: There are ongoing efforts in the AI community aimed at improving the trustworthiness of LLMs through methods such as retrieval-augmented generation and memory-augmented models. Our approach offers a general framework for post-hoc factuality verification, where advancements in these AI methods can be seamlessly integrated into our framework. Limitations: The process of obtaining semantic interpretations using LLMs is time-consuming, primarily due to the slow inference speed of current LLMs. We anticipate that future advancements in accelerating LLMs will enhance the efficiency of our approach. Although we used four public datasets in the evaluation, they remain relatively small in scale with a limited number of activities. A larger dataset could provide a more comprehensive evaluation and further validate our method™s effectiveness. Human activity recognition: Human Activity Recognition (HAR) leverages data from various sensors such as accelerometers and gyroscopes to detect and classify human activities (Yao et al., 2017). To mitigate the issue of data heterogeneity, existing work can be classified into three categories, including self-supervised learning methods, domain adaptation methods, and data augmentation methods. Self-supervised methods (Xu et al., 2021; Saeed et al., 2019) aim to maximize the use of unlabeled data to extract valuable features. This approach allows models to require only a minimal amount of labeled data to efficiently adapt to downstream tasks. Despite their capabilities, these models still depend on a certain amount of labeled data to train classifiers, which may underperform when applied to target datasets that lack any labeled data. The core idea of domain adaptation methods (Chang et al., 2020; Qin et al., 2019; Khan et al., 2018; Zhou et al., 2020) involves taking a classifier that has been pre-trained on a source domain and using it with an unlabeled dataset from a target domain, then adjusting the source model™s weights to enhance its performance in the target domain. Data augmentation (Um et al., 2017; Xu et al., 2023; Qian et al., 2022; Saeed et al., 2019; Wang et al., 2022; Tang et al., 2021) in HAR aims to increase the diversity and quality of training data, thereby enhancing the model™s generalization ability and accuracy. However, they still face data heterogeneity and heterogeneity can amplify the impact of classification errors on downstream applications. Additionally, cross-dataset scenarios often introduce new activities that existing methods struggle to handle. Some work (Cheng et al., 2013) attempts to address the challenge of recognizing new activities by using predefined semantics, such as actions of specific body parts. These approaches train two mappings: one from sensor readings to predefined semantics, and another from predefined semantics to activity labels. However, the quality of these predefined semantics heavily impacts the model™s performance. Additionally, this work are not well-suited for cross-dataset new activities, as the new activities may differ significantly from the original dataset. LLM for human activity recognition: Recently, there has been a surge of interest in leveraging the capabilities of large language models (LLMs) to enhance mobile computing applications (Leng et al., 2024). UniHAR (Xu et al., 2023) explore the potential of leveraging large language models (LLMs) to comprehend and interact with the physical world. Kim et al. (Kim et al., 2024) harness the power of large language models (LLMs) to improve health prediction using data collected from wearable sensors. IMUGPT2.0 (Leng et al., 2024) utilize LLMs to help data generation since IMU-labeled data is challenging to collect. MotionGPT (Jiang et al., 2023) fuse language data with large-scale motion models, and motion-language pre-training that can enhance the performance of motion-related tasks becomes feasible. We design LanHAR, a novel system that leverages LLMs to generate semantic interpretations of sensor readings and activity labels for cross-dataset human activity recognition and new activity recognition. It adopts a two-stage training framework to transfer the capabilities of LLMs to mobile devices. The experimental results demonstrate that the proposed approach outperforms other state-of-the-art methods.",
        "keywords": "Human activity recognition, Natural language, Large language models"
    },
    {
        "id": 20,
        "title": "Retro-li: Small-Scale Retrieval Augmented Generation Supporting Noisy Similarity Searches and Domain Shift Generalization",
        "abstract": "AbstractThe retrieval augmented generation (RAG) system such asRetrohas been shown to improve language modeling capabilities and reduce toxicity and hallucinations by retrieving from a database of non-parametric memory containing trillions of entries. We introduceRetro-lithat shows retrieval can also help using a small scale database, but it demands more accurate and better neighbors when searching in a smaller hencesparsernon-parametric memory. This can be met by using a proper semantic similarity search. We further propose adding a regularization to the non-parametric memory for the first time: it significantly reduces perplexity when the neighbor search operations are noisy during inference, and it improves generalization when a domain shift occurs. We also show that theRetro-li™s non-parametric memory can potentially be implemented on analog in-memory computing hardware, exhibitingO(1)‚1O(1)O ( 1 )search time while causing noise in retrieving neighbors, with minimal (<1%) performance loss. Our code is available at:https://github.com/IBM/Retrieval-Enhanced-Transformer-Little",
        "corpus": "The retrieval augmented generation (RAG) system such as Retro has been shown to improve language modeling capabilities and reduce toxicity and hallucinations by retrieving from a database of non-parametric memory containing trillions of entries. We introduce Retro-li that shows retrieval can also help using a small scale database, but it demands more accurate and better neighbors when searching in a smaller hence sparser non-parametric memory. This can be met by using a proper semantic similarity search. We further propose adding a regularization to the non-parametric memory for the first time: it significantly reduces perplexity when the neighbor search operations are noisy during inference, and it improves generalization when a domain shift occurs. We also show that the Retro-li™s non-parametric memory can potentially be implemented on analog in-memory computing hardware, exhibiting O(1)‚1O(1)O ( 1 ) search time while causing noise in retrieving neighbors, with minimal (<1%) performance loss. Our code is available at: https://github.com/IBM/Retrieval-Enhanced-Transformer-Little 2402 Natural language processing is a rapidly growing field in machine learning. Advancements in large language models are mostly a product of the neural scaling law [20]. Not only are language models themselves becoming larger, but their training data also follows suit. While GPT-3 [3] was trained on 300 billion tokens, GPT-4 [36] was trained on around 13 trillion tokens. As a result, the information retained by a language model is difficult to update or fact-check. Many of these data sources contain discriminatory language or misinformation, which are difficult to remove given the sizes of these datasets [7]. In retrieval augmented generation (RAG) [26], a language model is enhanced with a non-parametric memory as depicted in Figure 1. During training and inference, for each input sequence, RAG searches this memory for the kkk most similar sequences (the so-called nearest neighbors) and uses them as additional input. RAG has been shown to help with under-fitted language models as the non-parametric memory increases with the number of training tokens [15]. Moreover, it has been shown to reduce toxic language and hallucinations [52] through its use of retrieval. Furthermore, unlike in purely parametric models, it is straightforward and does not require extensive computing power to update the retrieval database of a RAG model to reflect updated information. Finally, in a question-answering context, the ability to provide sources for the answer helps combat misinformation. This provides users with the opportunity to double-check the responses themselves, aiding the models™ interpretability. Despite all the aforementioned functional advantages of RAG, in many cases, the retrieval database of non-parametric memory contains trillions of tokens to be searched. In fact, in Retro [2], the scale begins at a retrieval database size of billions of tokens, which raises the question of search speed and optimization. Although similarity search acceleration libraries such as Faiss [19] and ScaNN [10] can compare millions of vectors in milliseconds, they quickly become a bottleneck, especially for retrieval databases containing trillions of tokens (e.g., see Section 3.3.1) . To address this critical bottleneck, we suggest three directions whose effectiveness is shown in our proposed Retro-li. The contributions of this paper are as follows: 1) We enable RAG to work with a small-scale database. In Retro-li, we change the embedding of the retrieval system, and for the first time add regularization in the form of Gaussian noise to the neighbor embeddings of the non-parametric memory. We employ a novel embedding model that excels in semantic similarity search allowing retrieval of higher quality neighbors despite searching in a sparse non-parametric memory, and the regularization consistently improves domain shift generalization. We show that the new retrieval helps with language modeling on small-sized retrieval databases over having no retrieval database. 2) We advocate for the use of in-memory computing (IMC) hardware to reduce the complexity of search. Analog IMC performs certain operations in place in-memory by exploiting the physics of e.g., non-volatile memory devices, offering O(1)‚1O(1)O ( 1 ) computational complexity for similarity search with a set of precomputed vectors (see [43] for an overview). This feature is highly valuable for the efficient realization of memory-augmented neural networks as shown in [21]. Due to nonidealities, however, the analog search operation of IMC is noisy. We simulate this behavior of the IMC hardware by adding a wide range of noise to the neighbor embeddings at inference time. We show that the resulting noisy retrieval does not decrease the Retro-li™s performance (i.e., a maximum of <1% drop) thanks to our training with regularization. This suggests deployment of the non-parametric memory on the IMC hardware which would significantly improve inference time, in addition to energy consumption and computational density. 3) For domain shift generalization, we plug-and-play the related domain database without any fine-tuning in Retro-li. This is encouraged by having a closer look at the Retro that reveals that size is not the most important aspect of the retrieval database: a smaller but relevant retrieval database improved the model performance w.r.t. an order of magnitude larger database. Although Retro advocates for a large retrieval database with as much variety as possible to make the model more general, we show that their system can be even more modular than that. In Retro-li, it is straightforward to replace the retrieval database with data from domain A if inference sequences are also from domain A. With this high degree of plug-and-play, there is no need to build a database with trillions of tokens from as many sources as possible. The user can easily build a new database for each domain of interest. Furthermore, we show Retro-li can also benefit from fine-tuning similar to the other models without a retrieval. Our proposed Retro-li is a medium-sized parametric model based on Retro with a small-scale non-parametric database. In Retro-li, the embedding model used for the neighbor search is a Bert based sentence similarity model called SBert. The architecture diagram of Retro-li is shown in Figure 2, where we enhanced GPT-2 attention blocks similarly to what has been down with Retro-fitting to improve our language modeling performance without extensive re-training. Retro-fitting as introduced in the Retro paper [2], refers to taking a language model not trained with retrieval and adding retrieval in the form of a frozen retrieval database and chunked cross-attention (CCA) blocks in order to incorporate the information from neighbor embeddings in the training sequences. To query the key-value retrieval database (DB), each input sequence is split into chunks. An input sequence contains 1024 tokens, for a chunk length of 64 tokens this is 16 chunks per sequence. The neighbors consist of [N, C], each N containing 64 tokens and C containing another 64 tokens. The retrieval is based on N only, the key of this key-value pair, the value being C, which is the continuation of N. We retrieve them for each chunk. Equation 1 shows retrieval of 10 neighbors for each chunk in a sequence consisting of 16 chunks from the retrieval database (DB). It is worth noting that Retro does not exhibit any meaningful results concerning improved performance below a retrieval database size of 100 billion tokens. For their two smallest models, adding a retrieval database of two billion tokens improves performance slightly (0.1 bits-per-byte on C4). However, then even increasing the database ten-fold does not change performance further. The only meaningful jump in performance for the smallest models happens once the retrieval database increases from 360B tokens to 900B tokens. This is expensive in terms of space and compute power, thus unrealistic for most setups. Instead, in Retro-li, we show how performance can be improved using retrieval databases at orders of magnitude smaller scale (570K up to 2.89B database tokens) through architectural enhancements and training strategies. Retro™s retrieval database is built using ScaNN [10], a locality sensitivity hashing index, while for Retro-li we use Faiss [19], an inverted vector files (IVF) index. Retro embed their chunks using Bert embeddings [5], while for Retro-li we choose SBert [41], specifically trained for semantic similarity search. For the tokenization, Retro uses SentencePiece [24], a byte-level encoding model, and a vocabulary size of 128™000. Retro-li instead uses the GPT-2 tokenizer, thus a vocabulary size 50™257. In Retro-li, we set out to work with small retrieval databases and almost no token overlap (see Appendix A.2). Thus, we have to be judicious about which neighbors are returned. In this setup with smaller retrieval databases, the search space of sequences is not as densely populated, thus not finding the true closest sequence carries a bigger penalty. Considering both ease of use and theoretical results, we choose to use SBert [41], a Bert-based sentence-similarity model, in Retro-li. We use the pre-trained model multi-qa-mpnet-base-dot-v1 of SBert due to its superior performance in semantic textual similarity tasks in the massive text embedding benchmark (MTEB [32]). The embedding model outputs 768-dimensional vectors and has a model size of 420 MB. This vector dimensionality and model size are compatible with the embedding size and the model size of the GPT-2 backbone used in Retro-li. SBert adds a pooling operation to the output of Bert, resulting in a fixed-size sentence embedding. For our chosen embedding model, this pooling operation is CLS pooling, which involves using the CLS token embedding to represent the sentence. These embeddings were specifically trained for semantic similarity search. More details on SBert can be found in Appendix B. The role of noise in the retrieval has to the best of our knowledge never been analyzed. Here, a first foray into this topic is made. Adding noise to word embedding as a way of improving the generalization capabilities of language models has been done before [55], but not in combination with retrieval. Taking inspiration from NEFTune [18], we aim to improve generalization through a word-embedding regularizer. We introduce a new regularization method by adding noise to the word embeddings of the non-parametric memory, in contrast to NEFTune which regularizes the input sequence, in Retro-li their retrieved neighbors are regularized (see Figure 2). For a matrix M of dimension n_emb Ã seq_len describing the embedding matrix of a sequence, we compute: The relative standard deviation Î»tsubscript†¡ _ t describes the magnitude of the noise added. During training, we add a vector sampled from this ©(0,ƒ)©0 ( 0 , ƒ ) distribution to our neighbor embeddings. We explore a variety of regularizers, some similar to NEFTune, some more similar to the intrinsic noise in the stochastic IMC hardware (see Section 3.2.3), ensuring that the signal-to-noise ratio from different approaches remains comparable. We base our work on an earlier Retro implementation [50] in which the retrieval database and the similarity search are implemented with Faiss index. The training is accelerated with a single Tesla V100 GPU with a maximum 80 GB memory. The task is language modeling on WikiText-103 and the numbers we report are the perplexities for WikiText-103-Validation. Just as in Retro, we employ a sliding window approach, where we compute the perplexities for an overlapping proportion greater than or equal to 75% of the context, more on this in Appendix D. Our work is also inspired by the Chinchilla law [15] to determine the architecture and training strategy so that it fits a setup of any scale. For this particular task, we train on one GPU for 1™078™012 sample sequences resulting in a total of 1.104Ã1091.104superscript1091.104 10^{9}1.104 Ã 10 ^ 9 tokens. The time to train one epoch grows with the number of neighbors, but sub-linearly. Training on 10 neighbors takes about twice as long as training on 2 neighbors but training on 2 neighbors takes about the same time as training on no neighbors. We conduct an initial exploration of the experiment space with only a frozen GPT-2 component. So we train not only the CCA but also the feed-forward (FFW) blocks (see Appendix E for the corresponding architecture diagram). A pattern emerges, where SBert does better than Bert and 3 neighbors do best while not taking much more time to train on, compared to no neighbors. For our retrieval experiments we only freeze the GPT-2 layers as shown in Figure 2. We explored un-freezing these layers as well, see Appendix F. Both Retro-li-off and Retro-li-on are trained on the same data and, aside from the GPT-2 attention blocks and embeddings, from scratch. For the first batch of experiments, we gradually increase the number of neighbors from 2 up to 10 and run the experiments for three random seeds. In Table 1 we report their average. Bert embeddings with 5 neighbors already outperforms Retro-li-off. Moreover, changing the embedding model to SBert improves our performance significantly. This shows that not only does SBert find more semantically similar neighbors, but our model also correctly attends to them. Furthermore, we see that for Bert embeddings all cases except the number of neighbors equal to 5 under-perform Retro-li-off. At this stage of the model training, where we only freeze GPT-2 blocks, it is too difficult for our model to find its way through the loss landscape to get to the best possible performance. Adding sub-optimal neighbors at this point exacerbates the issue. This makes the SBert results even more remarkable. The neighbors found through the SBert embeddings genuinely inform the model, helping it to converge. Still, not all numbers of neighbors improve the performance compared to no retrieval, confirming what [1] and [42] have already observed. Finally, we note that with this setup, for the Bert embeddings, and Retro-li-off the variance between the random seeds is very large. This is partially because we were unable to train all checkpoints to convergence, as some got stuck in local minima. Thus another aspect we show here is how SBert embeddings help to avoid/escape these local minima. Overall, adding neighbors at a stage where the language model itself has not converged yet can help, but does not always. We conclude that we must first address the language modeling before we can add neighbors and benefit from them. Similarly to NEFTune, we add uniform noise to the word embeddings during training. The goal of it is to improve the generalization capabilities of our model. We add it to the training sequence itself, to the neighbors, and also both. We again average the result over three random seeds and ablate the noise type as well as the noise strength. Preliminary ablations performed to determine the best type of noise, both in terms of magnitude and in terms of where the noise is added (so whether to add on the input embeddings or to the neighbor embeddings), showed that regularization works best when moderately added only to the neighbor embeddings. The full results are in Appendix C. We also performed additional experiments to determine the best type of noise among the uniform and the Gaussian while ensuring a similar signal-to-noise ratio. Results are shown in Table 2. A Gaussian with zero mean and Î»t=0.2subscript†¡0.2 _ t = 0.2 (the variance as described by Equation 2) improves our generalization performance better than the rest of the regularizers we have tried. Table 3 shows the impact of this best-performing regularizer (the Gaussian regularizer with Î»t=0.2subscript†¡0.2 _ t = 0.2) in both language modeling and domain shift. In the WikiText-103 language modeling, the regularizer improves or at least does not impair the validation perplexity in various inference settings: having no neighbors, ideal retrieval, or different amounts of noise during inference (i.e., Î»i{0.2,0.4,1}subscript†0.20.41 _ i { 0.2 , 0.4 , 1 }). For the domain shift experiments, it exhibits more promising behavior and consistently shows benefits when different noisy scenarios are considered. More details are provided in the next subsection. WikiText BBC-News Reuters We use three architectural settings for domain shift generalization: Off (i.e., no retrieval). This model was never trained with retrieval. Retro-li-on with no neighbors: This model was trained with retrieval, but we do not give it any neighbors at inference time. Retro-li-on with ideal (i.e., not noisy) retrieval: This model was trained with retrieval and we give it the ten nearest neighbors per chunk at inference time. The setting Retro-li-on with no neighbors is meant to ascertain how much of the domain shift performance gain is simply due to improved language modeling capabilities, and not due to the retrieval itself. In settings with Retro-li-on, we further have a model trained with a regularizer or without one. In Table 3, we only include the best-performing regularizer, a Gaussian regularizer with Î»t=0.2subscript†¡0.2 _ t = 0.2 (see Equation 2), which also reflects the signal-to-noise ratio observed in the modern IMC hardware (more on this in Section 3.3.1). For domain shift experiments with Retro-li-on settings, we take the model and plug in a new retrieval database. In this case, creating a new retrieval database consists of the following steps. First, we train the models on a base dataset A. For a new domain dataset B, C, D, or E (i.e., a different domain) we create a retrieval database with the training tokens of B, C, D, or E, respectively, then chunk the validation sequences of B, C, D, or E, respectively, and find the closest neighbors in the retrieval database for each validation chunk. Then, we plug this database, which includes chunk-to-neighbor mapping, into our model, feed it with the validation sequences as input, and measure the new perplexity. We do not fine-tune the models on any of the target domains whatsoever. Table 4 lists the datasets used for domain shift experiments, which are ordered by the size of the retrieval databases. The datasets are chosen such that they are sourced from a variety of domains and have sufficiently distinctive characteristics to identify them as unique domains. The datasets are further explained with their differences highlighted in Appendix A. In the context of this work, a domain refers to the formality of the language. At one end, we have Wikipedia, which is highly formal text, and on the other end, we have SlimPajama, which consists mostly of text gathered through web crawling and thus contains predominantly informal, poorly or unstructured text, in a variety of languages and even quite a bit of code (see Appendix A.3 for more details). Unlike for the previous experiments, here we take our best baseline transformer (Retro-li-off), re-set the optimizer and only train the CCA blocks on top of it using dataset A (see above). For the neighbors, we choose three neighbors and SBert embeddings. This way, we can more closely emulate the experiments done by Retro and observe what the true performance gain of retrieval is. This has the benefit of not only decreasing the variance across seeds but additionally enabling us to keep the Retro-li-off performance exactly intact. We take the best Retro-li-off checkpoint and keep training it for six random seeds, with and without retrieval, and with and without regularization. WikiText-103 is taken as the base dataset (dataset A, see above) for the taining. We train both Retro-li-on and Retro-li-off, in order to confirm that the improved performance is not due to the increased number of training tokens. As is evident in the rows for Retro-li-on with ideal retrieval in Table 3, when compared to Table 1 with training from scratch, this setup decreases WikiText-103-Validation set perplexity for both, Retro-li-on and Retro-li-off. The seemingly significant perplexity improvement of Retro-li-off is due to there no longer being one particularly bad random seed, as this setup reduces the variance across seeds. Retro-li-off has 109M trainable parameters, and 234M parameters in total, while Retro-li-on has 24M additional parameters due to the CCA blocks. Moreover, since Retro-li-on freezes all parameters except for the CCA blocks, only those 24M are updated, as opposed to 109M parameters for Retro-li-off. As shown in Table 3, even without a regularizer, we see across seeds and datasets that Retro-li-on deals with the domain shift better than Retro-li-off. Adding a regularizer to the retrieval improves this performance even further. In this table, the datasets are ordered by the size of the retrieval databases. Although retrieval and regularization help, the perplexity also keeps going up, despite our retrieval database size increasing as well. However, considering the improved percentage, where Retro-li-off is our baseline, we can see that our Retro-li-off model struggles with a large domain shift, where retrieval and regularization help the most. This performance improvement is not solely due to the fact that Retro-li-on is already better at language modeling. Consider the \"No neighbors\" rows, where we replace the nearest neighbors of a sequence with the sequence itself and mask out the continuation. Hence, the model cannot benefit from retrieval in this setup (i.e., no extra information is retrieved) but includes the additional 24M parameters due to the CCA blocks. We observe that Retro-li-on™s performance without neighbors drops significantly and the performance is similar to Retro-li-off. This indicates that neither the additional parameters nor the better language modeling capability, but the actual retrieval improves our model™s generalization ability. For Retro-li-on with the Gaussian regularization we observe that the model outperforms Retro-li-on without a regularizer on all domain shift datasets. This suggests that the retrieval model can be trained with the regularizer to emulate a denser search space. Moreover, in the setup where no neighbors are given, the regularizer also improves the performance of the language model without any access to retrieval. Before delving into the potential benefits and challenges of retrieval with the IMC hardware, let us explain how it has been done in the state-of-the-art. Currently, we search for the nearest neighbors using Faiss which is an inverted vector file (IVF) index. An IVF index clusters the vectors to be searched into ccc centroids, where each vector is assigned to one centroid. Upon receiving a query, Faiss searches nprobenproben p r o b e of those ccc cells to find the nearest neighbors. This reduces the search time by reducing the search space to the number of centroids to be probed while preserving good performance. However, an IVF index requires training to identify optimal centroids. In Table 5, we measure the time it takes for the search function call to IndexIVF_search to complete. This function is called on 16 chunks concurrently on a Tesla V100 GPU. More on these measurements including the search time distributions can be found in Appendix G. Next, as a natural extension of memory-augmented neural networks [21], we assess whether the non-parametric memory of Retro-li can be moved to a specialized hardware that operates based on the IMC principles. This would make vector searches and thus retrieval much faster, as it foregoes the memory wall issues prevailing in GPUs caused by the high bandwidth data transfers. If the retrieval database is very large (such as in Retro™s case where we have trillions of tokens and 28 billion database keys) the data transfer becomes the bottleneck, especially during inference. We estimate that on a larger scale IMC hardware platform inspired by the early-stage prototype [22], the similarity search time could be brought down to several hundred nano-seconds. One drawback of the IMC hardware however is low-precision and noisy similarity searches due to analog computations with non-idealities. We simulate the noise on such a hardware platform by a Gaussian distribution with zero mean and a certain standard deviation ƒ described by Equation 2. This additive noise at inference time is denoted by Î»isubscript† _ i as opposed to Î»tsubscript†¡ _ t which is used to describe the noise during training. For a more in-depth explanation of noise modeling on IMC-based hardware, please refer to [39]. For instance, for a recent large-scale chip based on phase-change memory devices [22], this relative standard deviation is 0.2. Adding Gaussian noise with a variety of relative standard deviations to our neighbor word embeddings at inference time gives us stable results, despite it not being seen during training (i.e., trained with Î»t=0.2subscript†¡0.2 _ t = 0.2 while tested with Î»i{0,0.2,0.4,1.0}subscript†00.20.41.0 _ i { 0 , 0.2 , 0.4 , 1.0 } ). This is evident in Table 3, in the rows describing the setting noisy retrieval with no regularize (i.e., None). Even for a relative standard deviation as large as 1.0, the performance never drops more than 1% compared to the ideal retrieval without any noisy neighbors. This suggests that the searches on the non-parametric memory can be moved to the IMC hardware without issue. Training with the regularizer does not decrease the performance drop upon adding noisy retrieval at inference time in all cases. For Î»i=0.2subscript†0.2 _ i = 0.2 and Î»i=0.4subscript†0.4 _ i = 0.4, even when we did not train using a regularizer, our performance dropped about the same relative to ideal retrieval as if we had trained with a regularizer. On the other hand, for Î»i=1.0subscript†1.0 _ i = 1.0, a large relative standard deviation, this regularized training does make a difference. Where other models fail, the one trained with a Gaussian regularizer is barely affected. This can be seen more clearly in Table 2 where we also add the standard deviation error bar for the six random seeds. Finally, we perform an ablation study with a more detailed and accurate noise inference model given in Analog AI hardware kit [39]. This model splits the database to store its content on realistic IMC crossbar sizes. During the similarity search, it captures the non-idealities associated with the crossbars both short term such as read noise, and long term such as drift. For the memory device level noise modeling Phase-change Memory (PCM) preset [35] is chosen with a maximum conductance of 25 uS and the rest of the parameters as default. The results of this study are presented in Table 6. We use the best-trained models on the WikiText and BBC-News datasets and compare the inference results where in one case there is no noise on retrieved neighbors and in the other case the retrieval neighbors™ noise is modeled comprehensively using the Analog AI hardware kit. We notice that the noisy retrieval across both datasets drops only by a negligible margin (<0.003%) indicating the robustness of the Retro-li models trained with regularization towards noisy retrieval. In order to better understand the quantitative results, we present here some generated samples from our worst-performing domain, SlimPajama. We evaluate the semantic similarity of the model-generated chunk to the real continuation chunk. For Retro-li-on and Retro-li-off we pick their best-performing validation samples on SlimPajama-Validation, feed 75% of the context window into the model and generate the next chunk. In order to evaluate the similarity of the generated chunk to the real continuation, we embed it using SBert and report the similarity measure. We employed several generation modes of which we only present the best-performing ones from retrieval on and off in Tables 7 and 8 respectively, full results for this experiment can be found in Appendix H. The dot product results are in brackets for each chunk. We see that the Retro-li-on generated sample is not only more coherent but more similar semantically to the real continuation chunk than the Retro-li-off generated sample is to its own real continuation chunk. In addition to the plug-and-play style domain shift generalization experiments, we conduct experiments in which both Retro-li-on (with regularization of Gaussian with Î»t=0.2subscript†¡0.2 _ t = 0.2) and Retro-li-off models undergo a minimal amount of fine-tuning steps with partial parameter updates while the bulk of their parameters remain frozen. The main motivation for this exercise is to assess how far the perplexity performance can go down with a minimalistic fine-tuning effort (in terms of both the number of updated parameters and steps of updates) compared to the plug-and-play domain shift experiments which involved no fine-tuning whatsoever. From a selected dataset (e.g. B, C, D), we first randomly sample 15% of the training set data and reserve it as the input sequences to be used during the fine-tuning phase. The remaining 85% of the training set data are directed toward building the retrieval database to be used during the fine-tuning phase. Note that the training input sequences and the retrieval database are made mutually exclusive in order to avoid undesired leakage effects during retrieval. As a starting point, we take the best checkpoints created by training both Retro-li-off and Retro-li-on models on the base WikiText-103 dataset as mentioned in Section 3.3. For Retro-li-on models we observe updating CCA layer parameters leads to slightly worse performance. Hence we update only the parameters in the feed-forward layers and the read-out layers for both model types during fine-tuning for a few epochs. After fine-tuning for a few epochs, the updated model checkpoints are used to run inference on the validation sets from the corresponding datasets. The validation training sequences and the validation retrieval database are prepared exactly the same way as in plug-and-play experiments explained in Section 3.3 such that the performance can be compared under equal settings. The results of fine-tuning experiments are presented in Table 9. We evaluate on a subset of datasets used for the plug-and-play domain shift generalization experiments, namely, BBC-News, Reuters, CNN-DailyMail. At epoch=0, i.e., before any fine-tuning, the perplexities reported on the validation show that Retro-li-on consistently dominates Retro-li-off. This is in line with the plug-and-play results in Table 3; the slight deviations are due to the number of tokens considered in each sequence for the evaluation. We observe that for all datasets, the best perplexities are achieved within the first five epochs. This indicates that only a minimal amount of updates are sufficient to fine-tune both Retro-li-on and Retro-li-off models. Retro-li-on variant tends to take one or two additional epochs to reach the best perplexity and remains slightly higher compared to the best perplexity of Retro-li-off. However Retro-li-off shows signs of strong overfitting with training on more and more epochs. In summary, fine-tuning leads to competitive perplexities in both Retro-li-on and Retro-li-off models. In RAG, there are various approaches to make use of retrieved neighbors. For instance, one approach [26] adds the neighbors to enhance the context of an input sequence, whereas Retro [2] attends to the neighbors using the CCA mechanism. It is also possible to use both approaches by adding the nearest neighbor to the context window as well as attending to the other neighbors through CCA. Retro outperforms the approaches without CCA slightly on Natural Questions tasks, but [52] outperforms Retro by a wide margin by combining the two approaches. Scalability is central for Retro in particular, as it significantly improved results in language modeling once the retrieval database reached one trillion tokens. For the models closer to our model size adding retrieval barely made a difference. The figure on the second page of the Retro paper reveals that although bits-per-byte drops upon adding retrieval, even making the retrieval database almost 10Ã larger (up to 10 billion tokens) did not affect the performance. All of this culminates in a state-of-the-art test perplexity of 2.4 on WikiText-103-Validation. It has to be said that, as the Retro™s authors noted themselves, it is evident that this perplexity is mainly due to validation set leakage. This is practically unavoidable with a retrieval database of this size no matter the validation set, as there is only so much data that can be web-scraped and is public domain. Furthermore, a retrieval database consisting of trillions of tokens is unrealistic for most setups. Although it is a one-time cost and adding new entries is straightforward, it is difficult to find open-source datasets of this size, let alone have the resources to clean up and process them appropriately. Recent works [17, 27, 31] apply principles of RAG for few-shot learning tasks where retrieval helps with the meta-learning process. However, these models are typically larger compared to Retro-li, operate on different tasks/datasets, and crucially fail to establish a retrieval-off baseline for their respective models. This makes the quantitative comparison of Retro-li against these models somewhat unrealistic. Harnessing the inherent capabilities of language models for specific tasks can be done in many ways. One is through classical fine-tuning, where we add a task-specific head to a pre-trained model to obtain a task-specific model. We can also use pre-trained models via few-shot prompting for tasks such as generative question answering. Finally, in [18] they settled on instruction tuning, where existing samples were augmented with instructions in natural language. The authors observed that adding uniform noise, reminiscent of noise added in adversarial literature, to the word embeddings of the instructions improved the performance of such models significantly through a regularization effect. Given that one objective of these RAGs is to improve generalization through the non-parametric memory, we investigated adding such a regularizer to our non-parametric memory. With Retro-li, we have shown that by proper architectural enhancements and training strategies, there is a role for retrieval in the medium-size parametric models using small-sized retrieval databases that are orders of magnitude smaller (570K up to 2.89B database tokens) compared to Retro. Retro-li consistently improves language modeling and cross-domain generalization compared to the same architecture without retrieval. By applying regularization to the non-parametric memory, we improved the generalization ability of Retro-li even further. Additionally, we have shown that the non-parametric memory can be made robust against noisy similarity searches, which makes it amenable for deployment on the efficient IMC hardware without performance loss. When needed, Retro-li can similarly benefit from fine-tuning. Future work would evaluate task-specific performance, especially in the domain of question answering. Retro has been shown to lower hallucinations and be less toxic when compared to its non-retrieval counterpart by retrieving from trillions of tokens, but not yet at a small scale. Moreover, there are embedding models that are better suited for semantic similarity search than SBert, as measured by the MTEB benchmark. Changing the embedding model is straightforward, although the retrieval database must be re-computed. In Retro-li, we attend to all retrieved neighbors indiscriminately, whereas future work could add a similar mechanism as Self-RAG [1] to decide when and if to retrieve neighbors, or select an optimal subset of them [53]. It might also explore Retro-fitting better foundational models, or different attention-based architectures altogether. Future work would also include more accurate hardware-aware training, obtained by applying training techniques similar to the ones presented in [40]. These techniques so far are applied to parametric weights. It will be interesting to see how these training dynamics play out in the non-parametric memories similar to the ones used in Retro-li. This work is supported by the Swiss National Science foundation (SNF), grant 200800. We describe the dataset used in the language modeling experiments and the datasets used for the domain shift experiments. The goal was to use datasets with strong baselines to compare our model to, and which would need minimal pre-processing for our purposes. The WikiText dataset [30] was created to address the issue of there not being a text dataset with long-form content and original punctuation, capitalization, and numbers. As opposed to web-scraped content, the text is more structured and has fewer typos or colloquialisms. This is because the authors employed a measure of quality control by only considering Wikipedia articles, as well as restricting themselves to verified good or featured articles. A good article has been nominated by a reviewer as well as confirmed by an impartial editor to be \"well-written, contain factually accurate and verifiable information, are broad in coverage, neutral in point of view, stable, and illustrated, where possible, by relevant images with suitable copyright licenses.\"111https://en.wikipedia.org/wiki/Wikipedia:Good_articles. Only about 0.5% of Wikipedia articles make the cut. On the other hand, a featured article refers to one of the best articles on Wikipedia as decided upon by the editors. The criteria for a featured article are stricter than for a good article, and more comprehensive222https://en.wikipedia.org/wiki/ Wikipedia:Featured_article_criteria.. Such articles make up around 0.09% of Wikipedia. More details on the processing steps can be found in Section 4.3 of their paper. We use WikiText-103 which consists of around 103 million training and 254 thousand validation tokens. We use the same database for training and validation and generate it using WikiText-103-Train. Validation and training sequences of the WikiText dataset are disjoint by design. To avoid leakage during training we ensure that the neighbors of the training sequences are never direct continuations. Finally, we report the 1-gram Jaccard-Similarity [33] of the training sequences and their two nearest neighbors in Figure 4 and the ten nearest neighbors in Figure 5 as a way to determine leakage. As is evident in the histograms, for most neighbors we do not get a Jaccard-Similarity of more than 0.2, so limited leakage is assured. In the Retro paper they checked if the training sequences have eight or more contiguous tokens in common with one of their neighbors to determine the degree of leakage. In our case, for the training and validation set, 4.76% and 5.1% of the ten nearest neighbors respectively have at least eight contiguous tokens in common with the sequence used to query the index. Evaluating such samples qualitatively, in Figure 3 we can show that this is hardly leakage in a sense that our model could exploit, as the neighbors come from different articles. For our domain shift experiments, we choose datasets of varying textual structures and sources. If a train/test/validation split is given, we take the training data to create the retrieval database and validation data to evaluate on. If only a train/test split is given, we use the test data to evaluate on. Since for the inference experiments we created the retrieval database fully out of the training data and only predicted on the validation data, no leakage is possible. Retro was trained on MassiveText [37] which is unfortunately proprietary. Furthermore, The Pile [6] which is an open-source alternative to MassiveText and on which Retro was also trained for comparison purposes, was taken down in July of 2023 due to a DMCA (Digital Millenium Copyright Act) notice333https://academictorrents.com/details/ 0d366035664fdf51cfbe9f733953ba325776e667.. The Nvidia Retro implementation training data was mostly based on The Pile [45] as well. This means that we could not use the same dataset as Retro, which would make it harder for us to compare our performance to theirs. As an alternative, looking for datasets similar in data sources and size to The Pile, we decided on SlimPajama [46] which is a cleaned and de-duplicated version of RedPajama [4]. De-duplication is especially important for textual training data, see [25]. In Table 10 we compare The Pile to SlimPajama, where Pile-CC is comparable to CommonCrawl and C4 [38]. We see that although the data sources themselves are similar, the proportions are only somewhat comparable. As most text in the SlimPajama dataset comes from CommonCrawl, it is largely unstructured and conversational. Although for humans it is easy to read and understand, this type of content is significantly dissimilar to Wikipedia articles, in flow, grammar, as well as vocabulary. We use a sub-sampled version of Slimpajama-627B, namely Slimpajama-6B444https://huggingface.co/datasets/DKYoon/SlimPajama-6B.. Furthermore, Slimpajama-6B still needed some clean-up, as there were many instances of repeating characters used as filler or for code comments in the CommonCrawl portion of the dataset. This leads to a total of 5.5B tokens usable for training. Finally, to reduce memory usage and simplify the code structure, we truncate all samples longer than 1024 tokens. Considering all of this as well as using their train/validation split, we end up with 2™886™850™140 training tokens and 4™948™252 validation tokens. The BBC-News dataset [9] consists of 2™225 documents from the British Broadcast Corporation news website, across topics such as business, entertainment, politics, sports, and tech. News articles are highly structured in terms of flow, grammar, and vocabulary, so this dataset is similar to WikiText in that regard. Using their train/test split and our tokenization scheme, we end up with 589™677 training tokens and 468™831 validation tokens. The validation set size is unusually large with respect to the training set size. Since we used the training set as our retrieval database, this might have hindered performance somewhat. The Reuters-21578 dataset as created and used by Hayes in [11], consists of 19™043 documents and has 674 categories in total. This dataset is comprised of news stories from the Reuters financial news-wire service in 1987 and just like the BBC-News dataset, is very similar to WikiText in terms of structure and language. However, the differences in vocabulary might be larger here, as the articles are from decades ago. Using Hayes™ train/test split and our tokenization scheme, we end up with 3™454™605 training tokens and 174™335 validation tokens. The Pile-of-Law dataset [12] was created in order to mitigate the effects of potentially harmful and biased training data such as can be found through web-crawling alone. The 34 data sources range from \"legal case opinions and filings\", mostly from the U.S.A., to study materials for law exams and were extensively analyzed for toxicity and biases. We use two of their data sources. The Atticus contracts subset contains contracts from the Atticus project, specifically the commercial contracts dataset [13]. It consists of 510 contracts and was created to train a model to highlight portions of the contract a human should review. Contract language is highly structured and repetitive, but not necessarily similar to Wikipedia or news articles. We now move away from article style text into something new entirely. Using their train/test split and our tokenization scheme, we end up with 456™681™888 training tokens and 152™337™169 validation tokens, which for performance reasons we had to sub-sample into 7™605™872 validation tokens. This subset consists of letters by U.S.A., founders, which were scraped from Founders Online [49] and consists of 137™883 training and 45™781 validation samples. Although this is in the Pile-of-Law dataset as well, it is not at all similar in structure to the Atticus contracts. Since these are letters, they are structurally more similar to articles than contracts, but as they are letters from the 1800s, the vocabulary and sentence structure are quite dissimilar to contemporary Wikipedia articles. Using their train/test split and our tokenization scheme, we end up with 55™818™926 training tokens and 18™605™985 validation tokens. The original WebText dataset used to train GPT-2 has not been released. However, as the authors have published how to re-create it, it has been repeatedly reconstructed. OpenWebText is created by taking the text of articles linked on Reddit555www.reddit.com. which have at least 3 upvotes. We use [8], this version has over 8 million documents and no inherent train/test split. To keep the sizes manageable, we sub-sample 55% of the dataset to get 2™491™806™520 tokens which we split by taking 1% of it for validation. This is large enough to give us a good picture but not so large as to overwhelm the retrieval database size as in BBC-News. Overall we end up with 2™477™892™482 training tokens and 13™914™038 validation tokens. This is our second-largest retrieval database, right after SlimPajama-6B. The CNN-DailyMail dataset originally created in [14] for question answering, has been processed for summarization in [34], which is the version we use. Since these are again news articles, the text formality and structure are comparable to Reuters and BBC-News. Using their train/test split and our tokenization scheme, we end up with 223™216™223 training tokens and 10™116™369 validation tokens. WikiText BBC-News Reuters This is an extension of Section 4. In the main body of the work, only the three most important papers are chosen in order to adhere to the page limit. The papers presented here and their concepts were also crucial for this work and for possible future avenues to explore. In natural language processing the branch of retrieval augmented generation has been gaining traction for a few years now. Given the tasks language models are expected to fulfill, from simple question answering, to fact-checking, to multi-turn conversations, combining a model™s parametric memory with a non-parametric database is the natural next step. It builds upon an earlier idea of knowledge graphs [48] and enables not only a better understanding of our models™ reasoning but also makes it easier to update knowledge. This is difficult at best and virtually impossible at worst with only parametric knowledge. In the RAG paper [26] they did exactly that, where retrieval from the non-parametric memory is jointly learned with sequence generation. The neighbors are split into 100-word Wikipedia passages, as opposed to chunks. Just like Retro, RAG computes cross-attention over the neighbors of tokens/sequences. Moreover, RAG™s non-parametric memory only consists of a single Wikipedia dump (December 2018), as opposed to Retro which has many data sources. RAG set a new state-of-the-art on many open-domain QA tasks and even where they did not, they got close to the performance of more complex systems. Although since then there have been many systems that have outperformed RAG on open-domain QA, such as EDMR2superscriptEDMR2 ^ 2 [44] and FiD [16] which were specifically trained for QA. It must be said that Retro does not outperform these models either. Though interestingly, Nvidia™s Retro++, where they add the top-1 neighbor to the context, does outperform them. SentenceTransformers [41] is a Python framework for embeddings of images and text, specifically sentences. While Bert is a general encoder model, only trained for language modeling, SentenceTransformers (also called SBert) is designed to generate embeddings for entire sentences. The purpose is to encode sentences and improve semantic similarity search. It works by adding a simple pooling layer after the Bert embeddings. In this pooling layer word embeddings can be averaged, we can take the max embedding or we can take the embedding of the CLS token, which is used for classification. In this paper, the authors use Siamese and triplet networks to train Bert. These neural network architectures are designed to learn embeddings for pairs or triplets of data points in a way that emphasizes their similarities and/or differences. Siamese networks have two subnetworks with shared weights. Each subnetwork takes as input a data point (in our case a sentence) and produces embeddings for those inputs. The objective is then to minimize the distance between similar pairs of inputs and maximize the distance between dissimilar pairs. Triplet networks work analogously, but with an anchor (A), a positive example (P), and a negative example (N). The objective is then to minimize the distance between the anchor and the positive example and maximize the distance between the anchor and the negative example. In the end, we obtain sentence embeddings that carry semantic meaning and can be compared using cosine-similarity. The pre-trained model we use is called multi-qa-mpnet-base-dot-v1 and is the best SentenceTranformers model for semantic similarity search. It was trained on question-answer pairs, which is the main use case for retrieval-augmented language models. Retrieval may not always be helpful and can in some cases be detrimental to the model™s performance. In [1] they explored this question, where they retrieved passages on-demand, as opposed to indiscriminately retrieving a fixed number of neighbors like most retrieval systems do, such as our own. This decision is made using reflection tokens, which allow the language model to critique its own output and decide if it needs retrieval to improve the factuality and overall quality of the generated text. They evaluated their system on fact verification, multiple-choice reasoning, and a variety of question-answering datasets and setups. For these answers, they not only evaluated the exact match but considered fluency and precision/recall as well. They were able to show that their system outperformed other retrieval models on a majority of their tasks and datasets, achieving significant improvements in some cases. This is an entirely new approach compared to how most RAG systems currently work. Through their evaluation on a diverse set of tasks, they have shown the need for and role of such reflection. We have observed similar issues in our experiments, so the next step would be to add such a self-reflection mechanism as well. In [15] they argue that most large language models are under-trained. The authors analyze the relationship between the number of parameters and training tokens given a fixed compute budget. Given two out of the compute budget, the number of model parameters, and the number of training tokens, they explain and validate through their experiments how to compute the third. Due to our resource restrictions, we implemented the smallest Retro model with 175M parameters. Considering Figure 2 in the Chinchilla paper, we can fix the number of parameters and estimate the number of FLOPs as C=N‹…D¶‹…·C=N DC = N ‹… D where D·DD is the number of training tokens and NNN is the number of trainable parameters according to [20]. For Retro-li with WikiText-103 as both retrieval database and training data, N=120M120N=120MN = 120 M. We trained on 1™078™012 samples in total, for each sample we have 16 chunks, and for each chunk, we get 2 up to 10 neighbors, where each neighbor is 128 tokens long. This is 5.52Ã1095.52superscript1095.52 10^{9}5.52 Ã 10 ^ 9 to 2.32Ã10102.32superscript10102.32 10^{10}2.32 Ã 10 ^ 10 tokens in total, so C is 3.97Ã10183.97superscript10183.97 10^{18}3.97 Ã 10 ^ 18 to 1.7Ã10181.7superscript10181.7 10^{18}1.7 Ã 10 ^ 18 . Even for only two neighbors, this puts us in the optimal model size range. The non-parametric retrieval database helps with the Chinchilla law, as the retrieved neighbors increase the number of training tokens, alleviating the problem of under-trained large language models. We present some details on the NEFTune noise regularization and as it pertains to the approximation to the hardware platform. The regularizer is added to the neighbor embeddings for the CCA blocks during training. For validation and inference, no noise is added to the neighbors, unless specified. For this set of experiments, we chose three neighbors and the SBert word embedding model, which gave us the best results so far. Looking at the results for WikiText-103-Validation, we cannot see an improvement of perplexity in any combination of noise and noise placement when averaged over the random seeds, see Tables 16 to 16. It is not entirely surprising that the generalization capability of the train to test set did not improve, as adding noise at train time can lead to the model effectively memorizing the noise [54]. The best combination for these experiments is setting Î±=10¼10 = 10 and adding noise to the neighbors only, see Table 13. Adding noise to both, sequences and neighbors, see Table 16 gives worse results than adding noise only to the sequences. This suggests that the neighbors stabilize the negative effects of the noisy sequence. Furthermore, considering our best result comes from adding noise to the neighbors only, we can see that this NEFTune noise has a regularizing effect on the neighbors. This is likely due to the fact that our retrieval database is too small for our training set. In the original Retro the retrieval database consisted of trillions of tokens and only saw improvement once the size of the retrieval database was in the order of billions, see the Figure on page 1 of the Retro paper as well as the table on page 34. As we show in Table 17, the checkpoints we trained with uniform noise and Î±=10¼10 = 10 do not handle approximation to the hardware platform at inference time any better than the checkpoints trained without noise. Thus, uniform regularization does not play a role in this case. WikiText BBC-News Reuters It is clear in Table 17 and Figures 6 that not only is Gaussian, Î»t=0.2subscript†¡0.2 _ t = 0.2 the best checkpoint overall it can handle large amounts of noise better than other regularizers or no regularizer. We see how for Î»i=0.2subscript†0.2 _ i = 0.2 and Î»i=0.4subscript†0.4 _ i = 0.4 the perplexity barely increases for any of the regularizers (None, Uniform, Gaussian). For Î»i=1.0subscript†1.0 _ i = 1.0 which is the largest relative standard deviation in our setup this changes. Here clear patterns emerge. For instance, Gaussian, Î»t=0.4subscript†¡0.4 _ t = 0.4 does consistently worst because it has the lowest signal-to-noise ratio of the regularizers shown here. It is most affected by Î»i=1.0subscript†1.0 _ i = 1.0. Moreover, Gaussian, Î»t=0.2subscript†¡0.2 _ t = 0.2 does best for every dataset and noise level at inference time. It is least affected by Î»i=1.0subscript†1.0 _ i = 1.0. This is unsurprising, as we also observe that out of all the regularizers Gaussian, Î»t=0.2subscript†¡0.2 _ t = 0.2 is least affected by the inference mode no retrieval. Most of this model™s generalization performance comes from improved language modeling and not from the retrieved neighbors themselves. The uniform regularizer and no regularizer get similar results for almost all combinations of datasets and types of inference noise, but that changes for Î»i=1.0subscript†1.0 _ i = 1.0. For all datasets except BBC-News and WikiText-103, no regularizer is less affected by Î»i=1.0subscript†1.0 _ i = 1.0 than the uniform regularizer. However, it has to be said that, even for these two datasets, the performance decrease is very similar. Perplexity is a performance measure in natural language processing (NLP) used to evaluate how well a model predicts a sample. It quantifies how a probability distribution or language model represents the data it is trained on. A lower perplexity score indicates that the model is more accurate and has a better understanding of the data. The model is essentially less surprised by the next word it sees, hence the name. In language modeling, perplexity is the exponential of the cross-entropy loss. In NLP we usually use the exponent base as opposed to base 2 but both are acceptable. The state-of-the-art language modeling perplexity of 2.4 on WikiText-103-Test is achieved by the Retro model666https://paperswithcode.com/sota/language-modelling-on-wikitext-103., with a large gap to the second best perplexity of 10.6. As already discussed, advancements of perplexity on WikiText-103 are usually due to dataset leakage. Most if not all large language models train on some snapshot of English Wikipedia. With the emergence of foundational models, it is nearly impossible to avoid validation/test set leakage. Perplexity has other issues as well. As it is a manner of evaluating the probability distribution, it cannot handle words with zero probability, so out of vocabulary words. Language models with a closed vocabulary will have lower perplexity than those with an open vocabulary, which assign probabilities to all words, if need be down to the characters. Finally, it is difficult to compare perplexities between closed vocabulary tokenizers, due to the size of the vocabularies. A larger vocabulary can lead to higher perplexities, as there are more words to consider for the next word. Despite these issues, perplexity is used as a performance measure due to its simplicity and comparability. In NLP we are aware of these issues and attempt to alleviate them by using closed vocabularies of similar sizes, when our goal is to present and compare perplexities. Furthermore, it is difficult to find a quantitative measure for language models. The BLEU or ROUGE score, Word error rate, or Character error rate all depend on generating text. Generation is already the next step which is subject to hyperparameter-tuning and post-processing. Not to mention, that the goal might not be to generate the exact continuation, as depending on the use-case, semantically similar output should be accepted as well. To compare the output of the language model directly, we must use a measure that does not sample but evaluates the probability distributions directly. Recently, bits-per-byte is used in order to measure the performance of a language model. Bits-per-byte refers to data storage efficiency, it describes a compression ratio. The idea behind it is the same as behind perplexity, where bpb=00bpb=0b p b = 0 means that the model knows exactly what symbol will be next whereas bpb=log2(vocab_size)™subscript”2£_ §bpb=log_{2}(vocab p b = l o g _ 2 ( v o c a b _ s i z e ) means the model needs to be given the next symbol exactly. It is related to perplexity in the sense that cross-entropy loss for a character-level language model averaged over a dataset is bits-per-character. It is computed as LÃlog2(e)¿™subscript”2L log_{2}(e)L Ã l o g _ 2 ( e ) where L¿LL is the loss and is a loss-based performance measure. As it also highly depends on the vocabulary size, it is not a more general measure than perplexity. Here we present the full result tables, for each random seed and all number of neighbors. Variations across random seeds are likely due to the fact that we have to fix the number of training steps. We do not stop once we overfit, as we do not reach that stage with most of our experiments. For full transparency, we present both, the perplexity of the sliding window approach (as Retro computes it) and the full results (simply exp(loss)). We see strong variations across random seeds, suggesting that our system is sensitive to the initial samples and gets stuck in local minima quickly. Moreover, as we do not observe this behavior for SBert embeddings, we can conclude that this is due to less-than-ideal neighbors, which strongly impact the system, especially in the beginning. It has to be said that even Retro-Off has one particularly bad seed, which suggests that at least part of the reason we get stuck in local minima is due to the loss landscape itself. Finally, note how not all checkpoints improve upon utilizing the sliding window approach. This suggests that this approach is better suited for demonstrating the performance of a language model. If the performance decreases given 75% of the context, the model is truly unsuited for the task. In this section, we provide additional details on the architecture introduced in Section 2. Specifically, we elaborate on other changes we made to the Retro architecture and how we implemented them. One contribution of the Retro paper is that this type of architecture is straightforward to apply to other language models. This is called Retro-fitting. This is beneficial and efficient, as it allows us to leverage pre-trained models and augment them with retrieval capabilities. To do so, we must be able to access each attention block separately, as the chunked cross-attention blocks are added to every third layer starting from the sixth (or ninth for larger models). These attention blocks can then be frozen. This reduces the number of trainable parameters, making it faster to train. In the paper, instead of taking a pre-trained GPT-2 checkpoint, the authors built a GPT-2-like model with the appropriately changed parameters (see Table 20) . Then they trained it without retrieval before Retro-fitting it. Due to our limited resources, we use the publicly available GPT-2 checkpoints. Consequently, we had to change the aforementioned hyperparameters. Moreover, we had to train additional feed-forward network parameters at the end of the attention layers. Using a pre-trained model leads to a significant improvement of perplexity on the language modeling task compared to training Retro from scratch and it decreases our model size by 30%. For later experiments, we use our trained Retro-li-off with GPT-2 attention blocks as a checkpoint. This enables us to freeze all decoder layers except the CCA, just as in the original paper. Additionally, it decreases the number of trainable parameters even further from the complete Retro-li by 90.29%. The architecture is visualized in Diagram 7. Note that Retro differentiates between the baseline transformer, which is their GPT-2-like model, and Retro-off, which is their Retro-on without CCA. However, they repeatedly demonstrated that their performances are almost identical. In our work, we had no baseline transformer. We compare the Retro architecture with and without CCA. Most changes are made in order to adapt the Retro architecture to the GPT-2 attention blocks. Faiss is an open-source library for similarity search created by Facebook Research. Through clustering and quantization, it efficiently searches billions of vectors in milliseconds. Although Retro used ScaNN for their purposes, we chose Faiss due to its ease of use and its parallelization abilities. Faiss has several hyperparameters that must be set following the size of the database. According to their own recommendation, the number of centroids888Also called inverted lists. should be set as about n n where n is the number of index entries, in our case this refers to the number of chunks. For WikiText-103 this ends up being around 1024, for SlimPajama-6B and OpenWebText, this is around 4096. The number of inverted lists to be searched at query time can be set freely. Nvidia™s Megatron Retro implementation set it as 0.1% of the inverted lists, whereas we stick to ncentroidssubscript¡ n _ c e n t r o i d s . Creating the Index The workflow begins with the raw text files chosen for the retrieval database. We tokenize the text using the GPT-2 tokenizer. This first tokenization is only used to / the text into chunks of length 64. To add them to the index, we de-tokenize the chunks, and re-tokenize them using Bert / SBert, so we can embed them using these models. We prefer them over GPT-2 embeddings as they more effectively capture the semantic meaning. These chunk embeddings are then added to the Faiss index as keys, with their continuations (the next 64 tokens) as values. Before we tokenize with GPT-2, we normalize the text using the Bert normalizer. This is essential, as Bert and GPT treat neither special characters nor repeated characters the same way. Without normalization, the re-tokenized chunk might be longer than 512 tokens. The pseudo-code for this algorithm is in Algorithm 1 in detail. IVFPQ 999https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexIVFPQ.html. refers to an inverted file with product quantizer. The vectors are clustered and an inverted list for these clusters is created. Getting the nearest neighbors To make training more efficient, for each training sequence, we get the ten nearest neighbors for each chunk offline. We save them and their associated distance matrices to be loaded during training. In the first step, we again normalize using the Bert normalizer and tokenize using the GPT-2 tokenizer. Then we go through the entire tokenized text in steps of 1024 (our sequence length). Each sequence is split into chunks of length 64, which are used to query the index for their top k neighbors of each chunk. Once found, we save not only the source sequence, the nearest neighbors for all 16 chunks, and their distance matrices but also the target sequence, which is the source sequence shifted by one token. The distance matrices are not strictly necessary for our training setup and can be excluded. In our case, we kept them in order to analyze if adding noise to the distance matrix, thus shuffling the top 10 neighbors, then taking the new top k neighbors would significantly impact the performance. The goal of this experiment was to argue that saving these neighbors on specialized hardware with inherent noise is still feasible. It turned out that our system is even more robust, where we can not only shuffle the neighbors but also add noise to the neighbor embeddings themselves without significant performance degradation. Positional Embeddings The positional encodings were ablated in the paper, and Figure 8 shows minimal relevance to the choice of it. Thus, we instead use the state-of-the-art rotary positional embeddings [47]. They work by using rotational operations to capture positional information. Specifically, they rotate the embeddings based on the position in the sequence. These rotations are learnable and can be reversed. Sequence Length We use GPT-2 attention blocks, meaning the input sequence must be of length 1024. In the original paper, the chunk length was 64 tokens and the sequence length was 2048 tokens, neither of which is justified or ablated. As such, to have the input of the correct shape, we reduce the number of chunks per sequence to 16 as opposed to 32, whereas the chunk length remains unchanged. Optimizer and Batch Size Retro itself used AdamW [29], so Adam [23] with true weight decay (as opposed to Adam with L2 regularization), and a linearly increasing learning rate schedule with a batch size of 256 for their smallest model. Due to our memory restrictions, we can work with a batch size of 2 at most. Thus, using the same hyperparameters as Table 11 in [2] does not yield the same results in terms of convergence. As a consequence, we tried a variety of optimizers and decided on RAdam [28], which has the additional benefit of eliminating the need for us to tune the hyperparameters. RAdam works by regularizing the variance of the gradients, leading to more stable training, especially in the beginning where the variance might be particularly large. Note that what is displayed as \"noam\" in the figure, refers to the optimizer used in the original transformer paper [51], where we have Adam with linear warmup and a learning rate schedule that decays the learning rate proportional to the inverse square root of the step number. Nvidia™s Megatron project already has a working version of Retro [52], but everything from the creation of the search database to the training is based on having much more resources than we do. Moreover, as it is part of a larger project, even following the code flow is difficult, let alone adding changes for this work. Due to these reasons, we chose not to move forward with this implementation of Retro. Although we kept the implementation as a reference to check our architectural decisions against. As a basis for our experiments, we used the smaller and easier-to-work-with \"labml\" [50] implementation of Retro to build our code on. Although only meant as a tutorial to better understand the chunked cross-attention and retrieval mechanism, it has such a clean and simple structure that adding to it for our needs was straightforward. Many changes to this code base were necessary, most importantly to the database, dataset creation, and the training loop, as well as adding Retro-fitted GPT-2. Still, having this framework to start with was hugely beneficial. Most of our ablations concern the retrieval aspect of the system. Here we explore the language model itself. We wanted to see if it would be possible to fine-tune the GPT-2 attention blocks as well, in order to improve performance even further. It did not, which makes the case for using foundational models stronger. We kept the GPT-2 backbone frozen for most experiments but did one ablation where we (1) Unfroze GPT-2 and continued to train on one of the pre-trained checkpoints (denoted from ckp), (2) Trained from scratch by keeping the GPT-2 parameters trainable (denoted from scratch). This increases the number of trainable parameters, which consequently increases the training time. Considering the results in Table 22, we observe that unfreezing GPT-2 and continuing to train from a previous checkpoint does improve our performance slightly for the Retro-li-off case. It is clear, however, that this was due to there no longer being one significant outlier (random seed 42). So it is a stabilizing effect, rather than an overall improved training. Moving on to Retro-li-on (2 neighbors, SBert embeddings) in Table 23, the case is even clearer, namely unfreezing GPT-2 did not improve our language modeling capabilities. This is likely because GPT-2 was trained to a minima of language modeling, so when adding retrieval we only have to train the retrieval parameters. Adding chunked cross-attention changed the loss landscape, which is why unfreezing does not help. On the non-parametric side, we need to store document vectors in a manner that facilitates search and optimizes space usage. Maximum inner product search (MIPS) goes through all the vectors in a database, computes the inner product with the query vector, and returns k vectors with the largest inner product. This is linear in the number of entries of the database, thus entirely impractical for databases consisting of millions or even billions of entries. Performing such a search is recommended only when the quality of the retrieved neighbors is crucial and the search time is less critical. In order to reduce the search time, we must reduce the search scope. This can be done through locality sensitivity hashing (LSH), which is the method employed by ScaNN. LSH works by hashing the vectors into b buckets, aiming to group similar vectors based on the hash function. Therefore, it might perform sub-optimally if the vectors are poorly distributed. Moreover, it is highly sensitive to the number of buckets b and the hash function. We can also reduce the scope by adopting an inverted vector files (IVF) approach, like Faiss does. An IVF index clusters the vectors into c centroids, where each vector is assigned to one centroid. Upon receiving a query, we search nprobenproben p r o b e of those c cells to find the nearest neighbors. This reduces the search time while preserving good performance. However, an IVF index requires that we train it in order to identify optimal centroids. Both index types reduce data transfer by decreasing the search scope. In any case, there is a trade-off between search time and result quality. We have established the importance of good neighbors for training a RAG model. Such approximations would not be necessary if the search could be done in memory. This move to an IMC-based platform would lead to the introduction of noise and non-determinism. However, we have demonstrated that any hardware approximate noise is negligible, especially compared to the approximations made by the index itself. For illustration purposes, we measure the time it takes for the search function call to IndexIVF_search to complete and show the results in Table 5. This function is called on 16 chunks concurrently on a Tesla V100 GPU. As a way of profiling the speed of the index calls on Faiss we measure the time it takes for the search function call to IndexIVF_search to complete. This function is called on 16 chunks concurrently. We compare three retrieval databases, WikiText-103 with 1™877™559 entries, CNN-DailyMail with 3™488™000 entries, and SlimPajama with 45™107™000 entries. As the number of database entries almost doubles from WikiText-103 to CNN-DailyMail, so does the average search time. Due to the number of database entries, both have an index with 1024 centroids. However, as the number of database entries becomes 12 times larger from CNN-DailyMail to SlimPajama, the number of centroids becomes 4 times larger and the average search time only becomes 6 times longer. The nprobe parameter of how many out of the closest centroids to search is set to ncentroidsncentroids ncentroids . This emphasizes the role these parameters play in creating the index. Choosing such hyperparameters carefully can be either beneficial or detrimental to the results and speed of the retrieval. It is important to note that naively setting nprobe to ncentroids would lead to a longer search time than simply searching by brute force. 101010https://github.com/facebookresearch/faiss/wiki/Faster-search. Up to this point, all the results have been quantitative. To motivate retrieval, especially in domain shift, we now present some qualitative results. We generate the next chunk based on the context. Generation quality metrics such as fluency, diversity, and repetition depend almost exclusively on how well the generator function is written. Nvidia [52] and DeepMind [2] addressed this by only taking the greedy output. However, for real use cases, it is common to write the generator function with more care, as simple post-processing steps can make a substantial difference in the fluency and the diversity of the generated output. The model outputs a n_embÃn_vocab__£n n _ e m b Ã n _ v o c a b matrix which is interpreted as a probability distribution over the vocabulary words. In greedy generation, we take the vocabulary word with the highest probability, but that can lead to significant degeneration, see Figure 13. To generate more natural language, it can be beneficial to occasionally opt for something other than the most likely word. In multinomial generation, we address this issue with the help of the probability matrix. We cast the matrix as a multinomial distribution and sample from it. This ensures on one hand that it remains likely that we generate words with a higher probability, but also enables us to eventually pick less probable words as well, improving diversity and reducing repetition. Top-p generation is also called nucleus sampling and aims to address similar issues as multinomial generation. It also helps with diversity of language and repetition, but in this case, the focus is not just on reducing the use of common words, but eliminating them altogether. In nucleus sampling, we set a parameter ppp describing the cumulative probability. We then remove all tokens in the vocabulary with a cumulative probability above this threshold. This results in a new probability distribution, from which we now sample the next token. We generate the next chunks for the best checkpoints and the best-performing validation samples of Retro-li-on and -off. We chose the datasets WikiText-103 and SlimPajama, which are the two most extreme cases in terms of perplexity. In order to paint a fuller picture, we present the greedy output, the multinomial and top-p generated results, and the real continuation. Although there has been tremendous success in evaluating generated samples with GPT-4 (for instance [18] used it and motivated its use by comparing it to human judgment) we had no access to it. GPT-3.5-Turbo rankings of the generated samples are unfortunately not deterministic, and even its evaluation of this handful of qualitative examples rarely aligned with human judgment. For WikiText-103 both Retro-li-on and -off have the same validation sample with the lowest perplexity, so their continuations can be compared directly. Here we anticipate little difference in the outputs between off and on, as the perplexities are close. Their generated outputs are similar in fluency and comprehensiveness. The SlimPajama dataset is by far the worst in terms of perplexity. Though this can easily be addressed with minimal fine-tuning (see Section H.4), we are more interested in true plug-and-play performance, as it pertains to retrieval. The question becomes: Can retrieval alone help a language model generalize to unseen domains? In Section 3 we show that this is the case quantitatively, here we analyze the output qualitatively as well. Looking at Figure 13 and Figure 14 side-by-side, we can see the benefit of retrieval. The topic of this excerpt is a description of an HDMI cable. For Retro-li-off the generated outputs are vaguely about technology whereas Retro-li-on™s output is much more on-topic with respect to actual HDMI cables. To be completely fair, it must be said that this is the best validation sample for Retro-li-on, not for Retro-li-off. Although our focus so far has been on plug-and-play performance, fine-tuning is straightforward and improves the perplexities drastically with very few samples. We show some results here for minimal fine-tuning, on just one random seed, for our model trained with a Gaussian regularizer, Î»t=0.2subscript†¡0.2 _ t = 0.2. We set the number of samples to fine-tune on to 10% of the validation data size, to provide a balanced assessment. Interestingly, neither the initial perplexity nor the number of samples we fine-tune on is an indicator of how well the dataset takes to fine-tuning. Overall, we never have to fine-tune for longer than a few minutes to reach a perplexity of around 40. BBC-News has been fine-tuned the longest and yet has the second-highest final perplexity. This indicates that either the samples are chosen poorly or our model has trouble with this dataset in particular. Atticus contracts, on the other hand, dropped to a perplexity of around 16, which can be attributed to the repetitive nature of contracts.",
        "keywords": ""
    },
    {
        "id": 21,
        "title": "Some Diophantine Equations involving associated Pell numbers and repdigits",
        "abstract": "Abstract.In this paper, we explore the relationship between repdigits and associated Pell numbers, specifically focusing on two main aspects: expressing repdigits as the difference of two associated Pell numbers, and identifying which associated Pell numbers can be represented as the difference of two repdigits. Additionally, we investigate all associated Pell numbers which are concatenation of three repdigits. Our proof utilizes Baker™s theory on linear forms in logarithms of algebraic numbers, along with the Baker-Davenport reduction technique. The computations were carried out with the help of a simple computer program inMathematica.",
        "corpus": "In this paper, we explore the relationship between repdigits and associated Pell numbers, specifically focusing on two main aspects: expressing repdigits as the difference of two associated Pell numbers, and identifying which associated Pell numbers can be represented as the difference of two repdigits. Additionally, we investigate all associated Pell numbers which are concatenation of three repdigits. Our proof utilizes Baker™s theory on linear forms in logarithms of algebraic numbers, along with the Baker-Davenport reduction technique. The computations were carried out with the help of a simple computer program in Mathematica. A Diophantine equation is an algebraic or exponential equation with two or more variables intended to be solved in terms of integers only. These types of equations are named after the ancient Greek mathematician Diophantus. The associated Pell sequence (qn)n¥0subscriptsubscript0(q_{n})_{n 0}( q _ n ) _ n ¥ 0 is defined by the binary recurrence relation with initial conditions q0=1,q1=1formulae-sequencesubscript01subscript11 q_{0}=1, q_{1}=1q _ 0 = 1 , q _ 1 = 1. The closed form of associated Pell numbers is known as the Binet™s formula has the form qn=Î±n+Î²n2subscriptsuperscript¼superscript½2q_{n}= _ n = / Î± ^ n + Î² ^ n 2 , where (Î±,Î²)=(1+2,12)¼½1212( Î± , Î² ) = ( 1 + square-root 2 , 1 - square-root 2 ) is the pair of roots of the characteristic polynomial x22x1superscript¥22¥1x^{2}-2x-1x ^ 2 - 2 x - 1. This implies easily that the inequality holds for all n¥11n 1n ¥ 1. A palindromic number is one that remains unchanged when its digits are reversed. A special type of palindromic number, known as a repdigit, consists of a single distinct digit repeated multiple times in base 10. Mathematically, a repdigit can be expressed in the form d(10k19)d ( / 10 ^ k - 1 9 ) , where d{1,2,¦,9}12¦9d { 1 , 2 , ¦ , 9 } and k¥11k 1k ¥ 1. Notably, when k=11k=1k = 1, the result is simply the digit itself, representing a trivial case of a repdigit. Several authors have explored problems related to repdigits within the context of second-order linear recurrence sequences. All Balancing and Lucas-balancing numbers which are repdigits have been found in [14]. S. G. Rayaguru and G. K. Panda [13] investigated all balancing and Lucas-balancing numbers which can be expressed as the sums of two repdigits. Additionally, Rayaguru and Bravo [12] identified all Balancing and Lucas-balancing numbers formed by the concatenation of three repdigits. Erduwan et al. [7] found all Fibonacci and Lucas numbers which are difference of two repdigits. Edjeou and Faye [6] found all Pell and Pell-Lucas numbers, which are difference of two repdigits. MG Duman [5] identified all Padovan numbers representable as the difference of two repdigits. More recently, Mohapatra et al. [10] investigated the existence of repdigits as the difference of two Balancing or Lucas-balancing numbers, and they also enumerated all Balancing and Lucas-balancing numbers that can be expressed as the difference of two repdigits in [11]. This paper seeks to extend previous research by investigating the fascinating realm of repdigits. Specifically, we explore repdigits that can be expressed as the difference between two associated Pell numbers, those formed by concatenating three repdigits, and associated Pell numbers that can be represented as the difference of two repdigits. To facilitate this exploration, we consider the following equations: assume k¥11k 1k ¥ 1 to avoid trivial solutions. where m1,m2,m3¥1subscript1subscript2subscript31m_{1},m_{2},m_{3} 1m _ 1 , m _ 2 , m _ 3 ¥ 1, 1¤d1¤91subscript191 d_{1} 91 ¤ d _ 1 ¤ 9 and 0¤d2,d3¤9formulae-sequence0subscript2390 d_{2},d3 90 ¤ d _ 2 , d 3 ¤ 9. where (k,m,n)(k,m,n)( k , m , n ) are positive integers with n>mn>mn > m and n¥22n 2n ¥ 2. To solve the Diophantine equations, we will repeatedly invoke a Baker-type lower bound for a nonzero linear form in the logarithms of algebraic numbers. These lower bounds are instrumental in effectively resolving such equations. We shall commence by revisiting essential definitions and key results from the realm of algebraic number theory. Let Î»† be an algebraic number with minimal primitive polynomial where a0>0subscript00a_{0}>0a _ 0 > 0 is the leading coefficient and Î»(i)superscript† ^ ( i ) ™s are conjugates of Î»† Then the absolute ™¡absolutea b s o l u t e logarithmic™”¡logarithmicl o g a r i t h m i c height”¡heighth e i g h t of Î»† is given by h(Î»)=1k(loga0+j=1kh( a_{0}+ ( Î» ) = / 1 k ( roman_log a _ 0 + _ j = 1 ^ k max{0,log|Î»(j)|}) 0 , roman_log | Î» ^ ( j ) | } ). If Î»=ab† = / a b is a rational number with gcd(a,b)=1”1gcd(a,b)=1g c d ( a , b ) = 1 and b>11b>1b > 1, then h(Î»)=log†h( ( Î» ) = roman_log(max{|a|,b}) | a | , b } ). Here are some properties of the absolute ™¡absolutea b s o l u t e logarithmic™”¡logarithmicl o g a r i t h m i c height”¡heighth e i g h t whose proofs can be found in [2]. Let Î³¾ and Î·‚ be two algebraic numbers, then Building on the previous notations, we present a theorem that refines a result by Matveev [9], as extended by Bugeaud et al. [2]. This theorem offers a precise upper bound for our variables in equations (1.3), (1.4), and (1.5). [9]. Let Î³1,¦,Î³lsubscript¾1¦subscript¾™ _ 1 , ¦ , Î³ _ l be positive real numbers in an algebraic number field •ƒ•ƒ of degree d•ƒsubscript•ƒd_{ _ L and b1,¦,blsubscript1¦subscript™b_{1}, _ 1 , ¦ , b _ l be nonzero integers. If Î“=i=1lÎ³ibi1Î“superscriptsubscriptproduct1™superscriptsubscript¾subscript1 = _ i = 1 ^ l Î³ _ i ^ b _ i - 1 is not zero, then where D¥·absentD ¥ max{|b1|,¦,|bl|}subscript1¦subscript™ | b _ 1 | , ¦ , | b _ l | } and A1,‹¯,Alsubscript1‹¯subscript™A_{1}, _ 1 , ‹¯ , A _ l are positive integers such that Aj¥h²(Î³j)subscriptsuperscript²subscript¾A_{j} h^{ _ j ¥ h ^ ² ( Î³ _ j ) = max{d•ƒh(Î³j),|log¡Î³j|,0.16},¥subscript•ƒsubscript¾subscript¾0.16max a x { d _ L h ( Î³ _ j ) , | roman_log Î³ _ j | , 0.16 } , for j=1,¦,l1¦™j=1, = 1 , ¦ , l. The subsequent result, recognized as the Baker-Davenport theorem and credited to Dujella and PethÅ [4] is another tool in our proofs. It will be used to reduce the upper bounds on our variables. [4]. Let MMM be a positive integer and pq p q denote a convergent of the continued fraction of the irrational number such that q>6M6q>6Mq > 6 M. Consider the real numbers A,B,Î¼µA,B, , B , Î¼ with A>00A>0A > 0 and B>1µ1B>1B > 1. Let µ:=¥Î¼q¥M¥q¥assignitalic-µdelimited-¥¥delimited-¥¥ q q := ¥ Î¼ q ¥ - M ¥ q ¥, where ¥.¥ . ¥ denotes the distance from the nearest integer. If µ>0italic-µ0 > 0, then there exists no solution to the inequality in positive integers u,v,w£¤u,v,wu , v , w with u¤Mu Mu ¤ M and w¥log¡(Aq/µ)log¡B¤italic-µµw B}w ¥ / roman_log ( A q / µ ) roman_log B . We conclude this section by recalling the following lemma that we need in the sequel: [8]. Let r¥11r 1r ¥ 1 and H>0»0H>0H > 0 be such that H>(4r2)r»superscript4superscript2H>(4r^{2})^{r}H > ( 4 r ^ 2 ) ^ r and H>L/(log¡L)r»¿superscript¿H>L/( L)^{r}H > L / ( roman_log L ) ^ r . Then L<2rH(log¡H)r¿superscript2»superscript»L<2^{r}H( H)^{r}L < 2 ^ r H ( roman_log H ) ^ r . [3]. Let a,x¥a,x , x R. If 0<a<1010<a<10 < a < 1 and |x|<a¥|x|<a| x | < a, then and [15] The only associated Pell numbers which are repdigits are 1, 3, 7, and 99. [15] The only associated Pell numbers which are concatenation of two repdigits are 17, 41, and 577. All solutions of the Diophantine Eq.(1.3) in positive integers with n>mn>mn > m, k¥11k 1k ¥ 1, 1¤d¤9191 d 91 ¤ d ¤ 9, are (n,m,d,k)(n,m,d,k)( n , m , d , k ) {(2,0,2,1), (2,1,2,1), (3,2,4,1), (3,0,6,1), (3,1,6,1), (7,4,4,3)}. Using Mathematica¡¡MathematicaM a t h e m a t i c a, we get the repdigits as the difference of two associated Pell numbers for n¤9595n 95n ¤ 95 as listed in Theorem 3.1. So, assume that n>9595n>95n > 95. By using Binet™s formula, (1.3) can be written as We will now rearrange equation (3.1) into two distinct cases, as outlined below. First rearrangement of (3.1) is By applying the absolute value to both sides, we arrive at After applying Î±n2superscript¼2 Î± ^ n 2 as a divisor to both sides of the inequality, we find Put Î“1subscriptÎ“1 _ 1 = 1Î±n10k(2d9)1superscript¼superscript10291- - Î± ^ - n 10 ^ k ( / 2 d 9 ). It is enough to check that Î“1 0subscriptÎ“10 0roman_Î“ _ 1 0. On the contrary, suppose Î“1=0subscriptÎ“10 _ 1 = 0, then Î±n=10k(2d9) ^ n = 10 ^ k ( / 2 d 9 ) Q, which contradicts the fact that Î±nsuperscript¼ ^ n is irrational for any n>00n>0n > 0. Therefore, Î“1 0subscriptÎ“10 0roman_Î“ _ 1 0. To implement Theorem 1, define Î»1subscript†1 _ 1 = Î±¼ Î»2=10subscript†210 _ 2 = 10, Î»3subscript†3 _ 3 = 2d929 2 d 9 , b1=nsubscript1b_{1}=-nb _ 1 = - n, b2=ksubscript2b_{2}=kb _ 2 = k, b3=1,l=3formulae-sequencesubscript31™3b_{3}=1,l=3b _ 3 = 1 , l = 3, where Î»1subscript†1 _ 1 , Î»2subscript†2 _ 2 , Î»3subscript†3 _ 3 absent Q and b1,b2,b3¤subscript1subscript2subscript3¤b_{1},b_{2},b_{3} _ 1 , b _ 2 , b _ 3 Z. It can be observed that (Î»1,Î»2,Î»3)=(Î±)subscript†1subscript†2subscript†3¼ ( Î» _ 1 , Î» _ 2 , Î» _ 3 ) = Q ( Î± ), so d•ƒ=2subscript•ƒ2d_{ _ L = 2. Since k<nk<nk < n, we have D=·absentD=D =max{n,k,1}=n1 n , k , 1 } = n. The absolute logarithmic heights of Î»1subscript†1 _ 1 , Î»2subscript†2 _ 2 and Î»3subscript†3 _ 3 are computed as h(Î»1)=log¡Î±2subscript†1¼2h( ( Î» _ 1 ) = / roman_log Î± 2 , h(Î»2)=log¡10subscript†210h( 10h ( Î» _ 2 ) = roman_log 10 and h(Î»3)=h(2d)+h(9)<5.1subscript†3295.1h( ( Î» _ 3 ) = h ( 2 d ) + h ( 9 ) < 5.1 . Thus, we can express the following: Now, utilizing Theorem 2.1, we can determine a lower bound for log¡|Î“1|subscriptÎ“1 | roman_Î“ _ 1 | as By comparing the inequality presented above with (3.2), we find that We perform the second rearrangement of (3.1) as Î±n2Î±m2d10k9=Î²m2Î²n2d9superscript¼2superscript¼2superscript109superscript½2superscript½29 }}{2}- Î± ^ n 2 - / Î± ^ m 2 - / d 10 ^ k 9 = / Î² ^ m 2 - / Î² ^ n 2 - / d 9 . By taking the absolute values on both sides of the inequality, we arrive at When we / both sides of the inequality by Î±n2superscript¼2 Î± ^ n 2 (1Î±mn)1superscript¼(1- 1 - Î± ^ m - n ), it yields Define Î“2subscriptÎ“2 _ 2 = 1Î±n10k(2d9(1Î±mn))1- - Î± ^ - n 10 ^ k ( / 2 d 9 ( 1 - Î± ^ m - n ) ). In the same way, it can be shown that Î“2 0subscriptÎ“20 0roman_Î“ _ 2 0. Here we have h(Î»1)=h(Î±)=log¡Î±2subscript†1¼¼2h( ( Î» _ 1 ) = h ( Î± ) = / roman_log Î± 2 and h(Î»2)=h(10)=log¡10subscript†21010h( 10h ( Î» _ 2 ) = h ( 10 ) = roman_log 10. Let Î»3subscript†3 _ 3 = (2d9(1Î±mn)) / 2 d 9 ( 1 - Î± ^ m - n ) ). Then, Thus, from (3.3) we derive h(Î»3)<2.2‹…1013(1+log¡n)subscript†3‹…2.2superscript10131h( 10^{13}(1+ n)h ( Î» _ 3 ) < 2.2 ‹… 10 ^ 13 ( 1 + roman_log n ). Accordingly, we define A3subscript3A_{3}A _ 3 = 4.4 ‹…1013(1+log¡n)‹…absentsuperscript10131 10^{13}(1+ n)‹… 10 ^ 13 ( 1 + roman_log n ). By virtue of Theorem 2.1, A comparison of the above inequality with (3.4) yields With the notations of Lemma 2.2, we take r=2,L=2,H=1.9‹…1026log¡Î±formulae-sequence2formulae-sequence¿2»‹…1.9superscript1026¼r=2,L=2,H= 10^{26}}{ = 2 , L = 2 , H = / 1.9 ‹… 10 ^ 26 roman_log Î± to get n <<< 22superscript222^{2}2 ^ 2 (1.9‹…1026log¡Î±) 10^{26}}{ / 1.9 ‹… 10 ^ 26 roman_log Î± ) (log(1.9‹…1026log¡Î±))2 10^{26}}{ roman_log ( / 1.9 ‹… 10 ^ 26 roman_log Î± ) ) ^ 2 <<< 3.2‹…1030‹…3.2superscript10303.2 10^{30}3.2 ‹… 10 ^ 30 . We now need to apply the Baker-Davenport reduction method, as developed by Dujella and PethÅ [4], to refine the bound. Let We can rewrite the inequality (3.2) as Observe that Î1 0subscriptÎ10 0roman_Î _ 1 0 as eÎ11=Î“1 0superscriptsubscriptÎ11subscriptÎ“10e^{ 0e ^ roman_Î _ 1 - 1 = roman_Î“ _ 1 0. Under the assumption that nm¥44n-m 4n - m ¥ 4, the right-hand side of the above inequality is at most 9(1+2)4<129superscript12412 9 ( 1 + square-root 2 ) ^ 4 < / 1 2 . From the inequality |ez1|<ysuperscript§1¦|e^{z}-1|<y| e ^ z - 1 | < y with real values of z§zz and y¦yy, we deduce that z<2y§2¦z<2yz < 2 y. This leads us to the result |Î1|<18Î±nmsubscriptÎ118superscript¼| roman_Î _ 1 | < / 18 Î± ^ n - m , which indicates that |nlogÎ±+klog10+log(2d9)|<18Î±nm 10+ - n roman_log Î± + k roman_log 10 + roman_log ( / 2 d 9 ) | < / 18 Î± ^ n - m . Upon dividing both sides of the inequality by log¡Î±,¼ Î± , we find In order to implement Lemma 2.1, let us define u=k,=log¡10log¡Î±,v=n,Î¼=(log¡(2d9)log¡Î±),A=21,B=Î±,w=nmu=k, 10}{ )}{ = k , = / roman_log 10 roman_log Î± , v = n , Î¼ = ( / roman_log ( / 2 d 9 ) roman_log Î± ) , A = 21 , B = Î± , w = n - m. We can set M=3.2‹…1030‹…3.2superscript1030M=3.2 10^{30}M = 3.2 ‹… 10 ^ 30 as an upper bound on uuu. The denominator of 68-th convergent of denoted as q68subscript68q_{68}q _ 68 = 27232938992914655197439992935676, exceeds 6M66M6 M. Considering the fact that 1¤d¤9191 d 91 ¤ d ¤ 9, a quick computation with Mathematica¡¡MathematicaM a t h e m a t i c a gives us the inequality 0<µ:=¥Î¼q68¥M¥q68¥=0.0003890italic-µassigndelimited-¥¥subscript68delimited-¥¥subscript680.0003890< q_{68} q_{68} < µ := ¥ Î¼ q _ 68 ¥ - M ¥ q _ 68 ¥ = 0.000389. Applying Lemma 2.1 to the inequality (3.5) for 1¤d¤9191 d 91 ¤ d ¤ 9, we obtain nm¤9494n-m 94n - m ¤ 94. Now, for nm¤9494n-m 94n - m ¤ 94, put Î2=nlogÎ±+klog10+log(2d9(1Î±mn)) 10+ _ 2 = - n roman_log Î± + k roman_log 10 + roman_log ( / 2 d 9 ( 1 - Î± ^ m - n ) ). The inequality (3.4) can be expressed as Observe that Î2 0subscriptÎ20 0roman_Î _ 2 0 as eÎ21=Î“2 0superscriptsubscriptÎ21subscriptÎ“20e^{ 0e ^ roman_Î _ 2 - 1 = roman_Î“ _ 2 0. Assuming n¥22n 2n ¥ 2, the right-hand side in the above inequality is is limited to a maximum of 1212 1 2 . The inequality |ez1|<ysuperscript§1¦|e^{z}-1|<y| e ^ z - 1 | < y for real values of z§zz and y¦yy leads to the conclusion that z<2y§2¦z<2yz < 2 y. Therefore, we can assert |Î2|<10Î±nsubscriptÎ210superscript¼| roman_Î _ 2 | < / 10 Î± ^ n , which implies that Dividing both sides of the above inequality by log¡Î±¼ Î± yields Let Choose M=3.2‹…1030‹…3.2superscript1030M=3.2 10^{30}M = 3.2 ‹… 10 ^ 30 . We find q73=497885304750610764058413408775840subscript73497885304750610764058413408775840q_{73}=497885304750610764058413408775840q _ 73 = 497885304750610764058413408775840, the denominator of 73-th convergent of exceeds 6M66M6 M with 0 <µ:=¥Î¼q73¥M¥q73¥=0.107792absentitalic-µassigndelimited-¥¥subscript73delimited-¥¥subscript730.107792< q_{73} q_{73} µ := ¥ Î¼ q _ 73 ¥ - M ¥ q _ 73 ¥ = 0.107792. Applying Lemma 2.1 to the inequality (3.6) for 1¤d¤9191 d 91 ¤ d ¤ 9, we obtain n¤9191n 91n ¤ 91. This conclusion directly contradicts our initial assumption that n>9595n>95n > 95. The only associated Pell numbers which are concatenation of three repdigits are 239, 3363, and 8119. Assuming that (1.4) holds, we examined the first 50 associated Pell numbers and found that the solutions to the Diophantine equation (1.4) are qn{239,3363,8119}subscript23933638119q_{n} _ n { 239 , 3363 , 8119 } for d1,d2{0,1,¦,9}subscript1subscript201¦9d_{1},d_{2} _ 1 , d _ 2 { 0 , 1 , ¦ , 9 } with d1>0subscript10d_{1}>0d _ 1 > 0. From this point forward, we assume that n>5050n>50n > 50. The scenarios d1=d2 d3subscript1subscript2subscript3d_{1}=d_{2} d_{3}d _ 1 = d _ 2 d _ 3 and d1 d2=d3subscript1subscript2subscript3d_{1} d_{2}=d_{3}d _ 1 d _ 2 = d _ 3 in (1.4) are ruled out, as the only associated Pell numbers, which are concatenation of two repdigits are 17, 41, 577 by Lemma 2.5. Furthermore, the case d1=d2=d3subscript1subscript2subscript3d_{1}=d_{2}=d_{3}d _ 1 = d _ 2 = d _ 3 in (1.4) is also impossible since the largest repdigit in the associated Pell sequence is 99 as stated in Lemma 2.4. Let us define Then we can express qnsubscriptq_{n}q _ n as: which leads us to Alternatively, we can express it as: Combining the right side of inequality (1.2) with (4.1), we arrive at the following relationship: From this, we can conclude that: m1+m2+m3<n+2subscript1subscript2subscript32m_{1}+m_{2}+m_{3}<n+2m _ 1 + m _ 2 + m _ 3 < n + 2. We will now rearrange equation (4.2) in three distinct cases by using the Binet™s formula of associated Pell numbers. The first rearrangement of (4.2) is given by Taking the absolute values of both sides of (4.3) yields where n>5050n>50n > 50. Therefore, Dividing both sides of (4.4) by d110m1+m2+m3subscript1superscript10subscript1subscript2subscript3d_{1}10^{m_{1}+m_{2}+m_{3}}d _ 1 10 ^ m _ 1 + m _ 2 + m _ 3 gives us Now, let us apply Theorem 2.1 with the parameters Î³1:=9/(2d1),Î³2:=Î±,Î³3:=10formulae-sequenceassignsubscript¾192subscript1formulae-sequenceassignsubscript¾2¼assignsubscript¾310 _ 1 := 9 / ( 2 d _ 1 ) , Î³ _ 2 := Î± , Î³ _ 3 := 10 and b1:=1,b2:=nformulae-sequenceassignsubscript11assignsubscript2b_{1}:=1,b_{2}:=nb _ 1 := 1 , b _ 2 := n, b3:=m1m2m3assignsubscript3subscript1subscript2subscript3b_{3}:=-m_{1}-m_{2}-m_{3}b _ 3 := - m _ 1 - m _ 2 - m _ 3 . It is worth noting that Î³1,Î³2subscript¾1subscript¾2 _ 1 , Î³ _ 2 , and Î³3subscript¾3 _ 3 are positive real numbers and belong to the field •‚=(2)•‚2 = Q ( square-root 2 ), which has degree dL=2subscript¿2d_{L}=2d _ L = 2. We define: Setting Î“3=0subscriptÎ“30 _ 3 = 0, we arrive at However, this leads to a contradiction as Î±nsuperscript¼ ^ n is irrational for n¥11n 1n ¥ 1, which implies Î“3subscriptÎ“3 _ 3 must indeed be nonzero. Furthermore, utilizing the properties of absolute logarithmic height, we can obtain and Now, we can assign values A1:=5.8,A2:=0.9formulae-sequenceassignsubscript15.8assignsubscript20.9A_{1}:=5.8,A_{2}:=0.9A _ 1 := 5.8 , A _ 2 := 0.9, and A3:=4.62assignsubscript34.62A_{3}:=4.62A _ 3 := 4.62. Since, m1+m2+m3<n+2subscript1subscript2subscript32m_{1}+m_{2}+m_{3}<n+2m _ 1 + m _ 2 + m _ 3 < n + 2 and D¥max¡{|1|,|n|,|m1m2m3|}·1subscript1subscript2subscript3D ¥ roman_max { | 1 | , | n | , | - m _ 1 - m _ 2 - m _ 3 | }, we can conveniently set D:=n+2assign·2D:=n+2D := n + 2. Let us define: Analyzing inequality (4.5) in conjunction with Theorem 2.1, we obtain A straightforward computation yields the inequality Proceeding with the second rearrangement of equation (4.2) as and taking absolute values of both sides of (4.7), we get This equality leads to a series of inequalities based on the properties of absolute values as i.e. Dividing both sides of (4.8) by (d110m1(d1d2))10m2+m3subscript1superscript10subscript1subscript1subscript2superscript10subscript2subscript3 d _ 1 10 ^ m _ 1 - ( d _ 1 - d _ 2 ) ) 10 ^ m _ 2 + m _ 3 , we arrive at Let us introduce the following parameters: and With these definitions established, we can proceed to apply Theorem 2.1 . Here, dL=2subscript¿2d_{L}=2d _ L = 2 as the values Î³1,Î³2subscript¾1subscript¾2 _ 1 , Î³ _ 2 , and Î³3subscript¾3 _ 3 are positive real numbers and elements of the field K=Q(2)¾2K=Q( = Q ( square-root 2 ). We define Using the same arguments applied earlier for Î“3subscriptÎ“3 _ 3 , we conclude that Î“4 0subscriptÎ“40 0roman_Î“ _ 4 0. By employing the properties of the absolute logarithmic height, we obtain So, we can assign A1:=15.96+2m1log¡10,A2:=0.9formulae-sequenceassignsubscript115.962subscript110assignsubscript20.9A_{1}:=15.96+2m_{1} 10,A_{2}:=0.9A _ 1 := 15.96 + 2 m _ 1 roman_log 10 , A _ 2 := 0.9, and A3:=4.62assignsubscript34.62A_{3}:=4.62A _ 3 := 4.62. As m2+m3<nsubscript2subscript3m_{2}+m_{3}<nm _ 2 + m _ 3 < n and D¥max¡{|1|,|n|,|m2m3|}·1subscript2subscript3D ¥ roman_max { | 1 | , | - n | , | - m _ 2 - m _ 3 | }, we can take D:=nassign·D:=nD := n. Considering the inequality (4.9) and applying Theorem 2.1, we obtain From this, we derive Carrying out the third rearrangement of equation (4.2), we have By taking the absolute values of both sides of equation (4.11), we arrive at: This leads us to the pivotal inequality: Upon dividing both sides of (4.12) by 9Î±n/29superscript¼29 Î± ^ n / 2, we obtain Let us introduce the following parameters: Additionally, we set b1:=1,b2:=n,b3:=m3formulae-sequenceassignsubscript11formulae-sequenceassignsubscript2assignsubscript3subscript3b_{1}:=1,b_{2}:=-n,b_{3}:=m_{3}b _ 1 := 1 , b _ 2 := - n , b _ 3 := m _ 3 . This configuration allows us to apply Theorem 2.1. The parameters Î³1,Î³2subscript¾1subscript¾2 _ 1 , Î³ _ 2 , and Î³3subscript¾3 _ 3 are all positive real numbers that lie in the field K=(2)¾2K= = Q ( square-root 2 ) implying that dL=2subscript¿2d_{L}=2d _ L = 2. Let We can confirm that Î“5 0subscriptÎ“50 0roman_Î“ _ 5 0, just as for Î“3subscriptÎ“3 _ 3 . By leveraging the properties of the absolute logarithmic height, we obtain So, we assign A1:=21.8+2(m1+m2)log¡10+2m2log¡10,A2:=0.9formulae-sequenceassignsubscript121.82subscript1subscript2102subscript210assignsubscript20.9A_{1}:=21.8+2(m_{1}+m_{2}) 10+2m_{2} 10,A_{2}:=0.9A _ 1 := 21.8 + 2 ( m _ 1 + m _ 2 ) roman_log 10 + 2 m _ 2 roman_log 10 , A _ 2 := 0.9, and A3:=4.62assignsubscript34.62A_{3}:=4.62A _ 3 := 4.62. As m3<n1subscript31m_{3}<n-1m _ 3 < n - 1 and D¥max¡{|1|,|n|,|m3|}·1subscript3D ¥ roman_max { | 1 | , | - n | , | m _ 3 | }, we can take D:=nassign·D:=nD := n. Consequently, by taking the inequality (4.13) into account and invoking Theorem 2.1, we derive or Leveraging the inequalities (4.6), (13), and (4.14), a computational search conducted with Mathematica¡¡MathematicaM a t h e m a t i c a yields the result that n<5‹…1027‹…5superscript1027n<5 10^{27}n < 5 ‹… 10 ^ 27 . Now, let us try to tighten the upper bound on nnn by applying Lemma 2.2. We define From (4.5), we can deduce that By selecting a:=0.9995assign0.9995a:=0.9995a := 0.9995, we derive the inequality according to Lemma 2.3. Thus, it follows that Dividing this inequality by log¡Î±¼ Î±, we get We can now invoke Lemma 2.2. Let us define Let M:=5‹…1027assign‹…5superscript1027M:=5 10^{27}M := 5 ‹… 10 ^ 27 . Then M>m1+m2+m3subscript1subscript2subscript3M>m_{1}+m_{2}+m_{3}M > m _ 1 + m _ 2 + m _ 3 , ensuring that the denominator of the 66666666-th convergent of Î³¾ exceeds 6M66M6 M. We further define Consequently, the inequality (4.15) admits no solutions for Thus, we conclude that m1¤83subscript183m_{1} 83m _ 1 ¤ 83. Utilizing inequalities (4.10) and (4.14) together, and substituting this upper bound for m1subscript1m_{1}m _ 1 into (4.14), we obtain n<1.2‹…1027‹…1.2superscript1027n<1.2 10^{27}n < 1.2 ‹… 10 ^ 27 . Now, let us introduce From (4.9), we can assert that for m2¥1subscript21m_{2} 1m _ 2 ¥ 1. Choosing a:=0.2assign0.2a:=0.2a := 0.2, we derive the inequality as confirmed by Lemma 2.3. This leads us to the conclusion that Dividing both sides of the preceding inequality by log¡Î±¼ Î±, we get Putting Î³:=log¡10log¡Î±assign¾10¼ 10}{ := / roman_log 10 roman_log Î± and taking m2+m3<M:=1.2‹…1027subscript2subscript3assign‹…1.2superscript1027m_{2}+m_{3}<M:=1.2 10^{27}m _ 2 + m _ 3 < M := 1.2 ‹… 10 ^ 27 , we found that q63=7250590983807477127734940855subscript637250590983807477127734940855q_{63}=7250590983807477127734940855q _ 63 = 7250590983807477127734940855, the denominator of the 63-th convergent of Î³¾ exceeds 6M66M6 M. Next, we define Considering the constraints m1¤83,d1 d2,1¤d1¤9formulae-sequencesubscript183formulae-sequencesubscript1subscript21subscript19m_{1} 83,d_{1} d_{2},1 d_{1} 9m _ 1 ¤ 83 , d _ 1 d _ 2 , 1 ¤ d _ 1 ¤ 9 and 0¤d2¤90subscript290 d_{2} 90 ¤ d _ 2 ¤ 9, a quick computation with Mathematica¡¡MathematicaM a t h e m a t i c a yields the inequality Let A:=1.5,B:=10formulae-sequenceassign1.5assignµ10A:=1.5,B:=10A := 1.5 , B := 10, and w:=m2assign¤subscript2w:=m_{2}w := m _ 2 in Lemma 2.2. Using Mathematica¡¡MathematicaM a t h e m a t i c a, we conclude that the inequality (4.16) has no solutions for Consequently, we have m2¤75subscript275m_{2} 75m _ 2 ¤ 75. As m1¤83subscript183m_{1} 83m _ 1 ¤ 83 and m2¤75subscript275m_{2} 75m _ 2 ¤ 75, substituting this upper bounds for m1subscript1m_{1}m _ 1 and m2subscript2m_{2}m _ 2 into (4.14), we obtain n<5‹…1015‹…5superscript1015n<5 10^{15}n < 5 ‹… 10 ^ 15 . Now, let From (4.13), we can express Î“5subscriptÎ“5 _ 5 as valid for n¥5050n 50n ¥ 50. Choosing a:=0.01assign0.01a:=0.01a := 0.01, it follows that as per Lemma 2.3. This leads us to the conclusion: Dividing both sides of the above inequality by log¡Î±¼ Î±, we derive Let Î³:=log¡10log¡Î±assign¾10¼ 10}{ := / roman_log 10 roman_log Î± and consider m3<M:=5‹…1015subscript3assign‹…5superscript1015m_{3}<M:=5 10^{15}m _ 3 < M := 5 ‹… 10 ^ 15 . We have found that q40=30910886367884945subscript4030910886367884945q_{40}=30910886367884945q _ 40 = 30910886367884945, and notably, the denominator of the 40-th convergent of Î³¾ exceeds 6M66M6 M. Defining and taking into account the constraints m1¤83,m2¤75,1¤d1¤9formulae-sequencesubscript183formulae-sequencesubscript2751subscript19m_{1} 83,m_{2} 75,1 d_{1} 9m _ 1 ¤ 83 , m _ 2 ¤ 75 , 1 ¤ d _ 1 ¤ 9 and 0¤d2,d3¤9formulae-sequence0subscript2subscript390 d_{2},d_{3} 90 ¤ d _ 2 , d _ 3 ¤ 9, a swift computation with Mathematica¡¡MathematicaM a t h e m a t i c a reveals the inequality except in the scenarios where d1=d2 d3,d1 d2=d3formulae-sequencesubscript1subscript2subscript3subscript1subscript2subscript3d_{1}=d_{2} d_{3},d_{1} d_{2}=d_{3}d _ 1 = d _ 2 d _ 3 , d _ 1 d _ 2 = d _ 3 and d1=d2=d3subscript1subscript2subscript3d_{1}=d_{2}=d_{3}d _ 1 = d _ 2 = d _ 3 . Let us set A:=1.17,B:=Î±formulae-sequenceassign1.17assignµ¼A:=1.17,B:= := 1.17 , B := Î±, and w:=nassign¤w:=nw := n in Lemma 2.2. With the aid of Mathematica¡¡MathematicaM a t h e m a t i c a, we can confidently assert that the inequality (4.17) has no solution for Thus, we arrive at the conclusion n<4646n<46n < 46, which stands in direct contradiction to our assumption that n>5050n>50n > 50. This completes the proof. The only associated Pell numbers that can be expressed as the difference of two repdigits are 1, 3, 7, 17, and 41, i.e. q0=q1=1=98=87=76=65=54=43=32=21subscript0subscript119887766554433221q_{0}=q_{1}=1=9-8=8-7=7-6=6-5=5-4=4-3=3-2=2-1q _ 0 = q _ 1 = 1 = 9 - 8 = 8 - 7 = 7 - 6 = 6 - 5 = 5 - 4 = 4 - 3 = 3 - 2 = 2 - 1, q2=3=118subscript23118q_{2}=3=11-8q _ 2 = 3 = 11 - 8, q3=7=114subscript37114q_{3}=7=11-4q _ 3 = 7 = 11 - 4, q4=17=225subscript417225q_{4}=17=22-5q _ 4 = 17 = 22 - 5, and q5=41=443subscript541443q_{5}=41=44-3q _ 5 = 41 = 44 - 3. Assume that (1.5) holds. Let 1¤k¤501501 k 501 ¤ k ¤ 50 and n¥22n 2n ¥ 2. Utilizing Mathematica¡¡MathematicaM a t h e m a t i c a, we find only the solutions detailed in Theorem 5.1. So from this point forward, we assume that k>5050k>50k > 50. If n=mn=mn = m, then it follows that d1>d2subscript1subscript2d_{1}>d_{2}d _ 1 > d _ 2 , implying that qksubscriptq_{k}q _ k is a repdigit. However, the largest possible repdigit in qksubscriptq_{k}q _ k is 99 [14]. Thus, we get a contradiction since k>5050k>50k > 50. Next, the scenario where nm=11n-m=1n - m = 1. If d1¥d2subscript1subscript2d_{1} d_{2}d _ 1 ¥ d _ 2 , we encounter associated Pell numbers that are concatenations of two repdigits, which is impossible according to Lemma 2.5. If d1<d2subscript1subscript2d_{1}<d_{2}d _ 1 < d _ 2 , then we derive associated pell numbers that are concatenation of three repdigits, contradicting Theorem 4.1. Thus, we are left with the conclusion that nm¥22n-m 2n - m ¥ 2. The inequality implies that n<k+55n<k+5n < k + 5. We will now reorganize (1.5) into two distinct cases by using Binet™s formula of associated Pell numbers, as presented below. Continuing with the first rearrangement of equation (1.5), we obtain Taking the absolute value of both sides of (5.1), we obtain Dividing both sides of (5.2) by d110nsubscript1superscript10d_{1}10^{n}d _ 1 10 ^ n , we obtain From this, we conclude that Next, we apply Theorem 2.1 with (Î³1,Î³2,Î³3)subscript¾1subscript¾2subscript¾3( Î³ _ 1 , Î³ _ 2 , Î³ _ 3 ) = (Î±,10,9/2d1)¼1092subscript1( Î± , 10 , 9 / 2 d _ 1 ) and (b1,b2,b3)subscript1subscript2subscript3(b_{1},b_{2},b_{3})( b _ 1 , b _ 2 , b _ 3 ) = (k,n,1)1(k,-n,1)( k , - n , 1 ). Notably, Î³1subscript¾1 _ 1 , Î³2subscript¾2 _ 2 , and Î³3subscript¾3 _ 3 are positive real numbers and elements of the field •‚=(2)•‚2 = Q ( square-root 2 ). Consequently, the degree of the field •‚•‚ is equal to dL=2subscript¿2d_{L}=2d _ L = 2. Put We can ensure that Î“6 0subscriptÎ“60 0roman_Î“ _ 6 0 as per earlier arguments. Leveraging the properties of absolute logarithmic height, we can analyze the heights as h(Î³1)=log¡Î±2,subscript¾1¼2h( ( Î³ _ 1 ) = / roman_log Î± 2 , h(Î³2)=log¡10,subscript¾210h( 10,h ( Î³ _ 2 ) = roman_log 10 , and h(Î³3)¤h(2d1)+h(9)<5.09.subscript¾32subscript195.09h( h(2d_{1})+h(9)<5.09.h ( Î³ _ 3 ) ¤ h ( 2 d _ 1 ) + h ( 9 ) < 5.09 . We can set A1=log¡Î±subscript1¼A_{1}= _ 1 = roman_log Î±, A2=2log¡10subscript2210A_{2}=2 10A _ 2 = 2 roman_log 10, A3=10.18subscript310.18A_{3}=10.18A _ 3 = 10.18. Since n<k+55n<k+5n < k + 5 and D·DD ¥ max{n,k,1}1 n , k , 1 }, we can conveniently choose D=k+5·5D=k+5D = k + 5. Considering equation (5.3) and implementing Theorem 2.1, we obtain A straightforward calculation reveals that this inequality leads to Advancing to the second rearrangement of (1.5) as and taking the absolute value of both sides of (5.5), we arrive at Dividing both sides of the above inequality by Î±k2superscript¼2 Î± ^ k 2 , we find We can now apply Theorem 2.1 to the above inequality with (Î³1,Î³2,Î³3)subscript¾1subscript¾2subscript¾3( Î³ _ 1 , Î³ _ 2 , Î³ _ 3 ) = (Î±,10,(d1d210mn)18) Î± , 10 , / ( d _ 1 - d _ 2 10 ^ m - n ) 18 ) and (b1,b2,b3)subscript1subscript2subscript3(b_{1},b_{2},b_{3})( b _ 1 , b _ 2 , b _ 3 ) = (k,n,1)1(k,-n,1)( k , - n , 1 ). Crucially, Î³1subscript¾1 _ 1 , Î³2subscript¾2 _ 2 , and Î³3subscript¾3 _ 3 are positive real numbers that lie within the field •‚=(2)•‚2 = Q ( square-root 2 ). Thus, the degree of the field •‚•‚ is dL=2subscript¿2d_{L}=2d _ L = 2. Let If Î“7=0subscriptÎ“70 _ 7 = 0, then This leads to (d1d210mn)‹…10n‹…Î±k=18‹…subscript1subscript2superscript10superscript10superscript¼18(d_{1}-d_{2}10^{m-n}) 10^{n} d _ 1 - d _ 2 10 ^ m - n ) ‹… 10 ^ n ‹… Î± ^ - k = 18, implying Î±ksuperscript¼ ^ k Q, which is a contradiction for k>00k>0k > 0. Using properties of absolute logarithmic height, we obtain Next, we will estimate h(Î³3)=h((d1d210mn)18)h( ( Î³ _ 3 ) = h ( / ( d _ 1 - d _ 2 10 ^ m - n ) 18 ). Applying the properties of absolute logarithmic heights, we obtain With these heights established, we can define Since n<k+55n<k+5n < k + 5 and D·DD ¥ max{n,k,1}1 n , k , 1 }, we can take D=k+5·5D=k+5D = k + 5. Thus, considering (5.7) and applying Theorem 2.1, we derive where C=1.4‹…306‹…34.5‹…22¶‹…1.4superscript306superscript34.5superscript22C=-1.4 30^{6} 3^{4.5} 2^{2}C = - 1.4 ‹… 30 ^ 6 ‹… 3 ^ 4.5 ‹… 2 ^ 2 . By a simple computation, it follows that Utilizing (5.4) and (5.8), a computational search with Mathematica¡¡MathematicaM a t h e m a t i c a gives us that k<2‹…1028‹…2superscript1028k<2 10^{28}k < 2 ‹… 10 ^ 28 . Let us reduce the upper bound on kkk by using the Baker“Davenport algorithm as described in Lemma 2.1. We define From (5.3), we have for nm¥22n-m 2n - m ¥ 2. Choosing a=0.10.1a=0.1a = 0.1, we arrive at the inequality by Lemma2.2. Consequently, we deduce that Dividing this inequality by log¡1010 10roman_log 10, we obtain We can select =log¡Î±log¡10¼10 10} = / roman_log Î± roman_log 10 Q and M=2‹…1028‹…2superscript1028M=2 10^{28}M = 2 ‹… 10 ^ 28 . Notably, we find that q68=2512046602227734280329853086909subscript682512046602227734280329853086909q_{68}=2512046602227734280329853086909q _ 68 = 2512046602227734280329853086909, the denominator of the 68-th convergent of exceeding 6M66M6 M. Next, we define Î¼=log¡(9/2d1)log¡1092subscript110 10}Î¼ = / roman_log ( 9 / 2 d _ 1 ) roman_log 10 . In this case, considering the fact that 1¤d1¤91subscript191 d_{1} 91 ¤ d _ 1 ¤ 9, a quick computation with Mathematica¡¡MathematicaM a t h e m a t i c a reveals Let A=4.54.5A=4.5A = 4.5, B=10µ10B=10B = 10, and =nm” = n - m in Lemma 2.1. Thus, employing Mathematica¡¡MathematicaM a t h e m a t i c a, we can say that (5.9) has no solution if So nm¤8080n-m 80n - m ¤ 80. Substituting this upper bound for nmn-mn - m in (5.8), we derive the result k<8.8‹…1015‹…8.8superscript1015k<8.8 10^{15}k < 8.8 ‹… 10 ^ 15 . Now, let From (5.7), we have for k¥2525k 25k ¥ 25. Choosing a=0.10.1a=0.1a = 0.1, we get the inequality by Lemma 2.2. Thus, we conclude Dividing both sides by log¡Î±¼ Î±, we obtain Putting =log¡10log¡Î±10¼ 10}{ = / roman_log 10 roman_log Î± Q and taking M=8.8‹…1015‹…8.8superscript1015M=8.8 10^{15}M = 8.8 ‹… 10 ^ 15 , we found that q42=920197043232024959subscript42920197043232024959q_{42}=920197043232024959q _ 42 = 920197043232024959, the denominator of the 42-th convergent of exceeds 6M66M6 M. Now set Î¼=log(d1d210mn18)log¡Î± = / roman_log ( / d _ 1 - d _ 2 10 ^ m - n 18 ) roman_log Î± . In this scenario, noting that 1¤d1,d2¤9formulae-sequence1subscript1subscript291 d_{1},d_{2} 91 ¤ d _ 1 , d _ 2 ¤ 9 and 2¤nm¤802802 n-m 802 ¤ n - m ¤ 80, a quick computation yields the inequality Let A=3.63.6A=3.6A = 3.6, B=Î±µ¼B= = Î±, and =k” = k in Lemma 2.1. Therefore, utilizing Mathematica¡¡MathematicaM a t h e m a t i c a, we find that equation (5.10) has no solution if This leads to the result k¤4949k 49k ¤ 49, which contradicts our assumption that k>5050k>50k > 50. Hence, the proof is complete. Data Availability Statements: Data sharing is not applicable to this article as no datasets were generated or analyzed during the current study. Funding: The authors declare that no funds or grants were received during the preparation of this manuscript. Declarations: Conflict of interest: On behalf of all authors, the corresponding author states that there is no Conflict of interest.",
        "keywords": ""
    },
    {
        "id": 22,
        "title": "Noise Guided Structural Learning from Observing Stochastic Dynamics",
        "abstract": "AbstractWe develop an innovative learning framework that incorporate the noise structure to infer the governing equations from observation of trajectory data generated by stochastic dynamics. Our approach can proficiently captures both the noise and the drift terms. Furthermore, it can also accommodate a wide range of noise types, including correlated and state-dependent variations. Moreover, our method demonstrates scalability to high-dimensional systems. Through extensive numerical experiments, we showcase the exceptional performance of our learning algorithm in accurately reconstructing the underlying stochastic dynamics.",
        "corpus": "We develop an innovative learning framework that incorporate the noise structure to infer the governing equations from observation of trajectory data generated by stochastic dynamics. Our approach can proficiently captures both the noise and the drift terms. Furthermore, it can also accommodate a wide range of noise types, including correlated and state-dependent variations. Moreover, our method demonstrates scalability to high-dimensional systems. Through extensive numerical experiments, we showcase the exceptional performance of our learning algorithm in accurately reconstructing the underlying stochastic dynamics. Keywords” Stochastic Dynamics, Random Noise, System Identification Stochastic Differential Equations (SDEs) provides an accessible and flexible framework for fundamental modeling of stochastic phenomena arising in science and engineering applications [10, 28]. Compared to traditional deterministic differential equations (ODEs) models, SDEs can capture the underlying randomness of the systems, thus leading to more accurate descriptions of the complex behaviors. By incorporating a random component, typically through a Brownian motion, SDE provides a more realistic and flexible framework for simulating and predicting the behavior of these complex and dynamic systems. The SDE model considered here takes on the following form where the the drift term :d†d:†superscriptsuperscript{ : R ^ d † R ^ d and diffusion coefficient ƒ:d†dÃd:†superscriptsuperscript d}ƒ : R ^ d † R ^ d Ã d (ƒ is symmetric positive definite) can be both unknown, and the stochastic noise °tsubscript°¡{ _ t is a vector of independent standard Brownian motions. The noise structure of the SDE system is described by a state dependent covariance matrix Î£:d†dÃd:Î£†superscriptsuperscript d}roman_Î£ : R ^ d † R ^ d Ã d where Î£=ƒƒŠºÎ£superscriptŠº = ƒ ƒ ^ Šº . These SDE models are ubiquitous in physics, biology, finance, chemistry, and many other applications where they provide a robust framework for integrating noise directly into the evolution of system states. In physics, the Langevin equation [27, 6, 9, 32] models the behavior of particles under the influence of both systematic forces and random thermal fluctuations. The model offers insights into particle dynamics at microscopic scales where random forces dominate. Biology benefits from SDEs through models like the stochastic Lotka-Volterra equations, which describe the interactions between predator and prey populations under environmental uncertainty [31]. These models are vital for studying population dynamics where random events can significantly impact species survival and interaction.Additionally, SDEs are also applied in modeling biological systems [30] and cell dynamics [8]. In chemistry, SDEs model the kinetics of chemical reactions involving small numbers of molecules, where traditional deterministic models fail to capture the randomness of molecular collisions [36]. The Chemical Langevin Equation, for instance, is used to simulate reaction pathways in fluctuating environments. SDE models are at the core of mathematical finance, underpinning key areas such as option pricing, risk management, and modeling of interest rates, among which we mention classical Black-Scholes Model [2, 17], Vasicek Model [33] for analyzing interest rate dynamic, and Heston Model [14] for modeling stochastic volatility. Finally, we mention Diffusion Model [16] that currently got traction thanks to its formulation using SDE [29] that makes the analysis and improvement more flexible. The accurate application of SDEs critically depends on the proper calibration, or estimation of the drift and noise structure. Proper parameter estimation ensures that the SDEs not only reflect the theoretical properties of the systems but also closely align with observed phenomena. This alignment is crucial for the models to be truly predictive and reliable in practical applications. This requires the use of diverse statistical and mathematical techniques to ensure the models™ outputs align with empirical data, thereby enhancing their predictive and explanatory power. Since usually SDE models in each field of study have explicit function form of both drift and diffusion terms, one common method to calibrate or estimate the parameters is done by minimizing the least square error between the observation and model prediction [25, 1]. Statistical inference for SDEs has a long history, and we refer to [18] for more details. A canonical approach for estimating the drift is to derive a maximum-likelihood estimator by maximizing the likelihood function or the Radon“Nikodym derivative [20, Chapter 7], assuming that the entire trajectory {±t}t[0,T]subscriptsubscript±¡¡0 x _ t } _ t [ 0 , T ] is observed. This approach is employed in recent work of [13]. Following similar arguments, in this work we allowing state-dependent correlated noise, and using the likelihood function we are able to capture the essential structure of { from data with complex noise structure. System identification of the drift term from deterministic dynamics has been studied in many different scenarios, e.g. identification by enforcing sparsity such as SINDy [3], neural network based methods such as NeuralODE [4], PINN [26] and autoencoder [37], regression based [7], and high-dimensional reduction variational framework [23]. There are statistical methods which can be used to estimate the drift and noise terms using point-wise statistics. SINDy for SDEs was also developed in [34]. The observation data generated by SDEs can be treated as a time-series data with a mild assumption on the relationship between ±tsubscript±¡{ _ t and ±t+Î”tsubscript±¡Î”¡{ t}x _ t + roman_Î” t . Various deep neural network architectures can be used to learn the drift term as well as predicting the trajectory data, using RNN, LSTM, and Transformers, see [19, 38, 35] for detailed discussion. Furthermore, when the noise level becomes a constant, i.e. ƒ(±)=ƒ>0±0 ( x ) = ƒ > 0, we arrive at a much simpler loss which has been investigated in [22] in combination of high-dimensional ±±{ with a special structure in { the drift term. The uniqueness of our method is that we incorporate the covariance matrix into the learning and hence improving the estimation especially when the noise is correlated. In this paper, we develop a novel noise guided trajectory based learning approach to infer the governing structures of SDEs from observation data. Our method has contributed to the following aspects We develop a novel noise guided trajectory based learning method that can discover the governing structures of SDEs from data, including both the drift and the noise terms. Our method takes the noise into the consideration of the learning procedure and focuses on the overall evolution of the trajectory instead of focusing on one particular time point. We investigate the stability, accuracy and efficiency of our learning method over various kinds of SDEs with different noise structures. We showcase the superior performance of our algorithm using these examples. We allow the noise to have different structures. The remainder of the paper is structured as follows. Section 2 outlines the framework we use to learn the drift term and the noise structure. We demonstrates the effectiveness of our learning by testing it on various cases summarized in section 3. We conclude our paper in section 4 with a few pointers for ongoing and future developments. Let (Î©,”½,(”½t)0¤t¤T,™)Î©”½subscriptsubscript”½¡0¡™( t T}, roman_Î© , F , ( F _ t ) _ 0 ¤ t ¤ T , P ) be a filtered probability space, for a fixed and finite time horizon T>00T>0T > 0. As usual, the expectation operator with respect to ™™ will be denoted by ”¼™subscript”¼™ _ P or simply ”¼”¼ For random variables X,Y‹X,YX , Y we write X¼Ysimilar-to‹X YX ¼ Y, whenever X,Y‹X,YX , Y have the same distribution. We consider governing equations for stochastic dynamics of the following form with some given initial condition ±0¼Î¼0similar-tosubscript±0subscript0{ _ 0 ¼ Î¼ _ 0 , and where :d†d:†superscriptsuperscript{ : R ^ d † R ^ d is the drift term, ƒ:d†dÃd:†superscriptsuperscript d}ƒ : R ^ d † R ^ d Ã d is the diffusion coefficient (ƒ is symmetric and positive definite, i.e. ƒŠ¤=ƒsuperscripttop ^ Š¤ = ƒ and for any ±d±superscript{ R ^ d , ±Š¤ƒ±¥0superscript±top±0{ 0x ^ Š¤ ƒ x ¥ 0 and ±Š¤ƒ±=0superscript±top±0{ ^ Š¤ ƒ x = 0 if and only if ±=±0{ = 0), and °°{ represents a vector of independent standard Brownian Motions. The covariance matrix of the SDE system is a symmetric positive definite matrix denoted by Î£=Î£(™):d†dÃd:Î£Î£™†superscriptsuperscript d}roman_Î£ = roman_Î£ ( x ) : R ^ d † R ^ d Ã d where Î£=ƒƒŠºÎ£superscriptŠº = ƒ ƒ ^ Šº . We consider the experiment when we are given continuous observation data in the form of {±t,d¡±t}t[0,T]subscriptsubscript±¡dsubscript±¡¡0 x _ t , start_OPFUNCTION roman_d end_OPFUNCTION x _ t } _ t [ 0 , T ] for ±0¼Î¼0similar-tosubscript±0subscript0{ _ 0 ¼ Î¼ _ 0 , assuming { is the only unknown. We will estimate { by finding the minimizer to the following loss function for ~‹~‹ f caligraphic_H and Î£ superscriptÎ£ ^ is the pseudo-inverse of Î£Î£ (when ƒ is assumed to be SPD, Î£ =Î£1superscriptÎ£ superscriptÎ£1 ^ = roman_Î£ ^ - 1 ); the function space ‹‹{ is designed to be convex and compact w.r.t to the Lsuperscript¿L^{ ^ norm, and its construction is partially decided by the observation data, while ¨‹…,‹…©‹…‹… ‹… , ‹… © denotes the usuall inner product in dsuperscript ^ d . This loss function is derived from Girsanov theorem and the corresponding Randon-Nykodim derivative or likelihood ratio for stochastic processes; see [20, Chpater 7] and Section 2.2 for details. In the case of uncorrelated noise, i.e. Î£(™)=ƒ2(™)Î£™superscript2™ ( x ) = ƒ ^ 2 ( x ) I, where { is the dÃdd dd Ã d identity matrix and ƒ:d†+:†superscriptsuperscript : R ^ d † R ^ + is a scalar function depending on the state and representing the noise level, the loss function equation 2 can be simplified to We estimate the covariance matrix Î£Î£ by usual quadratic (co)variation arguments. Namely, the estimation of Î£Î£ is the minimizer of the following loss function where [±,±]Tsubscript±±[{ x , x ] _ T is the quadratic variation of the stochastic process ±tsubscript±¡{ _ t over time interval [0,T]0[0,T][ 0 , T ]. We recall that for two stochastic processes ±isubscript±{ _ i and ±jsubscript±{ _ j the quadratic variation over time interval [0,t]0¡[0,t][ 0 , t ] is defined by where {tk}subscript¡ t _ k } is a partition of interval [0,t]0¡[0,t][ 0 , t ]. Respectively, for a vector of stochastic processes ±t=(±1(t),±2(t),¦,±d(t))Šºsubscript±¡superscriptsubscript±1¡subscript±2¡¦subscript±¡Šº{ {d}(t))^{ _ t = ( x _ 1 ( t ) , x _ 2 ( t ) , ¦ , x _ d ( t ) ) ^ Šº , the quadratic variation [±,±]tsubscript±±¡[{ x , x ] _ t is the matrix with entries [±i,±j]t,i,j=1,¦,dformulae-sequencesubscriptsubscript±subscript±¡1¦[{ i,j=1, x _ i , x _ j ] _ t , i , j = 1 , ¦ , d. Estimation of the diffusion coefficient ƒ~~ ƒ is hence calculated by spectrum decomposition of Î£~~Î£ roman_Î£ . In particular, if Î£Î£ is constant, then the estimation can be simplified to Î£~=”¼[±,±]TT~Î£”¼subscript±± roman_Î£ = E / [ x , x ] _ T T . Note that estimation of Î£Î£ does not dependent on the drift function { Consequently, when both { and Î£Î£ are unknown, Î£Î£ can be estimated first, allowing the estimated covariance matrix to be used to implement equation 2. Discrete Data: however in real life applications, the data is not given in its time-continuous form, and usually, the observer has access to data collected over several independently sampled trajectories observed at some discrete time points {±lm}l,m=1L,Msuperscriptsubscriptsuperscriptsubscript±™™1¿ x _ l ^ m } _ l , m = 1 ^ L , M , where ±lm=±(m)(tl)superscriptsubscript±™superscript±subscript¡™{ _ l ^ m = x ^ ( m ) ( t _ l ) with 0=t1<¦<tL=T0subscript¡1¦subscript¡¿0=t_{1}< = t _ 1 < ¦ < t _ L = T and ±0msuperscriptsubscript±0{ _ 0 ^ m is an i.i.d sample from Î¼0subscript0 _ 0 . In order to properly gauge the accuracy of our learning estimators, we provide three different performance measures of our estimated drift. First, if we have access to original drift function { then we will use the following error to compute the difference between ^^ f (our estimator) to { with the following norm where the weighted measure defined on dsuperscript ^ d , is given as follows The norm given by equation 5 is useful only from the theoretical perspective, e.g. showing convergence. Under normal circumstances, { is most likely non-accessible. Thus we look at a performance measure that compares the difference between (,±0,T)={±t}t[0,T]subscript±0subscriptsubscript±¡¡0{ ( f , x _ 0 , T ) = { x _ t } _ t [ 0 , T ] (the observed trajectory that evolves from ±0¼Î¼0similar-tosubscript±0subscript0{ _ 0 ¼ Î¼ _ 0 with the unknown { and ^(^,±0,T)={±^t}t[0,T]^^subscript±0subscriptsubscript^±¡¡0 X ( over^ f , x _ 0 , T ) = { over^ x _ t } _ t [ 0 , T ] (the estimated trajectory that evolves from the same ±0subscript±0{ _ 0 with the learned ^^ f and driven by the same realized random noise as used by the original dynamics). Then, the difference between the two trajectories is measured as follows However, comparing two sets of trajectories (even with the same initial condition) on the same random noise is not realistic. We compare the distribution of the trajectories over different initial conditions and all possible noise at some chosen time snapshots using the Wasserstein distance at any given time t[0,T]¡0t [ 0 , T ]. Let Î¼tMsubscriptsuperscript¡ ^ M _ t be the empirical distribution at time t¡tt for the simulation under { with MMM trajectories, and Î¼^tMsubscriptsuperscript^¡ Î¼ ^ M _ t be the empirical distribution at time t¡tt for the simulation with MMM trajectories under ^^ f where: Then the Wasserstein distance of order two between Î¼tMsubscriptsuperscript¡ ^ M _ t and Î¼^tMsubscriptsuperscript^¡ Î¼ ^ M _ t is calculated as Here, Î (Î¼tM,Î¼^tM|Î¼0)Î subscriptsuperscript¡conditionalsubscriptsuperscript^¡subscript0 ( Î¼ ^ M _ t , over^ Î¼ ^ M _ t | Î¼ _ 0 ) is the set of all joint distributions on dÃdsuperscriptsuperscript ^ d Ã R ^ d with marginals Î¼tMsubscriptsuperscript¡ ^ M _ t and Î¼^tMsubscriptsuperscript^¡ Î¼ ^ M _ t , and with the additional constraint that the joint distribution must be consistent with the initial distribution of ±0subscript±0{ _ 0 following Î¼0subscript0 _ 0 . We discuss the theoretical foundation of our methods in this section. Consider two ItÃ processes defined over measurable space (Î©,”½)Î©”½( roman_Î© , F ) and let ™Xsubscript™‹ _ X , ™Ysubscript™ _ Y be probability measures corresponding to processes ±±{ and ²²{ where satisfying all assumptions in [20, Theorem 7.18] and its following corollary. Then, the Radon-Nikodym derivative, or the likelihood ratio, takes the form where Î£ superscriptÎ£ ^ is the pseudo-inverse of Î£=ƒƒŠºÎ£superscriptŠº = ƒ ƒ ^ Šº . Denote the observation as {±t}t[0,T]subscriptsubscript±¡¡0 x _ t } _ t [ 0 , T ] . Since the assumptions of [20, Theorem 7.18] are satisfied, Î=ƒ ((±t)(±t))Îsuperscript subscript±¡subscript±¡ = ƒ ^ ( f ( x _ t ) - g ( x _ t ) ) is an nnn-dimensional adapted process and «0TÎ2d¡t<superscriptsubscript0superscriptnormÎ2d¡ _ 0 ^ T | | roman_Î | | ^ 2 start_OPFUNCTION roman_d end_OPFUNCTION t < . By Girsanov theorem, °t~=°t+«0TÎsd¡s~subscript°¡subscript°¡superscriptsubscript0subscriptÎ d w _ t = w _ t + « _ 0 ^ T roman_Î _ s start_OPFUNCTION roman_d end_OPFUNCTION s is an nnn-dimensional standard Brownian motion under probability measure ™Ysubscript™ _ Y . Hence, d¡±t=(±t)d¡t+ƒ(±t)(d¡°t~Îtd¡t)=(±t)d¡t+ƒ(±t)d¡°t~dsubscript±¡subscript±¡d¡subscript±¡d~subscript°¡subscriptÎ¡d¡subscript±¡d¡subscript±¡d~subscript°¡ }t+ {t} roman_d end_OPFUNCTION x _ t = f ( x _ t ) start_OPFUNCTION roman_d end_OPFUNCTION t + ƒ ( x _ t ) ( start_OPFUNCTION roman_d end_OPFUNCTION over~ w _ t - roman_Î _ t start_OPFUNCTION roman_d end_OPFUNCTION t ) = g ( x _ t ) start_OPFUNCTION roman_d end_OPFUNCTION t + ƒ ( x _ t ) start_OPFUNCTION roman_d end_OPFUNCTION over~ w _ t . To simplify calculation, we set =00{ = 0. Then ±tsubscript±¡{ _ t becomes a Brownian process under ™Ysubscript™ _ Y therefore ™Y({±t}t[0,T]|)subscript™conditionalsubscriptsubscript±¡¡0 _ Y ( { x _ t } _ t [ 0 , T ] | f ) is now independent from { since ±tsubscript±¡{ _ t has no drift term under ™Ysubscript™ _ Y . Then we derive our loss function as the negative log likelihood function In this subsection, we will discuss in details how the algorithm is implemented for our learning framework. Practically speaking, data is rarely sampled continuously in time. Instead, observers typically have access to fragmented data sets, gathered from multiple independently sampled trajectories at specific, discrete time points{±lm}l,m=1L,Msuperscriptsubscriptsuperscriptsubscript±™™1¿ x _ l ^ m } _ l , m = 1 ^ L , M , where ±lm=±(m)(tl)superscriptsubscript±™superscript±subscript¡™{ _ l ^ m = x ^ ( m ) ( t _ l ) with 0=t1<‹¯<tL=T0subscript¡1‹¯subscript¡¿0=t_{1}< = t _ 1 < ‹¯ < t _ L = T and ±0msuperscriptsubscript±0{ _ 0 ^ m is an i.i.d sample from Î¼0subscript0 _ 0 . We use a discretized version of 2, for ~‹~‹ f caligraphic_H. Moreover, we also assume that ‹‹{ is a finite-dimensional function space, i.e. dim¡(‹)=n<dim‹ ( caligraphic_H ) = n < . Then for any ~‹~‹ f caligraphic_H, ~(™)=i=1n‚ii(™)~™superscriptsubscript1subscript‚subscript“™ f ( x ) = _ i = 1 ^ n a _ i _ i ( x ), where ‚idsubscript‚superscript{ _ i R ^ d is a constant vector coefficient and i:«Š‚d†:subscript“«superscript† _ i : D Š‚ R ^ d † R is a basis of ‹‹{ and the domain ««{ is constructed by finding out the min/max / roman_max of the components of ±tdsubscript±¡superscript{ _ t R ^ d for t[0,T]¡0t [ 0 , T ]. We consider two methods for constructing isubscript“ _ i : i) use pre-determined basis such as piecewise polynomials or Clamped B-spline, Fourier basis, or a mixture of all of the aforementioned ones; ii) use neural networks, where the basis functions are also trained from data. Next, we can put the basis representation of ~~ f back to equation 12, we obtain the following loss based on the coefficients In the case of diagonal covariance matrix Î£Î£ i.e. we can re-write equation 13 as Here (™)ksubscript™({ x ) _ k is the kthsuperscript¡k^{th}k ^ t h component of any vector ™d™superscript{ R ^ d . We define ¶k=[(‚1)k‹¯(‚n)k]Š¤nsubscript¶superscriptmatrixsubscriptsubscript‚1‹¯subscriptsubscript‚topsuperscript _ k = [ start_ROW start_CELL ( a _ 1 ) _ k end_CELL start_CELL ‹¯ end_CELL start_CELL ( a _ n ) _ k end_CELL end_ROW ] ^ Š¤ R ^ n , and AknÃnsubscriptsuperscriptA_{k} n}A _ k R ^ n Ã n as and ƒknsubscriptƒsuperscript{ _ k R ^ n as Then equation 13 can be re-written as Since each ¶kŠ¤Ak¶k2¶kŠ¤ƒksuperscriptsubscript¶topsubscriptsubscript¶2superscriptsubscript¶topsubscriptƒ _ k ^ Š¤ A _ k Î± _ k - 2 Î± _ k ^ Š¤ b _ k is decoupled from each other, we just need to solve simultaneously Then we can obtain ^(™)=i=1n‚^ik(™)^™superscriptsubscript1subscript^‚subscript“™ f ( x ) = _ i = 1 ^ n over^ a _ i _ k ( x ). However when Î£Î£ does not have a diagonal structure, we will have to resolve to gradient descent methods to minimize equation 13 in order to find the coefficients {‚i}i=1nsuperscriptsubscriptsubscript‚1 a _ i } _ i = 1 ^ n for a total number of ndndn d parameters. If a data-driven basis is desired, we set ‹‹{ to be the space of neural networks with the same depth, number of neurons, and activation functions in the hidden layers. Furthermore, we find ^^ f by minimizing equation 12 using any deep learning optimizer, such as Stochastic Gradient Descent or Adam, from well-known deep learning packages. Similarly, we employ a discretized form of equation 4 for estimating Î£Î£ The estimation involves approximating (1+d)d212 ( 1 + d ) d 2 functions, corresponding to the components of the symmetric covariance matrix Î£Î£ We denote the jjj-th entry of the vector ±lmsuperscriptsubscript±™{ _ l ^ m as ±l,jmsuperscriptsubscript±™{ _ l , j ^ m and the (k,j)(k,j)( k , j ) component of Î£Î£ as Î£kjsubscriptÎ£ _ k j . The component Î£~kjsubscript~Î£ roman_Î£ _ k j is estimated by minimizing the loss function using a neural network approach, defined as The estimation of Î£~~Î£ roman_Î£ is completed by assembling the estimated components for each k,j=1,¦,dformulae-sequence1¦k,j=1, , j = 1 , ¦ , d. In this section, we demonstrate the application of our trajectory-based method for estimating drift functions and noise structures, showcasing a variety of examples. We explore drift functions ranging from polynomials to trigonometric functions. We also tested our method on various types of covariance matrices, including constant, non-diagonal, and state-dependent covariance matrices. Our function estimation job is carried out in both basis method and deep learning method with 2 and 4 being loss functions for estimating drift and covariance, respectively. The observations, serving as the input dataset for testing our method, are generated by the Euler-Maruyama scheme [15], utilizing the drift functions as we just mentioned. The basis space ‹‹{ is constructed employing either B-spline or piecewise polynomial methods for maximum degree p-max equals 2222. For higher order dimensions where d¥22d 2d ¥ 2, each basis function is derived through a tensor grid product, utilizing one-dimensional basis defined by knots that segment the domain in each dimension. The common parameters for the following examples are listed in Table 1. Other parameters will be specified in each subsection of examples. The estimation results are evaluated using several different metrics. We record the noise terms, d¡°tdsubscript°¡ roman_d end_OPFUNCTION w _ t , from the trajectory generation process and compare the trajectories produced by the estimated drift functions, ^^ f , under identical noise conditions. We examine trajectory-wise errors using equation 7 with relative trajectory error and plot both { and ^^ f to calculate the relative L2superscript¿2L^{2}L ^ 2 error using 5, where is derived by 6. When plotting, trajectories with different initial conditions are represented by distinct colors. In trajectory-wise comparisons, solid lines depict the true trajectories, while dashed lines represent those generated by the estimated drift functions. Additionally, the empirical measure is shown in the background of each 1d plot. Furthermore, we assess the distribution-wise discrepancies between observed and estimated results, computing the Wasserstein distance at various time steps with equation 9. For d=11d=1d = 1, we first test our learning scheme on polynomial drift function =2+0.08±0.01±220.08±0.01superscript±2{ = 2 + 0.08 x - 0.01 x ^ 2 . The estimation results are depicted in 1 and detailed in Table 2. We initiate our numerical study with the estimation of a one-dimensional (d=11d=1d = 1) drift function that incorporates both polynomial and trigonometric components, given by =2+0.08±0.05sin¡(±)+0.02cos2¡(±)20.08±0.05±0.02superscript2±{ = 2 + 0.08 x - 0.05 roman_sin ( x ) + 0.02 roman_cos ^ 2 ( x ). In this example, we assume the diffusion coefficient is a known constant and set ƒ=0.60.6 = 0.6. Figure 2 illustrates the comparison between the true drift function { and the estimated drift function ^^ f , alongside a comparison of trajectories. Notably, Figure 2(a) on the left includes a background region depicting the histogram of empirical as defined in equation 6. This visualization reveals that in regions where ±±{ has a higher density of observations”indicated by higher histogram values”the estimation of ^^ f tends to be more accurate. Conversely, in less dense regions of the dataset (two ends of the domain), the estimation accuracy of ^^ f diminishes. Table 3 presents a detailed quantitative analysis of the estimation results, including the L2superscript¿2L^{2}L ^ 2 norm difference between { and ^^ f , as well as the trajectory error. Furthermore, the table compares the distributional distances between ±tsubscript±¡{ _ t and ±^tsubscript^±¡ x _ t at selected time steps, with the Wasserstein distance results included. We continue our numerical investigation with a one-dimensional (d=11d=1d = 1) drift function which is given by =0.08±0.08±{ = 0.08 x. Figure 3 illustrates the comparison between the true drift function { and the estimated drift function ^^ f , alongside a comparison of trajectories. The setup of figures are similar to the ones presented in previous section. The error for learning { turns out to be bigger, especially towards the two end points of the interval. However, the errors happen mostly during the two end points of the data interval, where the distribution of the data appears to be small, i.e. few data present in the learning. We are able to recover most of the trajectory. For d=22d=2d = 2, we test two types of drift function { polynomial and trigonometric. Denote where i:2†:subscript†superscript2{ _ i : R ^ 2 † R and ±isubscript±{ _ i R for i{1,2}12i { 1 , 2 }. For polynomial drift function { we set Figure 4, Figure 5(a), 5(b) and Table 4 shows evaluation of the polynomial drift function estimation result. For trigonometric drift function { we set Figure 6, Figure 7(a), 7(b) and Table 5 shows evaluation of the trigonometric drift function estimation result. In this example, we incorporate a non-diagonal covariance matrix into a two-dimensional (d=22d=2d = 2) SDE system. As specified in table 1, all parameters remain unchanged except for MMM. We change total trajectory observation number MMM to 1000100010001000 for faster calculation. And we assume covariance matrix is known and set This change in ƒ implies that the Brownian motions within the system are correlated. The drift function is defined using the notation, =[f1(±)f2(±)]Š¤superscriptmatrixsubscript“1±subscript“2±top{ = [ start_ROW start_CELL f _ 1 ( x ) end_CELL start_CELL f _ 2 ( x ) end_CELL end_ROW ] ^ Š¤ and ±=[±1±2]Š¤±superscriptmatrixsubscript±1subscript±2top{ = [ start_ROW start_CELL x _ 1 end_CELL start_CELL x _ 2 end_CELL end_ROW ] ^ Š¤ , where i:2†:subscript†superscript2{ _ i : R ^ 2 † R and ±isubscript±{ _ i R for i{1,2}12i { 1 , 2 }. For polynomial drift function { we set f1=0.4±10.1±1±2subscript“10.4subscript±10.1subscript±1subscript±2f_{1}=0.4{ _ 1 = 0.4 x _ 1 - 0.1 x _ 1 x _ 2 and f2=0.8±2+0.2±12subscript“20.8subscript±20.2superscriptsubscript±12f_{2}=-0.8{ _ 2 = - 0.8 x _ 2 + 0.2 x _ 1 ^ 2 . The estimation results are presented in figure 8(a) and 8(b) and table 6. Note that differences in boundary values of { and ^^ f are attributed to less density of observations, similar to what we discussed in the 1d11d1 d case at both endpoints. In this example, we assume that both the drift function { and the variance Î£Î£ are unknown. We define ƒ=0.2±0.2± = 0.2 x. In the one-dimensional (d=11d=1d = 1) case, the covariance matrix Î£Î£ reduces to the variance ƒ2=0.04±2superscript20.04superscript±2 ^ 2 = 0.04 x ^ 2 . The estimation of ƒ2superscript2 ^ 2 is conducted via deep learning, using the loss function defined in equation 4. Figure 9 shows the estimation result. The background of the figure is the histogram of ±tsubscript±¡{ _ t , indicating regions with higher and lower observation densities. In the regions with more observations, the estimated variance closely follows the true variance, which validates our learning theory. We push forward our estimation of covariance matrix Î£Î£ to the two-dimensional (d=22d=2d = 2) case. We keep the assumption that both { and Î£Î£ are unknown. We set ƒ as: where the components ƒ11=0.4±1subscript110.4subscript±1 _ 11 = 0.4 x _ 1 , ƒ12=ƒ21=0.025±1±2subscript12subscript210.025subscript±1subscript±2 _ 12 = ƒ _ 21 = 0.025 x _ 1 x _ 2 , ƒ22=0.6±2subscript220.6subscript±2 _ 22 = 0.6 x _ 2 are all state dependent and ±=[x1x2]Š¤±superscriptmatrixsubscript¥1subscript¥2top{ = [ start_ROW start_CELL x _ 1 end_CELL start_CELL x _ 2 end_CELL end_ROW ] ^ Š¤ . Figure 10 shows the estimation results where the first row displays the true surfaces of the components of Î£Î£ and the second row presents the corresponding estimated surfaces. The estimated surfaces show only slight differences from the true ones, demonstrating the accuracy of our method. We extend our numerical test to the three-dimensional (d=33d=3d = 3) case. In this scenario, we assume that ƒ is a known constant matrix where It is changeling to visualize three-dimensional function in a clear and neat way. Therefore, estimation result for this example is evaluated by the measures outlined in Section 2.1. The drift function is defined using the notation, =[f1(±)f2(±)f3(±)]Š¤superscriptmatrixsubscript“1±subscript“2±subscript“3±top{ {x}}) = [ start_ROW start_CELL f _ 1 ( x ) end_CELL start_CELL f _ 2 ( x ) end_CELL start_CELL f _ 3 ( x ) end_CELL end_ROW ] ^ Š¤ and ±=[x1x2x3]Š¤±superscriptmatrixsubscript¥1subscript¥2subscript¥3top{ = [ start_ROW start_CELL x _ 1 end_CELL start_CELL x _ 2 end_CELL start_CELL x _ 3 end_CELL end_ROW ] ^ Š¤ , where i:3†:subscript†superscript3{ _ i : R ^ 3 † R and ±isubscript±{ _ i R for i{1,2,3}123i { 1 , 2 , 3 }. For drift function { we set f1=0.05±10.01±1±2subscript“10.05subscript±10.01subscript±1subscript±2f_{1}=0.05{ _ 1 = 0.05 x _ 1 - 0.01 x _ 1 x _ 2 , f2=0.08±20.05±22subscript“20.08subscript±20.05superscriptsubscript±22f_{2}=0.08{ _ 2 = 0.08 x _ 2 - 0.05 x _ 2 ^ 2 and f3=0.05±30.02±2±3subscript“30.05subscript±30.02subscript±2subscript±3f_{3}=0.05{ _ 3 = 0.05 x _ 3 - 0.02 x _ 2 x _ 3 . We change total trajectory observation number MMM to 1000100010001000 for faster calculation. The estimation result is displayed in table 7. We consider a high dimensional SDE case where the drift term has a special structure. Such special structure will allow us to learn the high-dimensional SDE more effectively through an innate dimension reduction approach. This high dimensional SDE case is a presentation of an interacting agent system. Learning of such systems without stochastic noise terms had been investigated in [23, 39, 24, 11, 12]. We consider such system with correlated stochastic noise, i.e. for a system of NNN agents, where each agent is associated with a state vector ±id²subscript±superscriptsuperscript²{ _ i R ^ d ^ ² . The agents™ states are governed by the following SDEs Here •:+†:italic-•†superscript : R ^ + † R is an interaction kernel that governs how agent jjj influences the behavior of agent iii, and ƒ:d²†d²:†superscriptsuperscript²superscriptsuperscript² : R ^ d ^ ² † R ^ d ^ ² is a symmetric positive definite matrix that represents the noise. If we define the vectorized notations, i.e. ±=[±1Š¤‹¯±NŠ¤]Š¤±superscriptmatrixsuperscriptsubscript±1top‹¯superscriptsubscript±toptop{ = [ start_ROW start_CELL x _ 1 ^ Š¤ end_CELL start_CELL ‹¯ end_CELL start_CELL x _ N ^ Š¤ end_CELL end_ROW ] ^ Š¤ , °=[°1Š¤‹¯°NŠ¤]Š¤d=Nd²°superscriptmatrixsuperscriptsubscript°1top‹¯superscriptsubscript°toptopsuperscriptsuperscript²{ = [ start_ROW start_CELL w _ 1 ^ Š¤ end_CELL start_CELL ‹¯ end_CELL start_CELL w _ N ^ Š¤ end_CELL end_ROW ] ^ Š¤ R ^ d = N d ^ ² and Here :d†d:†superscriptsuperscript{ : R ^ d † R ^ d and ƒ~:d†dÃd:~†superscriptsuperscript d}over~ ƒ : R ^ d † R ^ d Ã d . Then the system can be put into one single SDE of the form d¡±t=(±t)d¡t+ƒ~(±t)d¡°tdsubscript±¡subscript±¡d¡~subscript±¡dsubscript°¡ }t+ roman_d end_OPFUNCTION x _ t = f ( x _ t ) start_OPFUNCTION roman_d end_OPFUNCTION t + over~ ƒ ( x _ t ) start_OPFUNCTION roman_d end_OPFUNCTION w _ t . We will consider a slightly changed “2subscript“2 _ 2 inner product for these vectors, i.e. for ®,¯d®¯superscript{ , v R ^ d with then Here the ¨‹…,‹…©‹…‹… ‹… , ‹… © is the usual “2subscript“2 _ 2 inner product for vectors in d²superscriptsuperscript² ^ d ^ ² . With this new norm, we can carry out the learning as usual in dsuperscript ^ d yet with a lower dimensional structure for •subscriptitalic-•{ _ • . We test our learning with the following parameters N=2020N=20N = 20, d²=2superscript²2d^{ ^ ² = 2 (hence d=Nd²=40superscript²40d=Nd^{ = N d ^ ² = 40), •(r)=r1italic-•1 ( r ) = r - 1, T=11T=1T = 1, Î”t=0.004Î”¡0.004 t=0.004roman_Î” t = 0.004, and M=500500M=500M = 500, and obtained the following comparison of the •italic-• instead of the high-dimensional { in figure 11. Our method can be also extended to special case of Stochastic Partial Differential Equation (SPDE) estimation. Consider the stochastic heat equation driven by an additive noise on a smooth bounded domain ±GŠ‚d±ºsuperscript{ G G Š‚ R ^ d , with initial condition ®(0,±)=0®0±0{ ( 0 , x ) = 0, zero boundary condition, and Î”Î” being the Laplace operator with zero boundary conditions in a suitable underlying Hilbert space H»HH, and where °°{ is a Gaussian noise, white in time and possible colored in space. The existence, uniqueness and other analytical properties of the solution ®®{ are well understood, and we refer to [21]. In this case there exists a complete orthonormal system {hk}k•Š‚Hsubscriptsubscript•» H{ h _ k } _ k N Š‚ H, such that hksubscripth_{k}h _ k is an eigenfunction of Î”Î” We denote by Î»ksubscript†- Î» _ k the corresponding eigenvalue, i.e. Î”hk=Î»jhkÎ”subscriptsubscript†subscript h_{k}=- h _ k = - Î» _ j h _ k . The noise term can be written, informally, as °(t,±)=k•qkhk(±)°k(t)°¡±subscript•subscriptsubscript±subscript°¡{ ( t , x ) = _ k N q _ k h _ k ( x ) w _ k ( t ), with qksubscriptq_{k}q _ k some positive scalars, and °k,k•subscript°•{ k _ k , k N, independent one dimensional Brownian motions. Assume that Îƒ and ƒ are some positive constant and we are interested in the estimation of parameter Îƒ Following the spectral approach surveyed in [5], we define the projection operator P:H†HN:ƒ†»superscript»P:H H^{N}P : H † H ^ N , where HN=span¡{h1,¦,hN}superscript»spansubscript1¦subscriptH^{N}= ^ N = roman_span { h _ 1 , ¦ , h _ N }. Then ®N=PN®=k=1N®k(t)hk(±)superscript®superscriptƒ®superscriptsubscript1subscript®¡subscript±{ ^ N = P ^ N u = _ k = 1 ^ N u _ k ( t ) h _ k ( x ) is the Fourier approximation of the solution ®®{ by the first NNN eigenmodes, that satisfies the following equation Since {hk(±)}k=1Nsuperscriptsubscriptsubscript±1 h _ k ( x ) } _ k = 1 ^ N are orthogonal to each other, we get that Then Îƒ can be estimated by 2 and we obtain the loss function Since this is a simple scalar quadratic optimization problem, the estimator Î^^ƒ Î can be calculated explicitly. Assuming that MMM trajectories of each eigenmode ®ksubscript®{ _ k can be observed, we derive that following estimator In the implementation, we focus on one dimensional stochastic heat equation, d=11d=1d = 1, and take the domain G=[0,]º0‹G=[0, = [ 0 , ]. In this case hk(x)=sin¡(kx)subscript¥¥h_{k}(x)= _ k ( x ) = roman_sin ( k x ) and Î»k=k2subscript†superscript2 _ k = k ^ 2 . The parameters are set as follows: T=11T=1T = 1, Î”t=0.01Î”¡0.01 t=0.01roman_Î” t = 0.01, ƒ=0.10.1 = 0.1, and qk=1subscript1q_{k}=1q _ k = 1, that corresponds to space-time white noise. We set the parameter of interest Î=2ƒ2 = 2. The estimation result is displayed in table 8 where we applied our estimation method to an SPDE with various numbers of modes N and trajectories M. The results shows that as NNN increases, the estimation of Î=2ƒ2 = 2 converges rapidly to the true value even with a small number of trajectories. The power of the proposed method lies in the fact that Îƒ can depend on spatial variable ±±{ and/or time. We explore next our method when Îƒ is a piecewise function, with initial and boundary conditions staying the same and where we assume Î1subscriptƒ1 _ 1 and Î2subscriptƒ2 _ 2 are unknown. With the same approach, we obtain Note that in contrast to previous (diagonalizable) case, each Fourier mode ®jsubscript®{ _ j is coupled with all other modes. Hence, we consider a Galerkin type projection, i.e. and Then the stochastic processes with dynamics becomes where ®^j,°^jsubscript^®subscript^° u _ j , over^ w _ j are approximations of true Fourier modes ®j,°jsubscript®subscript°{ _ j , w _ j for j••j N. Since We can define two matrices ©N(1),©N(2)NÃNsubscriptsuperscript©1subscriptsuperscript©2superscript{ N}B ^ ( 1 ) _ N , B ^ ( 2 ) _ N R ^ N Ã N , given by and with the vectors we can rewrite SDE system 20 into where ÎNsubscriptÎ _ N and Nsubscript{ _ N are diagonal matrices with diagonal entries being ÎN(i,i)=Î»isubscriptÎsubscript† _ N ( i , i ) = Î» _ i and N(i,i)=ƒqisubscriptsubscript{ q_{i}Q _ N ( i , i ) = ƒ q _ i for i=1,¦,N1¦i=1, = 1 , ¦ , N respectively. Notice now Î£N=N2subscriptÎ£superscriptsubscript2 _ N = Q _ N ^ 2 is non-singular and diagonal. In view of 2, we deduce the following loss function Define Then we have our estimator We have theoretical guarantee that the matrix (I11I12I21I22)matrixsubscript¼11subscript¼12subscript¼21subscript¼22 I_{21}&I_{22} start_ROW start_CELL I _ 11 end_CELL start_CELL I _ 12 end_CELL end_ROW start_ROW start_CELL I _ 21 end_CELL start_CELL I _ 22 end_CELL end_ROW ) is invertible, indicating the loss defined by equation 22 has a unique minimizer. The convergence theory is in our future work. However, for numerical experiments, we set the parameters as follows: T=11T=1T = 1, Ît=0.01¿¡0.01 t=0.01Î t = 0.01, ƒ=0.50.5 = 0.5, qk=1subscript1q_{k}=1q _ k = 1, Î»k=k24subscript†superscript24 _ k = / k ^ 2 4 and M=11M=1M = 1. We obtain the following results in table 9. The estimation of the piecewise function Îƒ converges fast to the true values, even with only one trajectory, as the number of modes increases to a moderate level. Furthermore, the L2superscript¿2L^{2}L ^ 2 error rapidly shrinks toward zero as the number of modes increases to a moderate level. We have demonstrated a novel learning methodology for inferring the drift term and diffusion coefficient in a general SDE system driven by Brownian noise. Our estimation approach, rooted in the statistical analysis of continuous time stochastic systems, does not assume a specific functional structure for the drift or diffusion term of SDE system, thereby enhancing its applicability across a diverse range of SDE models. This approach can efficiently handle high-dimensional SDE systems by leveraging deep learning and vectorization techniques. We estimate both the drift term and diffusion coefficient using a trajectory-based loss function, which is itself guided by noise. The loss function for the drift is derived from the negative logarithm of the ratio of likelihood functions, quantifying the probability ratios of observing two stochastic processes that originate from the same initial condition. For the diffusion coefficient, the loss function is based on the quadratic variation, which operates independently of the drift function. This independence makes our method particularly effective in scenarios where only trajectory observations are available, without prior knowledge of the drift or diffusion. Additionally, our approach is adaptable to various noise structures, including constant, non-diagonal, and state-dependent covariance matrices. The limitation and strength of our algorithm is caused by the introduction of the diffusion matrix Î£Î£ of the noise into the loss function. Although Î£Î£ is assumed to be invertible, the inversion of a possibly high-dimensional matrix at every epoch of training will cause significant delay in the computation. When Î£1superscriptÎ£1 ^ - 1 can be computed component-wise (or block-wise), then a parallel algorithm can be implemented to easily handle the high-dimensional observation data. We have shown a possible way to get around such limitation by using the special structure of the drift/noise term in section 3.7. This will be the next focus of our future research. Another possible direction is combining our learning of SDE with the learning of SPDEs, where the SPDEs are expressed as a systems of SDEs [5], as shown in section 3.8. ZG developed the algorithm, analyzed the data and implemented the software package. ZG develops the theory with IC and MZ. MZ designed the research. All authors wrote the manuscript. IC research was partially supported by NSF Grant DMS-2407549. MZ gratefully acknowledges funding provided by the Oak Ridge Associated Universities (ORAU) Ralph E Powe Junior Faculty Enhancement Award and NSF Grant CCF-AF-2225507222550722255072225507.",
        "keywords": ""
    },
    {
        "id": 23,
        "title": "IC/DC: Surpassing Heuristic Solvers in \nCombinatorial Optimization with Diffusion Models",
        "abstract": "AbstractRecent advancements in learning-based combinatorial optimization (CO) methods have shown promising results in solving NP-hard problems without the need for expert-crafted heuristics. However, high performance of these approaches often rely on problem-specific human-expertise-based search after generating candidate solutions, limiting their applicability to commonly solved CO problems such as Travelling Salesman Problem (TSP). In this paper, we present IC/DC, a CO framework that operates without any supervision. IC/DC is specialized in addressing problems involving two distinct sets of items, and it does not need problem-specific search processes to generate valid solutions. IC/DC employs a novel architecture capable of capturing the intricate relationships between items, and thereby enabling effective optimization in challenging CO scenarios. We train our model in a self-supervised way to minimize the cost of the solution while adhering to the problem-specific constraints. IC/DC not only achieves state-of-the-art performance compared to previous learning methods, but also surpasses well-known solvers and heuristic approaches on Asymmetric Traveling Salesman Problem (ATSP).",
        "corpus": "Recent advancements in learning-based combinatorial optimization (CO) methods have shown promising results in solving NP-hard problems without the need for expert-crafted heuristics. However, high performance of these approaches often rely on problem-specific human-expertise-based search after generating candidate solutions, limiting their applicability to commonly solved CO problems such as Travelling Salesman Problem (TSP). In this paper, we present IC/DC, a CO framework that operates without any supervision. IC/DC is specialized in addressing problems involving two distinct sets of items, and it does not need problem-specific search processes to generate valid solutions. IC/DC employs a novel architecture capable of capturing the intricate relationships between items, and thereby enabling effective optimization in challenging CO scenarios. We train our model in a self-supervised way to minimize the cost of the solution while adhering to the problem-specific constraints. IC/DC not only achieves state-of-the-art performance compared to previous learning methods, but also surpasses well-known solvers and heuristic approaches on Asymmetric Traveling Salesman Problem (ATSP). Combinatorial optimization (CO) aims to find the optimal solution that maximizes or minimizes an objective function from a large, discrete set of feasible solutions. This field has been extensively studied due to its broad industrial applications, including logistics, supply chain optimization, job allocation, and more (Zhang et al. 2023). Despite its significance, many CO problems are NP-complete, and developing efficient approximation algorithms is essential. Traditionally, approximation algorithms for CO have been developed using mathematical programming or hand-crafted heuristics (Miller, Tucker, and Zemlin 1960; Helsgaun 2023). However, the need for problem-specific expertise and the high computational demands of these methods has sparked increasing interest in applying deep learning techniques to CO problems. Early deep learning approaches framed CO problems as sequential decision-making tasks, generating solutions in an auto-regressive manner (Bello et al. 2016; Kool, Van Hoof, and Welling 2018). However, these methods were relatively limited in performance due to their inability to revise previously made decisions. In contrast, diffusion-based methods generate complete solutions in a single diffusion timestep and then iteratively refine them during the denoising process (Sun and Yang 2023; Min, Bai, and Gomes 2024). This iterative refinement enhances the overall quality of the solutions through corrections and adjustments. By allowing for the revision of earlier decisions, diffusion-based methods overcome the limitations of auto-regressive approaches and avoid the compounding errors typically associated with early decisions. Despite the impressive performance of diffusion-based methods, previously proposed algorithms had several significant drawbacks, such as the need for costly supervision (Sun and Yang 2023), or use of problem-specific objective that are applicable to other CO problems (Min, Bai, and Gomes 2024). Additionally, due to the challenges in imposing constraint within a diffusion model, previous studies relied on problem-specific search process to extract feasible solutions from the diffusion model™s generations. Designing these problem-specific search requires specialized knowledge and cannot be easily adapted to different CO problems. In this work, we propose a novel method for training a diffusion model in a self-supervised manner, eliminating the need for costly supervision, problem-specific objectives, or problem-specific search process. We demonstrate our approach on two distinct and challenging CO problems”the parallel machine scheduling problem (PMSP) and the asymmetric travelling salesman problem (ATSP)”which have received less attention (Kwon et al. 2021). Our method not only achieves state-of-the-art performance among deep learning approaches, but also surpasses well-known solvers and heuristic methods on ATSP instances by a significant margin. For widely-studied CO problems like the travelling salesman problem (TSP), several off-the-shelf solvers are available, such as CPLEX (IBM 2022) and OR-tools (Google 2024). These solvers are build on a variety of heuristics, incorporating search methods (Helsgaun 2023), mathematical programming (Arora 1996), and graph algorithms (Christofides 2022). Typically, these approaches rely on problem-specific, handcrafted techniques, which limits their flexibility in adapting to diverse variants encountered in real-world scenarios. To overcome this limitation, learning-based solvers have been developed, with early studies primarily focusing on auto-regressive approaches. Bello et al. (2016) were the first to propose solving CO problems using a pointer network trained via reinforcement learning (RL). Kwon et al. (2021) introduced an architecture called MatNet, which builds on the graph attention network (GAT) to encode various types of objects, enabling it to tackle more complex CO problems. While these auto-regressive models offer fast solution generation and can manage intricate CO problems, they are limited by their inability to revise previously made decisions and have been outperformed by methods utilizing diffusion-based methods. To the best of our knowledge, two notable works have effectively addressed CO problems using diffusion models, both achieving solution quality comparable to off-the-shelf solvers while significantly reducing generation time. DIFUSCO (Sun and Yang 2023) is a graph neural network (GNN)-based diffusion model trained in a supervised manner to replicate solutions generated by solvers. Similarly, UTSP (Min, Bai, and Gomes 2024) employs a GNN-based diffusion model trained in an unsupervised manner, eliminating the need for costly solution generation from traditional solvers. However, UTSP™s objective is based on the concept of the Hamiltonian cycle, limiting its applicability to TSP. However, these algorithms fall into the category of heatmap-generating approaches. While CO problems typically impose strict constraints on solutions, such as forming a Hamiltonian cycle, these algorithms are not trained to inherently satisfy those constraints. Instead, they rely on additional heatmap search techniques, such as active search methods (Qiu, Sun, and Yang 2022) or Monte Carlo Tree Search (MCTS) (Silver et al. 2016; Fu, Qiu, and Zha 2021), to produce feasible solutions. This reliance limits their applicability to CO problems with varying constraints on solutions. To combine the high-quality solutions of diffusion-based methods with the flexibility of auto-regressive approaches, we propose Improving Combinatorial optimization through Diffusion with Constraints (IC/DC). This approach ensures the feasibility of solutions while training diffusion model in a self-supervised manner, eliminating the need for costly supervision and problem-specific search processes. We consider a family of CO problems which involve two distinct sets of items. Each problem cc caligraphic_C is defined by two sets of items and ¬¬ and matrices that describe the relationships between these two sets of items, as illustrated on the left side of Figure 1. The solution to a CO problem ccc is represented by a binary matrix X³={0,1}||Ã|¬|‹³superscript01¬X caligraphic_X = { 0 , 1 } ^ | caligraphic_A | Ã | caligraphic_B | . Typically, for each problem ccc, there exists a feasible set of solutions, and a particular solution X‹XX is evaluated using a problem-specific scoring function score:³Ã†:score†³ : caligraphic_X Ã caligraphic_C † R when it is feasible. For clarity, we define the reward function R:³Ã†:…†³R: : caligraphic_X Ã caligraphic_C † R as follows: which allows us to express the objective of the CO problem ccc as maxX³¡R(X,c)subscript‹³…‹ _ X caligraphic_X R ( X , c ). We build upon a discrete diffusion model with categorical corruption processes (Austin et al. 2021). We represent the uncorrupted solution that we aim to generate as X0subscript‹0X_{0}X _ 0 , with the corrupted latent variables denoted as X1,¦,XTsubscript‹1¦subscript‹X_{1},...,X_{T}X _ 1 , ¦ , X _ T . We use lowercase x¥xx to represent the vectorized forms of X‹XX, where xt=vec¡(Xt){0,1}|||¬|subscript¥¡vecsubscript‹¡superscript01¬x_{t}= _ t = roman_vec ( X _ t ) { 0 , 1 } ^ | caligraphic_A | | caligraphic_B | , and tilded x~~¥ x to denote the one-hot encoded versions, x~t{0,1}|||¬|Ã2subscript~¥¡superscript01¬2 2}over~ x _ t { 0 , 1 } ^ | caligraphic_A | | caligraphic_B | Ã 2 . In line with diffusion model conventions, we use q(‹…)‹…q( ( ‹… ) to denote the data distribution/generative forward process, while pÎ(‹…)subscriptƒ‹…p_{ _ Î ( ‹… ) represents the denoising reverse process, which is learned to generate the solutions. Our forward process is defined as: where Q1:t=Q1Q2¦Qtsubscript:1¡subscript1subscript2¦subscript¡Q_{1:t}=Q_{1}Q_{2}...Q_{t}Q _ 1 : t = Q _ 1 Q _ 2 ¦ Q _ t , Š™direct-product denotes the element-wise multiplication, and vec1superscriptvec1 ^ - 1 reshapes the input to the shape ||Ã|¬|Ã2¬2| 2| caligraphic_A | Ã | caligraphic_B | Ã 2. The matrix Qt[0,1]2Ã2subscript¡superscript0122Q_{t} 2}Q _ t [ 0 , 1 ] ^ 2 Ã 2 is a noise transition matrix that independently applies noise to each element of the solution matrix. We design this noise transition matrix to align with the prior distribution of feasible solutions q¯¯ q in the limit (Vignac et al. 2022), such that limT†Q1:Tz=q¯subscript†subscript:1§¯ _ T † Q _ 1 : T z = over¯ q for any vector z§zz. This is achieved by defining: where 11 is vector of ones, and Î±tsubscript¼¡ _ t and Î²tsubscript½¡ _ t are scheduled appropriately with typical diffusion schedulers. The formulas for computing q¯¯ q for the CO problems demonstrated in the experiments are detailed in Appendix B.1. We follow the parametrization of Austin et al. (2021), where neural network fÎ(‹…)subscript“ƒ‹…f_{ _ Î ( ‹… ) is trained to directly predict logits of X0subscript‹0X_{0}X _ 0 from each Xtsubscript‹¡X_{t}X _ t , as follows: Although this parameterization facilitates the easy computation of diffusion loss when a target dataset is provided, samples from pÎsubscriptƒp_{ _ Î may not satisfy the constraints since each element of the solution matrix is independently sampled from a Categorical distribution. This limitation required the use of a feasibility-enforcing search process in previous diffusion-based studies on CO (Sun and Yang 2023; Min, Bai, and Gomes 2024). However, this approach requires a search process specifically tailored to each CO problem, and there is no assurance that the search process will preserve the quality of the solution that the diffusion model aims to generate. To ensure the generation of feasible solutions, we design a process inspired by auto-regressive methods, which we call the feasibility-enforced generation process. In this approach, we sample one element of the solution matrix at a time, ensuring its feasibility based on the previously sampled elements, as follows: This approach, similar to the flexibility of auto-regressive methods, allows for the straightforward enforcement of feasibility in the generated samples. However, p^Îsubscript^ƒ p _ Î cannot be directly utilized as the reverse process because it involves discrete sampling, which prevents the use of the reparameterization trick, thereby making conventional and efficient variational training methods inapplicable. When using pÎsubscriptƒp_{ _ Î as the reverse process, the connection between pÎsubscriptƒp_{ _ Î and p^Îsubscript^ƒ p _ Î becomes weak, leading to a lack of guarantee that samples from p^Îsubscript^ƒ p _ Î will retain the desired characteristics. To this end, we propose an iterative training approach that alternates between the CLONING step and the IMPROVEMENT step. In the CLONING step, we update the reverse process pÎsubscriptƒp_{ _ Î by maximizing the (lower bound of the) log likelihood of a set of high scoring feasible solutions: the surrogate targets. This guides pÎsubscriptƒp_{ _ Î toward generating high-quality feasible solutions, and strengthen its alignment with p^Îsubscript^ƒ p _ Î , as they become identical when pÎsubscriptƒp_{ _ Î generates feasible samples only. In the IMPROVEMENT step, we directly update p^Îsubscript^ƒ p _ Î using reinforcement learning to maximize the scores of generated solutions. These two steps work in tandem, the IMPROVEMENT step is direct but computationally intensive, and CLONING step is more efficient but is only an indirect method of improving samples from p^Îsubscript^ƒ p _ Î . In standard diffusion model training, the target distribution q(X0|c)conditionalsubscript‹0q(X_{0}|c)q ( X _ 0 | c ), which represents the distribution of optimal solutions given a problem ccc, is typically available. However, in CO problems, obtaining such supervised dataset is often prohibitively expensive. To address this, we propose training our diffusion model in a self-supervised manner using a surrogate target distribution q~(X0|c)~conditionalsubscript‹0 q ( X _ 0 | c ) instead. This surrogate distribution is progressively refined during training and is defined as a reward-weighted mixture of two distributions: where the mixture is controlled by the hyperparameter Î±[0,1]¼01 [ 0 , 1 ]. In the initial stage of training, the solutions generated by reverse process pÎ(X0|c)subscriptƒconditionalsubscript‹0p_{ _ Î ( X _ 0 | c ) are not feasible in general. This leads to using more samples from the prior distribution of feasible solutions q(X0)subscript‹0q(X_{0})q ( X _ 0 ), and guide the diffusion model to more generate feasible solutions. As training progresses and as pÎ(X0|c)subscriptƒconditionalsubscript‹0p_{ _ Î ( X _ 0 | c ) begins to generate feasible solutions, the reward-weighting allows the diffusion model to refine itself by focusing on its high-scoring, feasible generations. Meanwhile, the inclusion of prior distribution of feasible solutions q(X0)subscript‹0q(X_{0})q ( X _ 0 ) introduces diversity to the training, counteracting the tendency of pÎsubscriptƒp_{ _ Î to become too narrow as training progresses. With the surrogate target distribution defined above, we perform standard diffusion training by minimizing the KL-divergence between the model™s generative distribution and the surrogate target distribution: which results in a variational bound objective (see Appendix A.2 for detailed derivations), VB(Î):=assignsubscriptVBƒabsent _ VB ( Î ) := Inspired by recent practices (Austin et al. 2021), we also incorporate the following auxiliary losses: where prdsubscriptprd _ prd encourages accurate predictions of the data X0subscript‹0X_{0}X _ 0 at each time step, and cstsubscriptcst _ cst discourages infeasible predictions, with C(‹…)¶‹…C( ( ‹… ) being a differentiable function that approximately measures constraint violations of samples using the Gumbel-softmax trick (see Appendix. A.3 for details on C¶CC). In summary, during the CLONING step, we minimize: To directly improve the feasibility-enforced generations, we minimize the following objective: Similar to auto-regressive methods, the feasibility-enforced generation process can be viewed as a sequential decision making task, where each element of the solution matrix X0subscript‹0X_{0}X _ 0 is determined step by step. This perspective allows us to compute the gradient of the above objective using the REINFORCE algorithm (Williams 1992). After sampling a set of solutions {X0(1),X0(2),¦,X0(N)}superscriptsubscript‹01superscriptsubscript‹02¦superscriptsubscript‹0 X _ 0 ^ ( 1 ) , X _ 0 ^ ( 2 ) , ¦ , X _ 0 ^ ( N ) } using p^Î(X0|c)subscript^ƒconditionalsubscript‹0 p _ Î ( X _ 0 | c ), we approximate the gradient as follows: ÎIMP(Î)subscriptƒsubscriptIMPƒabsent _ Î caligraphic_L _ IMP ( Î ) where R(i)=R(X0(i),c)superscript……superscriptsubscript‹0R^{(i)}=R(X_{0}^{(i)},c)R ^ ( i ) = R ( X _ 0 ^ ( i ) , c ). This approach is adapted from the baseline estimation method of POMO (Kwon et al. 2020). We train IC/DC by alternating between the CLONING step“diffusion model training with surrogate targets“and the IMPROVEMENT step“reinforcement learning of feasibility-enforced generation, as shown in Algorithm 1. In practice, we use a replay memory q~subscript~ _ over~ q to implement a surrogate target distribution, and perform multiple CLONING steps for each IMPROVEMENT step. This is because CLONING updates only a single timestep of the diffusion model, and IMPROVEMENT is significantly slower due to the online generations. Input: A set of CO problems learning late Î³¾ diffusion step TTT, target mix ratio Î±¼ The neural network we need is f:³Ã†³:“†³³f: : caligraphic_X Ã caligraphic_C † caligraphic_X, which encodes the CO problem and outputs a distribution over binary matrices given an input binary matrix. To achieve this, we propose a problem encoder that effectively encodes CO problem, and a denoiser, a specialized variant of GNN that processes the bipartite graph between two sets of items. For the CO problems we consider, a problem instance ccc consists of information about two sets of items and their relationships. For simplicity, let™s assume that all items share the same number of features ddd; if not, different embedding layers can be used to standardize the feature dimensions. The information for the items in set is represented by the matrix A||ÃdsuperscriptA d}A R ^ | caligraphic_A | Ã d , and similarly, the items in set ¬¬ are represented by the matrix B|¬|Ãdµsuperscript¬B d}B R ^ | caligraphic_B | Ã d . There relationship between these items is captured by the matrix D||Ã|¬|·superscript¬D R ^ | caligraphic_A | Ã | caligraphic_B | . Together, these matrices define the problem instance, i.e., c=(A,B,D)µ·c=(A,B,D)c = ( A , B , D ). To effectively encode the problem represented by these matrices, we adopt the dual graph attentional layer structure from MatNet (Kwon et al. 2021), but replace the attention layer with a modified version of graph attention networks (GAT, VeliÄkoviÄ et al. 2017) that is specifically designed to process a bipartite graph. The problem encoder consists of L¿LL layers, where each layer takes (A,B,D)µ·(A,B,D)( A , B , D ) as input and outputs updated features (A²,B²)superscript²superscriptµ²(A^{ A ^ ² , B ^ ² ). The outputs of the final layer are then passed to the denoiser. The bottom-right side of Figure 2 illustrates this problem encoder. Detailed equations of problem encoder layer are provided in Appendix B.2. Building on recent empirical successes (Joshi et al. 2020; Qiu, Sun, and Yang 2022), we extend the anisotropic graph neural network (AGNN) to handle bipartite graphs, allowing us to consider two distinct sets of items, and use it as the denoiser. The input embedding layer maps each element of the noisy solution matrix Xtsubscript‹¡X_{t}X _ t and the timestep t¡tt into ddd-dimensional features. These embeddings are then passed to the AGNN, along with the problem embedding A²superscript²A^{ ^ ² and B²superscriptµ²B^{ ^ ² . After L²superscript¿²L^{ ^ ² layers of AGNN, the embedded solution matrix with updated features is passed through a linear layer to produce the denoised solution matrix X0subscript‹0X_{0}X _ 0 . The bottom-left side of Figure 2 illustrates this denoiser. Detailed equations of denoiser layer are provided in Appendix B.3. We begin by describing the combinatorial optimization (CO) problems on which we conducted experiments: the Parallel Machine Scheduling Problem (PMSP) and the Asymmetric Travelling Salesman Problem (ATSP). In PMSP, a problem instance ccc consists of |¥|¥| caligraphic_J | jobs and |³|³| caligraphic_M | machines. Each job j¥¥j caligraphic_J must be scheduled on a machine m³³m caligraphic_M, with varying workloads for each job and different processing capabilities for each machine. The primary objective in PMSP is to minimize the makespan, which is the total length of the schedule upon the completion of all jobs. In this context, having [X0]j,m=1subscriptdelimited-[]subscript‹01[X_{0}]_{j,m}=1[ X _ 0 ] _ j , m = 1 indicates that job jjj is assigned to machine mmm, which takes a processing time of [P]j,msubscriptdelimited-[]ƒ[P]_{j,m}[ P ] _ j , m where P+|¥|Ã|³|ƒsuperscriptsubscript¥³P R _ + ^ | caligraphic_J | Ã | caligraphic_M | is a matrix of processing times for all combinations. The goal is to determine the solution matrix X0={0,1}|¥|Ã|³|subscript‹0superscript01¥³X_{0}= _ 0 = { 0 , 1 } ^ | caligraphic_J | Ã | caligraphic_M | that minimizes the makespan for a given problem c=(³,¥,P)³¥ƒc=( = ( caligraphic_M , caligraphic_J , P ): The solution matrix that assigns a job to multiple machines is considered infeasible. An ATSP instance c=(|©|,D)©·c=(| = ( | caligraphic_N | , D ) comprises |©|©| caligraphic_N | cities and an asymmetric distance matrix D+|©|Ã|©|·superscriptsubscript©©D R _ + ^ | caligraphic_N | Ã | caligraphic_N | where each element of it specifies the distance between two cities. The solution to ATSP is a tour, which is an adjacency matrix X0={0,1}|©|Ã|©|subscript‹0superscript01©©X_{0}= _ 0 = { 0 , 1 } ^ | caligraphic_N | Ã | caligraphic_N | for a directed graph visiting all cities once. The goal is find a solution that minimizes the tour length: The solution matrix that is not a Hamiltonian cycle is considered infeasible. In accordance with Kwon et al. (2021) we employ tmat-class ATSP instances (see Appendix D.1). We evaluated the baselines and the proposed algorithm using two Intel Xeon Gold 6330 CPUs and an RTX 3090 GPU for both PMSP and ATSP. For the evaluation, 1000 problem instances were randomly generated using a standard generation process (see appendix D.1). To fully leverage the stochastic nature of generative learning-based methods, we also evaluated these methods by generating multiple samples (Ãnabsent nÃ n) for each problem instance and selecting the one with the best score. For MatNet (Kwon et al. 2021), we followed the authors™ implementation, including instance augmentation, which yielded better result. As a problem-specific search process has not been studied on PMSP and ATSP, for diffusion-based methods, we report simple discrete diffusion models, either trained with supervised learning (Sun and Yang 2023) or with reinforcement learning (Black et al. 2023). For further experimental details, please refer to Appendix C and Appendix D. We evaluated our method on PMSP-20 and PMSP-50, where the numbers 20 and 50 correspond to the number of jobs in each instance, with the number of machines fixed at 4. Our approach is compared against various baselines, including (meta-)heuristics, auto-regressive, and diffusion-based methods. Details of these baselines can be found in Appendix C.2. As shown in Table 1, IC/DC achieves the smallest optimality gap among learning-based methods. IC/DC demonstrates a 0.142%percent0.1420.142 % performance gap compared to CP-SAT, whereas the previous SOTA, MatNet, shows a 0.615%percent0.6150.615 % gap on PMSP-20. On PSMP-50 IC/DC reduces the gap from MaNet™s 0.182%percent0.1820.182 % to 0.112%percent0.1120.112 %. Although IC/DC has a slower inference speed compared to auto-regressive methods, its high-quality solutions remain highly competitive with other baselines. We evaluated our method on ATSP-20 and ATSP-50, where the number 20 and 50 correspond to the number of cities. IC/DC generates solutions that not only surpass those generated by SOTA learning-based methods but also outperform those obtained through (meta-)heuristics and off-the-shelf solvers. IC/DC achieves a 0.235%percent0.235-0.235 0.235 % gap on ATSP-20 and a 0.532%percent0.532-0.532 0.532 % gap on ATSP-50, demonstrating a negative gap compared to a powerful CPLEX solver”which no other learning-based method has achieved. This substantial improvement positions IC/DC as a new SOTA method for addressing these less-studied CO problems, highlighting its potential. As discussed earlier, the performance of auto-regressive models is limited by their sequential decision-making process, which locks in previously made decisions, preventing any revisions. While IC/DC employs a similar auto-regressive approach for feasibility-enforced generation, it also benefits from the accuracy of diffusion-based methods by iteratively refining the solution through T11T-1T - 1 denoising steps before the final auto-regressive generation. Moreover, this two-stage generation process enables IC/DC to achieve much greater sample diversity compared to auto-regressive models, as the denoising process provides varied foundations for solutions before the auto-regressive generation. The results that support these arguments can be found in both tables, particularly in Table 2. While IC/DC™s performance lags behind MatNet when using a single generation to solve the problem, its quickly surpasses MatNet as the number of samples increases, highlighting the high diversity of IC/DC™s samples. Upon closer examination, we observed that the stochasticity of MatNet primarily stems from the starting point of the auto-regressive generation (i.e., the selection of the initial city to begin the tour). In contrast, IC/DC™s denoising process offers diverse backbones for the auto-regressive generation, resulting in varied samples even when starting from the same initial city. Previously proposed diffusion-based methods (Qiu, Sun, and Yang 2022; Sun and Yang 2023; Min, Bai, and Gomes 2024) have employed problem-specific search processes to enforce feasibility and enhance performance. However, developing such search processes with strong performance and feasibility guarantees typically requires significant expertise in the specific CO problem being addressed. As the complexity of CO problem increases, such as with ATSP or more complex TSP variants compared to TSP, the cost of implementing these search algorithms becomes even greater. As shown in Table 1 and Table 2, simple implementations of diffusion models, whether trained with supervised learning (Sun and Yang 2023) or reinforcement learning (Black et al. 2023), struggle to generate feasible solutions without the aid of problem-specific search processes, which have not been studied for these particular CO problems. This highlights the critical importance of our feasibility-enforced generation process, which integrates the flexibility of auto-regressive methods into diffusion-based methods, allowing for the straightforward imposition of various constraints. In this regard, the proposed IC/DC approach not only demonstrates superior performance compared to previous learning-based methods, but also significantly broadens the applicability of diffusion-based methods to a wide range of less-explored CO problems with diverse constraints. We propose a diffusion-based approach named IC/DC to tackle the CO problems involving two distinct sets of items. Our algorithm demonstrates superior performance in both PMSP and ATSP, outperforming existing baselines and successfully generating feasible solutions that cannot be easily achieved with simple diffusion-based methods. We emphasize that this is the first study to successfully adapt a diffusion-based method for CO problems involving two distinct sets of items. We believe the proposed IC/DC framework has significant potential for generlizability. For example, in Appendix D.4, we demonstrate its capability to address real-world CO problems with sophisticated item and relationship features. Despite its strong performance, IC/DC has a notable limitation: the GAT-based encoder we use requires O(max(||,|¬|)2)O( ( roman_max ( | caligraphic_A | , | caligraphic_B | ) ^ 2 ) memory complexity. This demands substantial memory resources when training on large instances. To address these challenges, we plan to explore memory-efficient techniques in future work, such as those introduced by Zhu et al. (2024). We derive the upper bound using the Evidence Upper Bound (EUBO) (Ji and Shen 2019) The inequality is established by the Gibbs™ inequality: «q~(Z|X,c)log¡q~(Z|X,c)¤«q~(Z|X,c)log¡pÎ(Z|X,c)~conditional‹~conditional‹~conditional‹subscriptƒconditional‹- p_{% « over~ q ( Z | X , c ) roman_log over~ q ( Z | X , c ) ¤ - « over~ q ( Z | X , c ) roman_log p _ Î ( Z | X , c ). The last inequality is derived from «q~(X|c)log¡pÎ(X|c)X=«q~(X,Z|c)log¡pÎ(X,Z|c)ZX~conditional‹subscriptƒconditional‹differential-d‹~‹conditionalsubscriptƒ‹conditionaldifferential-ddifferential-d‹ p_{ p_{ ,Z|c)dZdX« over~ q ( X | c ) roman_log p _ Î ( X | c ) d X = « over~ q ( X , Z | c ) roman_log p _ Î ( X , Z | c ) d Z d X. We derive the reward-weighted diffusion loss from the objective in CLONING step. We aim to minimize the Kullback-Leibler (KL) divergence DKL(q~(X|c)¥pÎ(X|c))D_{KL}( p_{ _ K L ( over~ q ( X | c ) ¥ p _ Î ( X | c ) ) between the surrogate target and the distribution ppp parameterized by Îƒ however since evaluating the log-likelihood of the generative model pÎ(X|c)subscriptƒconditional‹p_{ _ Î ( X | c ) is difficult (Kingma and Welling 2013), we instead utilize a joint variational upper bound (see App. A.1): According to Gibbs™ inequality if the right-hand side of the inequality is zero, the inequality becomes an equality. In this case, the surrogate target is exactly approximated. Therefore, we focus on minimizing the right-hand side of eq (A.2). By applying the diffusion process where Z=X1:Tsubscript‹:1Z=X_{1:T}Z = X _ 1 : T , X=X0‹subscript‹0X=X_{0}X = X _ 0 , and TTT is the diffusion step, the forward process is defined by q(X1:T|X0)conditionalsubscript‹:1subscript‹0q(X_{1:T}|X_{0})q ( X _ 1 : T | X _ 0 ) with X0¼q~(X0|c)similar-tosubscript‹0~conditionalsubscript‹0X_{0} _ 0 ¼ over~ q ( X _ 0 | c ). We minimize the Reward-weighted Diffusion Loss RWDsubscriptRWD _ RWD : If TTT is sufficiently large, LTsubscript¿L_{T}L _ T will approach zero (Austin et al. 2021). Also, L0subscript¿0L_{0}L _ 0 can be derived as CE loss: Cross-entropy loss is equal to prdsubscriptprd _ prd at t=1¡1t=1t = 1. As H(p~)»~H( ( over~ p ) is independent to pÎsubscriptƒp_{ _ Î , it can be ignored for training. As fÎsubscript“ƒf_{ _ Î predicts the logit of X0subscript‹0X_{0}X _ 0 , we use the Cross-entropy loss for every timestep Lpredsubscript¿predL_{ _ pred instead of Cross-entropy loss at t=1¡1t=1t = 1. Training with this loss function results in an enhancement of solution quality. In CO problems, solutions must strictly satisfy specific constraints. To discourages infeasible predictions X0¼pÎ(X0|Xt,c)similar-tosubscript‹0subscriptƒconditionalsubscript‹0subscript‹¡X_{0} p_{ _ 0 ¼ p _ Î ( X _ 0 | X _ t , c ), we introduce a constraint loss: where C(‹…)¶‹…C( ( ‹… ) is a differentiable function that approximately measures constraint violations of samples In the Asymmetric Travelling Salesman Problem (ATSP), each node is required to travel to a distinct city other than itself. Consequently, in the resulting solution matrix, each row and each column must contain exactly one entry of ™1™: In the Parallel Machine Scheduling Problem (PMSP), each job is required to be assigned to a single machine. Accordingly, in the solution matrix, each column must contain exactly one entry of ™1™, with all other entries in that column being ™0™: where Gumbel-Softmax(x)i=exp¡((xi+gi)/)j=12exp¡((xj+gj)/)Gumbel-Softmaxsubscript¥subscript¥subscript”superscriptsubscript12subscript¥subscript” ( x ) _ i = / roman_exp ( ( x _ i + g _ i ) / ) _ j = 1 ^ 2 roman_exp ( ( x _ j + g _ j ) / ) , and gisubscript”g_{i}g _ i is a value sampled from the Gumbel(0,1) distribution, and is the temperature parameter. By combining reward-weighted diffusion loss A.2 and constraint loss A.3, our diffusion objective: where Î»1subscript†1 _ 1 and Î»2subscript†2 _ 2 are hyper parameter. According to (Austin et al. 2021), it is argued that incorporating domain-specific structures into the transition matrices Qtsubscript¡Q_{t}Q _ t within the diffusion process is a reasonable approach. In our case, due to the inherent sparsity in the solution matrices of CO problems, the marginal distribution of feasible solutions significantly deviates from the uniform distribution commonly used in standard diffusion processes. Therefore, we design this noise transition matrix to align with the prior distribution of feasible solutions q¯¯ q . Depending on what CO problem we are aiming to solve, we are often able to compute the prior distribution, averaged over solution elements q¯(x~)=q¯(1|||¬|i=1|||¬|[x~0]i)¯~¥¯1¬superscriptsubscript1¬subscriptdelimited-[]subscript~¥0 ^{| q ( over~ x ) = over¯ q ( / 1 | caligraphic_A | | caligraphic_B | _ i = 1 ^ | caligraphic_A | | caligraphic_B | [ over~ x _ 0 ] _ i ). The marginal probability of x=1¥1x=1x = 1 over a set of feasible solutions can be expressed as q¯(x=1)¯¥1 q ( x = 1 ). In ATSP, when considering |©|©| caligraphic_N | cities, the solution involves travelling through all |©|©| caligraphic_N | cities. The prior distribution follows the following: In PMSP, when considering |¥|¥| caligraphic_J | jobs and |³|³| caligraphic_M | machines, the solution involves assigning all |¥|¥| caligraphic_J | jobs to the machines. The prior distribution is as follows: In most combinatorial optimization problems, the marginal distribution q¯¯ q is typically known. However, in cases where the marginal distribution is not available, an alternative approach is to utilize a uniform distribution. As a substitute for q¯¯ q , one may consider using u¯=[0.5,0.5]¯0.50.5 u = [ 0.5 , 0.5 ]. Denoting each row of AAA as A=[a1,¦,a||]Š¤superscriptsubscript1¦subscripttopA=[a_{1},...,a_{| = [ a _ 1 , ¦ , a _ | caligraphic_A | ] ^ Š¤ , an attention block within each layer processes the input as: where WintersubscriptŠinterW_{ _ inter , WintrasubscriptŠintraW_{ _ intra , and WvsubscriptŠvW_{ _ v are weight matrices of dimension dÃdsuperscript d}R ^ d Ã d , and MLP(‹…)MLP‹… ( ‹… ) is a fully-connected neural network that maps 2222-dimensional inputs to 1111-dimensional outputs. The matrices Sintersubscript†interS_{ _ inter and Sintrasubscript†intraS_{ _ intra are designed to capture the inter-relationships within set and the intra-relationships and between sets and ¬¬ These matrices, along with D·DD, are combined to form the attention score D~~· D , which produces the output A~~ A . Using the described attention block, the layer outputs A²superscript²A^{ ^ ² as follows: where BN refers to a batch normalization (Ioffe and Szegedy 2015). The process for updating BµBB to B²superscriptµ²B^{ ^ ² is computed in the same way. As illustrated in the bottom-left side of Figure 2, the denoiser consists of L²superscript¿²L^{ ^ ² layers, where each layer gets input of (A,B,X,t)µ‹¡(A,B,X,t)( A , B , X , t ) and outputs (A²,B²,X²)superscript²superscriptµ²superscript‹²(A^{ A ^ ² , B ^ ² , X ^ ² ), which is processed as follows: Where hi“=0=hiLsubscriptsuperscript“0subscriptsuperscript¿h^{ ^ roman_“ = 0 _ i = h ^ L _ i and hj“=0=hjLsubscriptsuperscript“0subscriptsuperscript¿h^{ ^ roman_“ = 0 _ j = h ^ L _ j . For simplicity, the vector representation xi,j“=0subscriptsuperscript¥“0x^{ ^ roman_“ = 0 _ i , j at the (i,j)(i,j)( i , j )-th Xtsubscript‹¡X_{t}X _ t is denoted without t¡tt. The matrices Ua“,Ub“,Va“,Vb“,P“,Q“,R“dÃdsubscriptsuperscript“subscriptsuperscript“subscriptsuperscript“subscriptsuperscript“superscriptƒ“superscript“superscript…“superscriptU^{ d}U ^ roman_“ _ a , U ^ roman_“ _ b , V ^ roman_“ _ a , V ^ roman_“ _ b , P ^ roman_“ , Q ^ roman_“ , R ^ roman_“ R ^ d Ã d are learnable parameters of the ““ layer. SUM pooling is denoted by (Xu et al. 2018), the sigmoid function is represented by ƒ and the Hadamard product is denoted by Š™direct-product ©isubscript© _ i denotes the neighborhood of node iii among the BµBB items, while ©jsubscript© _ j denotes the neighborhood of node jjj among the AAA items. Additionally, the variable •¥•¥ denotes the sinusoidal features (Vaswani et al. 2017) corresponding to the denoising timestep t¡tt. After the final layer ”” the output xi,j”subscriptsuperscript¥”x^{ ^ fraktur_L _ i , j is passed through a linear layer to obtain clean matrix X0={x0,0,¦,xI,J}subscript‹0subscript¥00¦subscript¥¼½X_{0}= _ 0 = { x _ 0 , 0 , ¦ , x _ I , J }. In the literature on the parallel machine scheduling problem, most studies focus on a simplified variant where the processing time for a job is consistent across all machines, commonly referred to as uniform parallel machines. In contrast, we examine a more complex scenario where the processing times for each machine are entirely independent to one another. For our study, we generate the processing time matrix randomly and use it as our instance. Constraint Programming with Satisfiability (CP-SAT) is a highly efficient solver developed as part of the OR-Tools suite, designed for solving integer programming problems. It is particularly effective for solving scheduling problems, including PMSP. For PMSP our MIP model is based on Avalos-Rosales, Alvarez, and Angel-Bello (2013) Constraints (9) and (10) ensure that each job has exactly one predecessor on one of the machines. Constraint(12) specifies that if a job has a predecessor on a machine, it must also have a successor on that same machine. Constraint (13) guarantees that a valid sequence of jobs is scheduled on each machine, with no overlap in processing times. Constraint (8) ensures that only one job can be scheduled as the first job on each machine. Constraint (7) sets the completion time of job 0, an auxiliary job used to define the start of the schedule, to zero. Finally, constraint (11) establishes the relationship between the makespan of individual machines and the overall schedule makespan. CP-SAT is run on CPUs. Random and Shortest Job First (SJF) are greedy-selection algorithms designed to generate valid schedules using the Gantt chart completion strategy. SJF specifically prioritizes tasks by scheduling the shortest available jobs in ascending order at each time step t¡tt. Although simple, these methods can serve as effective baselines, providing a comparison point for evaluating more sophisticated scheduling algorithms. The greedy-selection algorithms run on CPUs. CO problems often require sophisticated methods to find high-quality solutions, particularly when traditional approaches like Mixed Integer Programming (MIP) are impractical due to complexity. In such cases, meta-heuristics provide a robust alternative. Genetic Algorithm (GA) and Particle Swarm Optimization (PSO) are two widely adopted meta-heuristics, known for their versatility and effectiveness across a range of problem domains. GA iteratively updates multiple candidate solutions, referred to as chromosomes. New child chromosomes are produced by combining two parent chromosomes through crossover methods, and mutations are applied to the chromosomes to enhance exploration. PSO iteratively updates multiple candidate solutions, referred to as particles. For every iteration each particles are updated based on the local best known and the global best known particles. We utilize the implementations provided by Kwon et al. (Kwon et al. 2021) and follow their setting. Both GA and PSO run on GPUs. MatNet, proposed by (Kwon et al. 2021), adapts the attention model (Kool, Van Hoof, and Welling 2018) to be applicable to bipartite graphs. It employs cross-attention mechanisms in place of self-attention to facilitate message passing between the two set of nodes, thereby encoding relationship information more effectively. The model is trained using reinforcement learning. Solution are generated in an auto-regressive manner, where each node is sequentially selected based on the current state of the solution. We trained the diffusion model to estimate solution matrices using a supervised learning approach, similar to Difusco (Sun and Yang 2023). This method adapts graph-based denoising diffusion models to more naturally formulate combinatorial optimization problems and generate high-quality solutions. By explicitly modeling the node and edge selection process through corresponding random variables, the model effectively captures the problem™s structure. The model is trained through supervised learning. Similar to DDPO, as proposed by (Black et al. 2023), which demonstrates that framing the denoising process as a multi-step decision-making problem enables policy gradient algorithms to directly optimize diffusion models for downstream objectives, we trained our diffusion model within the reinforcement learning framework. We used the REINFORCE algorithm (Williams 1992) to map the denoising process to the MDP framework. Following the implementation of (Kwon et al. 2021), we utilize 25 chromosomes with a mutation rate and crossover ratio both set to 0.3. Among the 25 initial chromosomes, one is initialized with the solution from the SJF heuristic. The best-performing chromosome is retained across all iterations. We run 1000 iterations per instance. Following the implementation of (Kwon et al. 2021), we utilize 25 particles with an inertial weight of 0.7. Both the cognitive and social constants are set to 1.5. Additionally, one particle is initialized with the solution from the SJF heuristic. We run 1000 iterations per instance. We use the same hyperparameters reported in (Kwon et al. 2021), with the exception that the number of stages is set to 1 instead of 3. Using CP-SAT, we generate 128,000 training samples for supervised learning. Since the solution matrices for PMSP are not square matrices, we use IC/DC™s encoder in combination with Difusco™s decoder, employing 12 layers for both encoder and the decoder. The denoising timestep is set to 1000 during training but reduced to 20 for faster inference. All other hyperparameters, follow those reported in (Sun and Yang 2023). We train our diffusion model with the objective function Î¥DDRL=”¼[t=0TÎlog¡pÎ(Xt1|Xt,c)r(X0,c)]subscriptƒsubscript¥DDRL”¼delimited-[]superscriptsubscript¡0subscriptƒsubscriptƒconditionalsubscript‹¡1subscript‹¡subscript‹0 { p_{ _ Î caligraphic_J _ DDRL = E [ _ t = 0 ^ T _ Î roman_log p _ Î ( X _ t - 1 | X _ t , c ) r ( X _ 0 , c ) ] proposed by (Black et al. 2023). We use the same encoder and decoder architecture as IC/DC, consisting of 5 encoder layers and 2 decoder layers. Apart from setting the denoising timestep to 20, all other hyperparameters are consistent with those used in IC/DC (ours). For PMSP-20 instances, we use 3 layers for both the encoder and the decoder, with the denoising timestep set to 10. For PMSP-50 instances, we utilize 5 encoder layers and 3 decoder layers, with the denoising timestep set to 15. The hyperparameters lambda1™subscript1lambda_{1}l a m b d a _ 1 and Î»2subscript†2 _ 2 are set to 1e-3 and 1e-6, respectively. The IMPROVEMENT step is executed every 30 epochs. Our model is trained using the Adam optimizer, with a batch size of 512 and a learning rate of 4e-4. Our code is available at: While random distance matrices can be generated by choosing random integers, such matrices lack meaningful correlations between distances and doesn™t reflect practical scenarios. Instead we are interested in problems which ATSP instances have the triangle inequality so called Tmat class (Kwon et al. 2021; Cirasella et al. 2001). That is, for a distance matrix D·DD with elements dijsubscriptd_{ij}d _ i j representing the distance between city cisubscriptc_{i}c _ i and cjsubscriptc_{j}c _ j , if d(ci,cj)¥d(ci,ck)+d(ck,cj)subscriptsubscriptsubscriptsubscriptsubscriptsubscriptd(c_{i},c_{j}) d(c_{i},c_{k})+d(c_{k},c_{j})d ( c _ i , c _ j ) ¥ d ( c _ i , c _ k ) + d ( c _ k , c _ j ) then we set d(ci,cj)=d(ci,cj)+d(ck,cj)subscriptsubscriptsubscriptsubscriptsubscriptsubscriptd(c_{i},c_{j})=d(c_{i},c_{j})+d(c_{k},c_{j})d ( c _ i , c _ j ) = d ( c _ i , c _ j ) + d ( c _ k , c _ j ), while diagonal elements are maintained as d(ci,ci)=0subscriptsubscript0d(c_{i},c_{i})=0d ( c _ i , c _ i ) = 0. We repeat this procedure until no more changes can be made. Mixed integer programming (MIP) is an optimization technique used to solve problems where some of the variables are required to be integers while others can be continuous. Solution methods such as branch and bound, branch and cut etc. are used to solve these kind of problems. To we use CPLEX (IBM 2022; Bliek1Ãº, Bonami, and Lodi 2014), one of the popular commercial optimization software use by the OR community and solve our test instances through benders decomposition (Rahmaniani et al. 2017). The MIP model serves as the mathematical representation of the problem. For ATSP, our MIP model is based on the formulation presented by Miller, Tucker, and Zemlin (1960). The constraints (15) and (16) ensure that each city is visited exactly once. Constraint (17) prevents subtours, ensuring that all cities are included in a single tours of length nnn. As the name suggests, Nearest Neighbor (NN), Nearest Insertion (NI), and Furthest Insertion (FI) are straightforward simple greedy-selection algorithms frequently used as baselines for TSP algorithms. We use the implementations provided by (Kwon et al. 2021) which are implemented in C++. LKH3 is a widely recognized state-of-the-art algorithm for addressing constrained TSP and Vehicle Routing Problems (VRP). It employs a local search approach utilizing kkk-opt operations to enhance its solutions. For solving the ATSP instances, we utilize version 3.0.6. We utilize the checkpoints provided by. (Kwon et al. 2021) and evaluate them on the same problem instances. Using LKH-3, we generate 128,000 training samples for supervised learning. We use Difusco™s encoder and decoder, employing 12 layers for both encoder and the decoder. The denoising timestep is set to 1000 during training but reduced to 20 for faster inference. All other hyperparameters, follow those reported in (Sun and Yang 2023). We train our diffusion model with the objective function Î¥DDRL=”¼[t=0TÎlog¡pÎ(Xt1|Xt,c)r(X0,c)]subscriptƒsubscript¥DDRL”¼delimited-[]superscriptsubscript¡0subscriptƒsubscriptƒconditionalsubscript‹¡1subscript‹¡subscript‹0 { p_{ _ Î caligraphic_J _ DDRL = E [ _ t = 0 ^ T _ Î roman_log p _ Î ( X _ t - 1 | X _ t , c ) r ( X _ 0 , c ) ] proposed by (Black et al. 2023). We use the same encoder and decoder architecture as IC/DC, consisting of 5 encoder layers and 2 decoder layers. Apart from setting the denoising timestep to 20, all other hyperparameters are consistent with those used in IC/DC. For ATSP-20 instances, we use 3 layers for both the encoder and the decoder, with the denoising timestep set to 10. For ATSP-50 instances, we utilize 5 encoder layers and 3 decoder layers, with the denoising timestep set to 15. The hyperparameters Î»1subscript†1 _ 1 and Î»2subscript†2 _ 2 are set to 1e-3 and 1e-6, respectively. The IMPROVEMENT step is executed every 30 epochs. Our model is trained using the Adam optimizer, with a batch size of 256 for 20 node instances and 64 for 50 node instances and a learning rate of 4e-4. In real-world combinatorial optimization (CO) problems, direct problem data, such as the distance matrix in ATSP or the processing time matrix in PMSP, may not always be explicitly provided. In such cases, solving the problem may require processing a broader range of data. For example, consider a navigation problem (NP) similar to ATSP, where the objective is to minimize the total travel time rather than distance. In this scenario, various types of information that influence travel time must be considered, including the coordinates of each city, time-per-distance data representing the relationships between cities, and traffic information. We define the actual travel time between |©|©| caligraphic_N | cities as follows: where, for each i,j©©i,j , j caligraphic_N, Ta+|©|Ã|©|superscriptsubscriptsuperscript©©T^{a} ^ a R ^ | caligraphic_N | Ã | caligraphic_N | _ + represents the actual travel time matrix, R+|©|Ã2…subscriptsuperscript©2R 2}_{+}R R ^ | caligraphic_N | Ã 2 _ + is the coordinate matrix of all cities, S+|©|Ã|©|†subscriptsuperscript©©S R ^ | caligraphic_N | Ã | caligraphic_N | _ + is the reciprocal speed matrix, and F|©|Ã|©|¹superscript©©F R ^ | caligraphic_N | Ã | caligraphic_N | is the traffic matrix. The goal is to minimize the total [T]i,jasubscriptsuperscriptdelimited-[][T]^{a}_{i,j}[ T ] ^ a _ i , j across the entire route. In this case, instead of the direct problem data TasuperscriptT^{a}T ^ a , we need to consider the complex problem instance c=(©,R,S,F)©…†¹c=( = ( caligraphic_N , R , S , F ). Typically, methods such as solvers or heuristics require significant expert effort to process such data. On the other hand, IC/DC is capable of handling these diverse types of data and demonstrates strong generalization performance. By the problem encoder, the information R…RR for each city is processed through an embedding layer with dimension ddd and then sent to AAA and BµBB. The relational information between cities, S†SS and F¹FF, is input as D=S||F|©|Ã|©|Ã2D=S||F 2}D = S | | F R ^ | caligraphic_N | Ã | caligraphic_N | Ã 2 . As shown in Table 3, IC/DC outperforms other baselines and demonstrates strong potential for generalizability. Additionally, an example of the NP is illustrated in Figure 5.",
        "keywords": ""
    },
    {
        "id": 24,
        "title": "Generalization of Arithmetico-Geometric Series and the Expectation Value of a kğ�‘˜kitalic_k-Run of a Bernoulli Trial",
        "abstract": "AbstractThe article uses an Arithmetic-Geometric Fibonacci series to find the expected value, denotedEE(\\chi)E , of trials needed to observekkkconsecutive successes for the first time in a Bernoulli experiment using a recurrence relation. The article establishes thatE=2(2k1)2superscript21E(\\chi)=2(2^{k}-1)E  = 2 ( 2 ^ k - 1 ). It is important to note that this is not a new result, but to the best of my knowledge, this is a novel derivation of a well-established result. The other derivations of this result are cited in the references section.",
        "corpus": "The article uses an Arithmetic-Geometric Fibonacci series to find the expected value, denoted EE( , of trials needed to observe kkk consecutive successes for the first time in a Bernoulli experiment using a recurrence relation. The article establishes that E=2(2k1)2superscript21E(  = 2 ( 2 ^ k - 1 ). It is important to note that this is not a new result, but to the best of my knowledge, this is a novel derivation of a well-established result. The other derivations of this result are cited in the references section. Consider a coin-tossing experiment where the probability of success (i.e., getting heads) is ppp, and the probability of failure (i.e., getting tails) is 1p11-p1 - p. For a fair coin, p=1212p= = / 1 2 . This article explores the general case where ppp is any real number within the interval (0,1)01(0,1)( 0 , 1 ). We perform trials and record the outcome of each trial”heads or tails”sequentially. For instance, a possible outcome sequence in a 4444-trial experiment could be hhth¡hhthh h t h, where hhh represents heads and t¡tt represents tails. Our goal is to determine the average number of trials required to achieve kkk-consecutive successes for any positive integer kkk. For example, when k=33k=3k = 3, a successful experiment may conclude in as few as 3333 trials with the outcome hhhhhhh h h. However, in other cases, it may take many more trials. For instance, the sequence hhtththttth¡¡¡¡¡¡hhtththttthh h t t h t h t t t h contains no occurrence of 3333-consecutive heads, so additional trials would be needed to achieve the first instance of 3333-consecutive heads. A Bernoulli experiment consists of a series of independent Bernoulli trials, where each trial has a random outcome with two possible results: success or failure. The probability of success remains constant throughout the trials. Common examples of Bernoulli trials include tossing a coin, where the outcome is either heads or tails, or rolling a die, where one outcome is designated as success, while the others represent failure. The aim of this article is to determine the expected number of trials (i.e., the average) needed to observe kkk-consecutive successes for the first time in a Bernoulli experiment. For the purposes of this paper, we refer to successes as heads and failures as tails, analogous to a coin toss experiment. Using a generalized form of arithmetic, geometric, and Fibonacci series, we calculate the expected value of trials needed to observe kkk-consecutive successes for the first time. While the results discussed here are not new and can be found in sources such as [3], [4], and [5], this article is written by a high school student with the goal of making these concepts accessible to other high school students. It aims to demonstrate how basic ideas from arithmetic, geometric, and Fibonacci series, along with recurrence relations, can be applied to solve an intriguing problem: determining the average number of trials required to observe kkk-consecutive heads for the first time in a coin-tossing experiment. In the next section, we discuss some preliminary concepts necessary to achieve our goal. For any positive integers kkk and nnn, let ±n,ksubscript± _ n , k be the set of sequences of heads and tails over nnn trials, such that the first occurrence of kkk consecutive heads appears in the nnn-th trial (i.e., the last kkk symbols in the sequence are all heads). Denote the cardinality of ±n,ksubscript± _ n , k by |±n,k|=gn,ksubscript±subscript”| caligraphic_F _ n , k | = g _ n , k . As implied by the above definition, we require gn,k=0subscript”0g_{n,k}=0g _ n , k = 0 whenever n<kn<kn < k and gn,k=1subscript”1g_{n,k}=1g _ n , k = 1 whenever n=kn=kn = k. Furthermore, define g0,k=0subscript”00g_{0,k}=0g _ 0 , k = 0 for all positive integers kkk. For any positive integers kkk and nnn with n¥kn kn ¥ k, the following holds: Proof. To demonstrate that gn,k=m=1kgnm,ksubscript”superscriptsubscript1subscript”g_{n,k}= _ n , k = _ m = 1 ^ k g _ n - m , k , we will establish a bijection between the set ±n,ksubscript± _ n , k and the union of the sets {±ni,k£1¤i¤k}conditional-setsubscript±1 1 i k caligraphic_F _ n - i , k £ 1 ¤ i ¤ k }. Note that by definition of ±n,ksubscript± _ n , k , ±ni,ksubscript± _ n - i , k and ±nj,ksubscript± _ n - j , k are disjoint for positive integers i¤ki ki ¤ k and j¤kj kj ¤ k whenever i ji ji j. Consider any valid sequence (i.e., an element) from ±n,ksubscript± _ n , k . In this sequence, let iii be the position of the first appearance of tails. If the first tails appears at position iii, then the segment of the sequence followed by this tails has the length nin-in - i. This segment must be a valid sequence in ±ni,ksubscript± _ n - i , k . Now, consider a sequence in ±ni,ksubscript± _ n - i , k . This sequence has kkk consecutive heads in the end by definition. These kkk heads occupy positions beginning at the position nik+11n-i-k+1n - i - k + 1. By placing iii tails in the beginning of this sequence, we obtain a sequence of length nnn where the first occurrence of kkk consecutive heads appears exactly at the nnn-th position. Thus, this extended sequence belongs to ±n,ksubscript± _ n , k . We have thus defined a bijection between ±n,ksubscript± _ n , k and the union of the sets {±ni,k£1¤i¤k}conditional-setsubscript±1 1 i k caligraphic_F _ n - i , k £ 1 ¤ i ¤ k }. Each sequence in ±n,ksubscript± _ n , k uniquely corresponds to one in ±ni,ksubscript± _ n - i , k for some iii, and each sequence in ±ni,ksubscript± _ n - i , k can be uniquely extended to one in ±n,ksubscript± _ n , k by concatenating a sequence of iii tails in the beginning. As ±ni,ksubscript± _ n - i , k and ±nj,ksubscript± _ n - j , k are disjoint whenever i ji ji j and there is bijection between ±n,ksubscript± _ n , k and the set {±ni,k£1¤i¤k}conditional-setsubscript±1 1 i k caligraphic_F _ n - i , k £ 1 ¤ i ¤ k }, the number of elements in ±n,ksubscript± _ n , k equals the sum of the number of elements in the sets ±ni,ksubscript± _ n - i , k for iii from 1 to kkk: Since |±ni,k|=gni,ksubscript±subscript”| caligraphic_F _ n - i , k | = g _ n - i , k , it follows that: For any positive integer kkk, consider performing a Bernoulli experiment, such as repeatedly tossing a coin, until we achieve kkk consecutive successes for the first time. Let Sksubscript†S_{k}S _ k represent the set of all outcome sequences from this experiment. Specifically, these sequences end with exactly kkk consecutive successes and contain no occurrence of kkk consecutive successes prior to the final trial. Let be the random variable that maps a sequence in Sksubscript†S_{k}S _ k to its length (i.e., to the number of trials in the sequence). Let p(=i)p( ( = i ) denote the probability that a sequence in Sksubscript†S_{k}S _ k is of length iii. For i=ni=ni = n by definition of Fn,ksubscript¹F_{n,k}F _ n , k , for a fair coin with success probability in each trial p=1212p= = / 1 2 , each sequence in Fn,ksubscript¹F_{n,k}F _ n , k is equally likely. That is each sequence in Fn,ksubscript¹F_{n,k}F _ n , k is equally likely independent of number of heads or tails in the sequence. Hence, The expectation value EE(  is expressed as: We now define the series y=i=1gi,k‹…ri¦superscriptsubscript1‹…subscript”superscripty= r^{i}y = _ i = 1 ^ g _ i , k ‹… r ^ i . Note that the term-by-term differentiation of the series y¦yy with respect to rrr results in: To prove that the series y¦yy can be term-by-term differentiated, we must establish that it converges absolutely. Consider the series y(r)=i=kgi,k‹…ri¦superscriptsubscript‹…subscript”superscripty(r)= r^{i}y ( r ) = _ i = k ^ g _ i , k ‹… r ^ i , with gi,ksubscript”g_{i,k}g _ i , k defined recursively. Let ai=gi,k‹…risubscript‹…subscript”superscripta_{i}=g_{i,k} r^{i}a _ i = g _ i , k ‹… r ^ i . Applying the ratio test, we compute: Substituting r=1212r= = / 1 2 , we obtain: Thus, we conclusively establish L<1¿1L<1L < 1 and prove that y¦yy converges absolutely via the ratio test. Let where 0<r<1010<r<10 < r < 1. As gi,k=0subscript”0g_{i,k}=0g _ i , k = 0 for integers i<ki<ki < k, we get To find a solution, we multiply the equation by successive powers of rrr up to rksuperscriptr^{k}r ^ k : Next, we subtract these equations from the original expression of y¦yy. As gi,k=j=1kgij,ksubscript”superscriptsubscript1subscript”g_{i,k}= _ i , k = _ j = 1 ^ k g _ i - j , k and gi,k=0subscript”0g_{i,k}=0g _ i , k = 0 for integers i<ki<ki < k, we get Note that all g(i,k)j=1kgij,k=0”superscriptsubscript1subscript”0g(i,k)- ( i , k ) - _ j = 1 ^ k g _ i - j , k = 0 for all i>ki>ki > k in the right side of the above equation. Evaluating the geometric series on the left side of the equation: Therefore, Finally, substituting the value of EE( , we obtain: Below are some evaluations of expected values for kkk-run for various values of kkk for a fair coin (i.e., r=1212r= = / 1 2 ) based on the above obtained formula. It can be easily shown using mathematical induction that for r=1212r= = / 1 2 , for any positive integer kkk the value of EE(  simplifies to: . This paper derived the expectation value for the length of the first kkk-consecutive successes in a Bernoulli trial using arithmetic-geometric generalized Fibonacci series. By applying recurrence relations and some basic calculus, we established a formula for the expected number of trials required. Our final expression is: Deriving an analogous formula becomes significantly more challenging when the probability of success in a Bernoulli trial deviates from 1212 1 2 (i.e., when the coin is not fair), and no simplified closed-form solution exists. Note that for r 1212r / 1 2 , all sequences in the set Fi,ksubscript¹F_{i,k}F _ i , k are equally likely, which implies that p(=i) gi,k2isubscript”superscript2p( ( = i ) / g _ i , k 2 ^ i as given in equation (1). Besides r 1212r / 1 2 , another promising direction for further research could involve extending these findings to other stochastic processes. I would like to extend my gratitude to Dr. Stoyan Dimitrov for introducing me to the problem, his advice, steering the direction and deciding the scope of this exploration. This paper would not have been possible without his support. I would also like to express my special thanks to Dr. Niraj Khare for putting me in touch with Dr. Stoyan Dimitrov and nurturing my love for mathematics.",
        "keywords": ""
    },
    {
        "id": 25,
        "title": "Inverse Zğ�‘�Zitalic_Z-matrices with the bi-diagonal south-west structure",
        "abstract": "Abstract.Two new matrix classes are introduced; inverse cyclic matrices and bi-diagonal south-west matrices. An interesting relation is established between these classes. Applications to two classes of inverseZZZ-matrices are provided.",
        "corpus": "Two new matrix classes are introduced; inverse cyclic matrices and bi-diagonal south-west matrices. An interesting relation is established between these classes. Applications to two classes of inverse ZZZ-matrices are provided. The objective of this article is twofold. One is to present a brief survey on the class of inverse ZZZ-matrices, with specific reference to its nonnegativity/nonpositivity properties. The second aim is to propose two matrix classes, motivated by a result on inverse MMM-matrices, and to understand their relationship. We start with a short survey of the literature on inverse ZZZ-matrices. The set of real square matrices of order nnn is denoted by nsubscript _ n ( R ). We refer to a matrix each of whose entry is nonzero as a full matrix. For any matrix AAA, we use A¥00A 0A ¥ 0 to denote the fact that all the entries of AAA are nonnegative; A>0(A<0)00A>0~{}(A<0)A > 0 ( A < 0 ) will signify that all the entries of AAA are positive (negative). The determinant of AAA is denoted by detA Aroman_det A. For Î±,Î²Š†{1,2,¦,n}¼½12¦ , Î² Š† { 1 , 2 , ¦ , n }, whose elements are in the ascending order, we use A[Î±Î²]matrix¼½A [ start_ROW start_CELL Î± end_CELL end_ROW start_ROW start_CELL Î² end_CELL end_ROW ] to denote the submatrix of AAA containing the rows and columns indexed by Î±¼ and Î²½ respectively. A[Î±]delimited-[]¼A[ [ Î± ] is referred to as a principal submatrix of AAA. If Î±ŠŠ{1,2,¦,n}¼12¦ ŠŠ { 1 , 2 , ¦ , n }, then A[Î±]delimited-[]¼A[ [ Î± ] will be called a proper principal submatrix. Finally, A(Î±)¼A( ( Î± ) denotes the principal submatrix in the rows and columns defined by the complement Î±²superscript¼² ^ ² of Î±¼ in {1,2,¦,n}12¦ 1 , 2 , ¦ , n }. We use (A) ( A ) to denote the spectral radius of A,A,A , which by definition is the maximum of the modulii of the eigenvalues of AAA. We shall be concerned with the notion of irreducibility of matrices, which we recall, next. For a matrix A=(aij)nsubscriptsubscriptA=(a_{ij}) = ( a _ i j ) M _ n ( R ), the digraph (directed graph) (A) ( A ) of AAA, has {v1,v2,¦,vn}subscript£1subscript£2¦subscript£ v _ 1 , v _ 2 , ¦ , v _ n } as the vertex set, and whose edge set consists of those ordered pairs (vi,vj),subscript£subscript£(v_{i},v_{j}),( v _ i , v _ j ) , if the corresponding entry aij 0subscript0a_{ij} 0a _ i j 0. Note that the above definition allows the digraph to have loops. A (directed) path in (A) ( A ) from vertex visubscript£v_{i}v _ i to vertex vjsubscript£v_{j}v _ j , is a set of distinct vertices {vi,vi1,¦,vir,vj}subscript£subscript£subscript1¦subscript£subscriptsubscript£ v _ i , v _ i _ 1 , ¦ , v _ i _ r , v _ j } such that (vi,vi1),(vi1,vi2),¦,(vir,vj)subscript£subscript£subscript1subscript£subscript1subscript£subscript2¦subscript£subscriptsubscript£(v_{i},v_{i_{1}}),(v_{i_{1}},v_{i_{2}}), v _ i , v _ i _ 1 ) , ( v _ i _ 1 , v _ i _ 2 ) , ¦ , ( v _ i _ r , v _ j ) are (directed) edges. A matrix AAA is called irreducible if its associated digraph (A) ( A ) is strongly connected, i.e., there is at least one directed path between any two vertices in (A) ( A ). We shall be interested in the following class of sign pattern matrices. A matrix A=(aij)nsubscriptsubscriptA=(a_{ij}) = ( a _ i j ) M _ n ( R ) is said to be a ZZZ-matrix if aij¤0subscript0a_{ij} 0a _ i j ¤ 0, for all i ji ji j (so that all the off-diagonal entries of AAA are nonpositive). Any such matrix AAA can be represented as Denote for r=1,2,¦,n12¦r=1,2, = 1 , 2 , ¦ , n, with the convention that 0(B):=assignsubscript0µ _ 0 ( B ) := - and n+1(B):=assignsubscript1µ _ n + 1 ( B ) := . The following classification of ZZZ-matrices was introduced in [4]. Let Ls(s=0,1,2,¦,n)subscript¿ 012¦L_{s}(s=0,1,2, _ s ( s = 0 , 1 , 2 , ¦ , n ) denote the class of real nÃnn nn Ã n matrices having the form A nonsingular matrix AAA is called an inverse ZZZ-matrix, if A1superscript1A^{-1}A ^ - 1 is a ZZZ-matrix. In order to understand the behaviour of those inverse ZZZ-matrices that will be considered here, first we recall the corresponding classes of ZZZ-matrices. First, we recall the notion of an MMM-matrix. Consider a ZZZ-matrix represented as above. If, in addition, one has t¥(B)¡µt ¥ ( B ), then AAA is called an MMM-matrix. Such a matrix AAA is invertible, if t>(B)¡µt> > ( B ). In that case, it is well known that A1¥0.superscript10A^{-1} 0.A ^ - 1 ¥ 0 . There is a variety of results that identify when a ZZZ-matrix is an MMM-matrix. In the book [1] more than fifty characterizations are presented. Two of them are recalled here. [1, Theorem 2.3] Let AAA be any ZZZ-matrix. Then the following are equivalent: AAA is a nonsingular MMM-matrix; all the principal minors of AAA are positive; A1superscript1A^{-1}A ^ - 1 exists and A1¥0superscript10A^{-1} 0A ^ - 1 ¥ 0. It follows that the diagonal entries of any nonsingular MMM-matrix are positive. For the case of irreducible matrices, there is something more to say. [1, Thorem 2.7] Let AAA be any ZZZ-matrix. Then AAA is a nonsingular irreducible MMM-matrix if and only if A1>0superscript10A^{-1}>0A ^ - 1 > 0. The other classes of ZZZ-matrices are obtained if the number t¡tt in the representation A=tIB¡¼µA=tI-BA = t I - B, for B¥0µ0B 0B ¥ 0, lies to the left of (B).µ ( B ) . Let us recall one such class, viz., NNN-matrices. Once again, let AAA be given as above. If t¡tt satisfies the inequalities n1(B)<t<(B),subscript1µ¡µ _ n - 1 ( B ) < t < ( B ) , for n¥22n 2n ¥ 2 then AAA is referred to as an NNN-matrix. This matrix class was introduced and investigated in [3]. The determinant of an NNN-matrix is negative and each proper principal submatrix is an invertible MMM-matrix (so that all its diagonal entries are positive). While an invertible MMM-matrix is inverse nonnegative, an invertible NNN-matrix is inverse negative. In particular, it follows that an NNN-matrix is irreducible. We record this result. [6, Corollary 2.8] Let AAA be a ZZZ-matrix. Then AAA is a NNN-matrix if and only if A1<0superscript10A^{-1}<0A ^ - 1 < 0. If equality is allowed in the strict inequality in the definition of an NNN-matrix i.e., in the given representation of AAA, if t¡tt satisfies the inequalities n1(B)¤t<(B)subscript1µ¡µ t< _ n - 1 ( B ) ¤ t < ( B ), then AAA is referred to as an N0subscript0N_{0}N _ 0 -matrix. This matrix class was investigated in [6] and a number of interesting properties were proved. Clearly, every NNN-matrix is an N0subscript0N_{0}N _ 0 -matrix and that the latter matrix class is the topological closure of the former. Interestingly, analogous to the property of any matrix belonging to the class of NNN-matrices, we have the following result: A matrix AAA is an N0subscript0N_{0}N _ 0 -matrix precisely if AAA is a ZZZ matrix with the property that all proper principal submatrices are (not necessarily invertible) MMM-matrices, and the determinant of AAA is negative [6, Lemma 2.1]. Here is another result: If AAA is a ZZZ-matrix, then AAA is an N0subscript0N_{0}N _ 0 -matrix if and only if A1¤0superscript10A^{-1} 0A ^ - 1 ¤ 0 and is irreducible [6, Theorem 2.7]. Note that a nonsingular matrix is irreducible if and only if its inverse is irreducible. We turn our attention to yet another class which has received considerable attention. A matrix AnsubscriptA M _ n ( R ) with n¥33n 3n ¥ 3, is an F0subscript¹0F_{0}F _ 0 -matrix if A=tIB,¡¼µA=tI-B,A = t I - B , with B¥0µ0B 0B ¥ 0 and n2(B)¤t<n1(B)subscript2µ¡subscript1µ t< _ n - 2 ( B ) ¤ t < _ n - 1 ( B ). An equivalent manner in which this class is defined is given by the following. Let AnsubscriptA M _ n ( R ) be a ZZZ-matrix. Then AAA is an F0subscript¹0F_{0}F _ 0 -matrix if and only if, it satisfies the following conditions: (1) all principal submatrices of order at most n22n-2n - 2 are MMM-matrices; (2) at least one principal submatrix of order n11n-1n - 1 is an N0subscript0N_{0}N _ 0 -matrix. While there is no nonpositivity or nonnegativity result that is true, in general, for F0subscript¹0F_{0}F _ 0 -matrices, we have: [2, Theorem 2.4] Let AAA be a ZZZ-matrix. Then AAA is an F0subscript¹0F_{0}F _ 0 -matrix if and only if: (1) det(A)<0;¡0det(A)<0;d e t ( A ) < 0 ; (2) all principal minors of A1superscript1A^{-1}A ^ - 1 of order at least two are nonpositive; (3) at least one diagonal entry of A1superscript1A^{-1}A ^ - 1 is positive. For more details, we refer the reader to [2] and [6]. Recalling the definition of Lssubscript¿ L_{s}L _ s , it now clear that, for s=n s=ns = n, we obtain the MMM-matrix class, the N0subscript0N_{0}N _ 0 -matrix class corresponds to s=n1 1s=n-1s = n - 1, while for s=n2 2s=n-2s = n - 2, it is the F0subscript¹0F_{0}F _ 0 -matrix class that one obtains. Next, we revisit a matrix class, historically important, due to its relevance to inverse ZZZ-matrices. Let real numbers a1,a2,¦,ansubscript1subscript2¦subscripta_{1},a_{2}, _ 1 , a _ 2 , ¦ , a _ n be such that an>an1>¦>a1.subscriptsubscript1¦subscript1a_{n}>a_{n-1}> _ n > a _ n - 1 > ¦ > a _ 1 . A matrix A:=(aij)assignsubscriptA:=(a_{ij})A := ( a _ i j ) is called a matrix of type D·DD, if A matrix of type D·DD of order nÃnn nn Ã n will be denoted by Dn.subscript·D_{n}.D _ n . For instance, We also have the following recurrence relation: where b=(a1,a2,¦,an)T.superscriptsubscript1subscript2¦subscriptb=(a_{1},a_{2}, = ( a _ 1 , a _ 2 , ¦ , a _ n ) ^ T . Type D·DD matrices were introduced in [9], where the first item below was proved. Let AnsubscriptA M _ n ( R ) be a type D·DD-matrix. If a1>0,subscript10a_{1}>0,a _ 1 > 0 , then AAA is an inverse MMM-matrix and A1superscript1A^{-1}A ^ - 1 is tridiagonal [9, Theorem 2.3]. If an<0,subscript0a_{n}<0,a _ n < 0 , then AAA is an inverse N0subscript0N_{0}N _ 0 -matrix such that A1superscript1A^{-1}A ^ - 1 is tridiagonal [7, Theorem 2.4]. If an>0subscript0a_{n}>0a _ n > 0 and an1<0,subscript10a_{n-1}<0,a _ n - 1 < 0 , then A1superscript1A^{-1}A ^ - 1 is a tridiagonal F0subscript¹0F_{0}F _ 0 -matrix [13, Theorem 4.21]. The results above, obtained for three special classes of inverse ZZZ-matrices, were extended to the full class Lssubscript¿ L_{s}L _ s , which we recall. We set L1:=Ln.assignsubscript¿1subscript¿L_{-1}:=L_{n}.L _ - 1 := L _ n . [12, Theorem 3.12] Let AnsubscriptA M _ n ( R ) be a matrix of type D·DD with a1 0subscript10a_{1} 0a _ 1 0. Let s ss denote the number of nonpositive parameters in the sequence an>an1>‹¯>a1subscriptsubscript1‹¯subscript1a_{n}>a_{n-1}> _ n > a _ n - 1 > ‹¯ > a _ 1 . Then A1superscript1A^{-1}A ^ - 1 is a tridiagonal ZZZ-matrix and A1Ls1superscript1subscript¿ 1A^{-1} L_{s-1}A ^ - 1 L _ s - 1 . In the next result, we obtain specific consequences, presented explicitly, for the three classes: NNN-matrices, N0subscript0N_{0}N _ 0 -matrices and F0subscript¹0F_{0}F _ 0 -matrices. It appears that these have not been noticed before (see the para after Theorem 3.12, [12]). We skip their proofs. Note that the number of nonpositive parameters in the first and the second items is nnn, while in the third instance, this number is n1.1n-1.n - 1 . Observe that the second item below, strengthens the first item above. Let AnsubscriptA M _ n ( R ) be a type D·DD-matrix. If an<0subscript0a_{n}<0a _ n < 0, then A1superscript1A^{-1}A ^ - 1 is a tridiagonal NNN-matrix (and hence a tridiagonal N0subscript0N_{0}N _ 0 -matrix). If an=0subscript0a_{n}=0a _ n = 0, then A1superscript1A^{-1}A ^ - 1 is a tridiagonal N0subscript0N_{0}N _ 0 -matrix, which is not an NNN-matrix. If an1=0subscript10a_{n-1}=0a _ n - 1 = 0, then A1superscript1A^{-1}A ^ - 1 is a tridiagonal F0subscript¹0F_{0}F _ 0 -matrix. In this article, we prove a result each for inverse MMM-matrices and inverse NNN-matrices. While the inverse MMM-matrix result is the same as what is stated in Theorem 1.9 to follow, we prove it as a consequence of the main result of this article. The result for inverse NNN-matrices is new. It is pertinent to note that the inverse MMM-matrix problem is to characterize those nonsingular nonnegative matrices that are inverses of MMM-matrices. While a number of matrix classes have been shown to be inverse MMM-matrices, the problem in its full generality remains open. We refer the reader to the recent monograph [5] for a comprehensive treatment. Consider the following two results. [11, Theorem 4.3] Let A=(aij)nsubscriptsubscriptA=(a_{ij}) = ( a _ i j ) M _ n ( R ). Then the following statements are equivalent: AAA is an inverse MMM-matrix such that (A1)superscript1 ( A ^ - 1 ) is the simple nnn-cycle v1†v2†¦†vn†v1†subscript£1subscript£2†¦†subscript£†subscript£1v_{1} v_{2} v_{n} v_{1}v _ 1 † v _ 2 † ¦ † v _ n † v _ 1 (with loops). A>00A>0A > 0 and the following hold: a11a22‹¯ann>a12a23‹¯a(n1)nan1subscript11subscript22‹¯subscriptsubscript12subscript23‹¯subscript1subscript1a_{11}a_{22} a_{nn}>a_{12}a_{23} a_{(n-1)n}a_{n1}a _ 11 a _ 22 ‹¯ a _ n n > a _ 12 a _ 23 ‹¯ a _ ( n - 1 ) n a _ n 1 , aij=aikakjakk,subscriptsubscriptsubscriptsubscripta_{ij}= _ i j = / a _ i k a _ k j a _ k k , for all distinct vertices vi,vj,vksubscript£subscript£subscript£v_{i},v_{j},v_{k}v _ i , v _ j , v _ k such that vksubscript£v_{k}v _ k lies on the path from visubscript£v_{i}v _ i to vjsubscript£v_{j}v _ j . Set Z:=(en,e1,e2,¦,en1)assignsuperscriptsuperscript1superscript2¦superscript1Z:=(e^{n},e^{1},e^{2}, := ( e ^ n , e ^ 1 , e ^ 2 , ¦ , e ^ n - 1 ), where eksuperscripte^{k}e ^ k denotes the kthsuperscript¡k^{th}k ^ t h standard basis vector of nsuperscript ^ n . [11, Corollary 4.5] Let ZZZ be the matrix defined as above. Let Î±1,Î±2,¦,Î±nsubscript¼1subscript¼2¦subscript¼ _ 1 , Î± _ 2 , ¦ , Î± _ n be nonnegative numbers and Then the following are equivalent: A1superscript1A^{-1}A ^ - 1 is an MMM-matrix with (A1)superscript1 ( A ^ - 1 ) being the simple nnn-cycle v1†v2†¦†vn†v1†subscript£1subscript£2†¦†subscript£†subscript£1v_{1} v_{2} v_{n} v_{1}v _ 1 † v _ 2 † ¦ † v _ n † v _ 1 , with loops. A>00A>0A > 0 satisfies Î±1>Î±2>0subscript¼1subscript¼20 _ 1 > Î± _ 2 > 0, Î±r=(Î±2)r1/(Î±1)r2,subscript¼superscriptsubscript¼21superscriptsubscript¼12 _ r = ( Î± _ 2 ) ^ r - 1 / ( Î± _ 1 ) ^ r - 2 , for r=3,¦,n3¦r=3, = 3 , ¦ , n. The motivation for the article comes from the question as to what lies in the background of Theorem 1.9, which when applied to matrices satisfying the second condition, gives rise to the special class of inverse MMM-matrices, described in the first item. Specifically, inspired by the first item above, we introduce a matrix class, without requiring it to be an MMM-matrix. We call such a matrix a bi-diagonal south-west matrix. Simultaneously, another new class is identified, taking cue from the second item, giving rise to what we refer to as an inverse cyclic matrix. A relationship is established between these two matrix classes in Theorem 2.11. This result, while enabling us to recover Theorem 1.9 as a particular case, further allows us to obtain a new result on inverse NNN-matrices, which is presented in Theorem 3.1. An analogue of Theorem 1.10 is obtained in Theorem 3.6, for inverse NNN-matrices. We propose two matrix classes and study their relationship. The first notion is that of the inverse cyclic property of a matrix. Let A=(aij)nsubscriptsubscriptA=(a_{ij}) = ( a _ i j ) M _ n ( R ) be such that aii 0,1¤i¤n.formulae-sequencesubscript01a_{ii} 0,1 i n.a _ i i 0 , 1 ¤ i ¤ n . Then AAA is said to have the inverse cyclic property (or is called an inverse cyclic matrix), if the entries of AAA satisfy the following conditions: The requirement on the entries above can also be written equivalently, as For an inverse cyclic matrix, it follows that all the elements in the upper triangular part are determined by the elements on the main diagonal and the super diagonal, while the elements in the lower triangular part are determined by those on the main diagonal, the super diagonal and the entry lying in the intersection of the first column and the last row of the matrix. It may be verified that the matrix A=(2240012000202241)matrix2240012000202241A= ~{}~{}0&~{}~{}1&~{}~{}2&~{}~{}0 ~{}~{}0&~{}~{}0&-2&~{}~{}0 ~{}~{}2&-2&-4&~{}~{}1 = ( start_ROW start_CELL 2 end_CELL start_CELL - 2 end_CELL start_CELL - 4 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 1 end_CELL start_CELL 2 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL - 2 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 2 end_CELL start_CELL - 2 end_CELL start_CELL - 4 end_CELL start_CELL 1 end_CELL end_ROW ) has the inverse cyclic property. For A=(aij)nsubscriptsubscriptA=(a_{ij}) = ( a _ i j ) M _ n ( R ), set i.e. the product of the diagonal entries of AAA, which we may refer to as the cyclic product. Let us denote by xksuperscript¥x^{k}x ^ k , the kthsuperscript¡k^{th}k ^ t h column of the matrix X=(xij)‹subscript¥X=(x_{ij})X = ( x _ i j ). Let AnsubscriptA M _ n ( R ) be an inverse cyclic matrix. Then First, note that an inverse cyclic matrix is one where all the diagonal entries are nonzero. Thus, d 0.0d 0.d 0 . Let C¶CC be the matrix obtained from AAA, after employing the following column operations: It is easy to see that C¶CC is a lower triangular matrix with diagonal entries Thus, we have Then, Next, we introduce the second new class of matrices called bi-diagonal south-west matrices. Let AnsubscriptA M _ n ( R ). Suppose that all the entries of AAA that lie on the main diagonal, the super diagonal and the entry in the south-west corner, are nonzero, while all the other entries are zero. Then AAA is called a bi-diagonal south-west matrix, or bdsw, for short. As an illustration, a pattern of the bdsw matrix (for n=44n=4n = 4), is given by where the symbol * stands for a nonzero entry. Let AAA be a bdsw matrix. Then every proper principal submatrix of AAA is either an upper triangular matrix or a block lower triangular matrix. Moreover, every proper principal submatrix of a bdsw matrix is nonsingular. The motivation for considering the class of bdsw matrices arises from the structure of the digraph associated with it. The digraph corresponding to a bdsw matrix (of order nÃnn nn Ã n) is the simple directed nnn-cycle v1†v2†¦†vn†v1†subscript£1subscript£2†¦†subscript£†subscript£1v_{1} v_{2} v_{n} v_{1}v _ 1 † v _ 2 † ¦ † v _ n † v _ 1 , with loops. The digraph for a bdsw matrix of order 4444 is given below. Recall that unipathic digraphs are those digraphs in which there is at most one (directed) path between any two vertices. However, matrices AAA possessing the bdsw structure are examples of irreducible unipathic matrices. This means that, in (A) ( A ), there is exactly one path between any two vertices. We will make use of an auxiliary result, which presents a graph theoretic formula for the inverse of a nonsingular matrix. Let us recall some terminology, in this context. Let p(vi†vj)†subscript£subscript£p(v_{i} v_{j})p ( v _ i † v _ j ) denote any (directed) path from visubscript£v_{i}v _ i to vjsubscript£v_{j}v _ j and l(p),™l(p),l ( p ) , the length of the path ppp. The set of vertices of (A) ( A ) not belonging to the path ppp will be denoted by V(p)V(p)V ( p ), and by A[p(vi†vj)]delimited-[]†subscript£subscript£A[p(v_{i} v_{j})]A [ p ( v _ i † v _ j ) ] we signify the product of those elements of AAA, that lie along the given path ppp of (A) ( A ), from vertex visubscript£v_{i}v _ i to vertex vjsubscript£v_{j}v _ j . We set, A[p(vi†vj)]=0delimited-[]†subscript£subscript£0A[p(v_{i} v_{j})]=0A [ p ( v _ i † v _ j ) ] = 0 if there is no path from vertex visubscript£v_{i}v _ i to vertex vjsubscript£v_{j}v _ j . [10, Corollary 9.1] Let A=(aij)nsubscriptsubscriptA=(a_{ij}) = ( a _ i j ) M _ n ( R ) be nonsingular. Set A1=(a~ij)superscript1subscript~A^{-1}=( ^ - 1 = ( over~ a _ i j ). Then, we have and We will make use of the following particular case. Let AAA be a nonsingular unipathic matrix. Then, the off-diagonal entries of A1superscript1A^{-1}A ^ - 1 are given by where ppp is the unique path from vertex visubscript£v_{i}v _ i to vertex vjsubscript£v_{j}v _ j , if it exists. The inverse of a nonsingular bdsw matrix is a full matrix. Let AAA be a nonsingular bdsw matrix. Then AAA is a unipathic matrix, so that the entries of A1=(a~ij)superscript1subscript~A^{-1}=( ^ - 1 = ( over~ a _ i j ) satisfy (2.5). Moreover, AAA is irreducible and so A[p(vi†vj)] 0delimited-[]†subscript£subscript£0A[p(v_{i} v_{j})] 0A [ p ( v _ i † v _ j ) ] 0, for all i,ji,ji , j. Also, from Remark 2.5, detA[V(p)] 0delimited-[]0 A[V(p)] 0roman_det A [ V ( p ) ] 0, for any path ppp. In particular, all the diagonal entries of A1superscript1A^{-1}A ^ - 1 are nonzero and hence a~ij 0, for all i,j.subscript~0 for all 0, for all }i,j.over~ a _ i j 0 , for all i , j . Thus, A1superscript1A^{-1}A ^ - 1 is a full matrix. The converse of Lemma 2.8 is false. For example, the matrix is a nonsingular full matrix, But its inverse is not a bdsw matrix. Now, we prove the main result of this article. This brings about the relationship between nonsingular matrices possessing the inverse cyclic property and bdsw matrices. This result also justifies the nomenclature inverse cyclic matrix. We shall make use of the following auxiliary result, which presents a formula for the determinant of a submatrix of a nonsingular matrix and a related submatrix of its inverse. ([8], pp. 5) Let AnsubscriptA M _ n ( R ) be nonsingular and B:=A1assignµsuperscript1B:=A^{-1}B := A ^ - 1 . Let Î±={i1,i2,¦,ip}¼subscript1subscript2¦subscript = { i _ 1 , i _ 2 , ¦ , i _ p } and Î²={j1,j2,¦,jp}½subscript1subscript2¦subscript = { j _ 1 , j _ 2 , ¦ , j _ p } with 1¤i1<i2<‹¯<ip¤n1subscript1subscript2‹¯subscript1 i_{1}<i_{2}< n1 ¤ i _ 1 < i _ 2 < ‹¯ < i _ p ¤ n and 1¤j1<j2<‹¯<jp¤n1subscript1subscript2‹¯subscript1 j_{1}<j_{2}< n1 ¤ j _ 1 < j _ 2 < ‹¯ < j _ p ¤ n. Set Î³p:=(1)m=1p(im+jm).assignsubscript¾superscript1superscriptsubscript1subscriptsubscript _ p := ( - 1 ) ^ _ m = 1 ^ p ( i _ m + j _ m ) . We then have: Suppose that AnsubscriptA M _ n ( R ) is nonsingular. Then AAA is a full matrix and has the inverse cyclic property, if and only if A1superscript1A^{-1}A ^ - 1 is a bdsw matrix. Let us assume that AAA is a full matrix and has the inverse cyclic property. Since AAA is nonsingular, by Theorem 2.3, dc 00d-c 0d - c 0. Define B=(bij)µsubscriptB=(b_{ij})B = ( b _ i j ), where We claim that B=A1µsuperscript1B=A^{-1}B = A ^ - 1 . Since AAA is a full matrix, it is clear that BµBB is a bdsw matrix. Now, for all i{1,2,¦,n}12¦i { 1 , 2 , ¦ , n }, We have, for 2¤j¤n22 j n2 ¤ j ¤ n, and Incorporating these in (2.8), we obtain Case 1: i=ji=ji = j. First, let i=j=11i=j=1i = j = 1. Then, using (2.2), we have Next, let i=j 11i=j 1i = j 1. Then, using (2.2) again, we have Case 2: i ji ji j. First, let j=11j=1j = 1. We have ai1=ainan1annsubscript1subscriptsubscript1subscripta_{i1}= _ i 1 = / a _ i n a _ n 1 a _ n n , so that (AB)i1=0subscriptµ10(AB)_{i1}=0( A B ) _ i 1 = 0. Next, we consider j 11j 1j 1. We have two cases. First, let i<ji<ji < j. Then Also for j<i<nj<i<nj < i < n, and for j<i=nj<i=nj < i = n, Then from (2.9), it follows that (AB)ij=0,subscriptµ0(AB)_{ij}=0,( A B ) _ i j = 0 , in this case, too. This completes the proof that AB=Iµ¼AB=IA B = I. Conversely, suppose that B=A1µsuperscript1B=A^{-1}B = A ^ - 1 is a bdsw matrix. Then from Lemma 2.8, AAA is a full matrix and so, in particular all the diagonal entries of AAA are nonzero. Now, using Lemma 2.10, we will show that AAA has the inverse cyclic property. This is equivalent to proving that the following are true: and In view of Lemma 2.10, it suffices to show that the following hold, for the matrix BµBB: Consider the case i<k<ji<k<ji < k < j. Let E1subscript1E_{1}E _ 1 be the submatrix of BµBB of order n22n-2n - 2, corresponding to the left hand side of (2.13). Let C1subscript¶1C_{1}C _ 1 be the submatrix of E1subscript1E_{1}E _ 1 obtained by taking the first k11k-1k - 1 rows and all the n22n-2n - 2 columns. Consider the submatrix F1subscript¹1F_{1}F _ 1 of C1subscript¶1C_{1}C _ 1 , obtained by taking all the k11k-1k - 1 rows and the first k22k-2k - 2 columns of C1subscript¶1C_{1}C _ 1 . The rank of F1subscript¹1F_{1}F _ 1 does not exceed k2.2k-2.k - 2 . Since BµBB is a bdsw matrix, and since the entries along all the other columns of C1subscript¶1C_{1}C _ 1 , not being in the submatrix F1subscript¹1F_{1}F _ 1 , are zero, it follows that the k11k-1k - 1 rows of C1subscript¶1C_{1}C _ 1 are linearly dependent. Thus, detE1=0,subscript10 E_{1}=0,roman_det E _ 1 = 0 , showing that (2.13) holds. Next, let j<i nj<i nj < i n. Suppose that E2subscript2E_{2}E _ 2 is the submatrix of BµBB of order n22n-2n - 2, corresponding to the left hand side of (2.14). Let C2subscript¶2C_{2}C _ 2 be the submatrix of E2subscript2E_{2}E _ 2 obtained by taking the last nin-in - i rows and all the n22n-2n - 2 columns. Consider the submatrix F2subscript¹2F_{2}F _ 2 of C2subscript¶2C_{2}C _ 2 , obtained by taking all the nin-in - i rows and the last ni11n-i-1n - i - 1 columns of C2subscript¶2C_{2}C _ 2 . The rank of F2subscript¹2F_{2}F _ 2 does not exceed ni1.1n-i-1.n - i - 1 . Moreover, since the entries along all the other columns of C2subscript¶2C_{2}C _ 2 , not being in the submatrix F2subscript¹2F_{2}F _ 2 , are zero (since BµBB is a bdsw matrix), it follows that the nin-in - i rows of C2subscript¶2C_{2}C _ 2 are linearly dependent. Thus, detE2=0,subscript20 E_{2}=0,roman_det E _ 2 = 0 , showing that (2.14) holds. Finally, let j<i=nj<i=nj < i = n. Denote by E3subscript3E_{3}E _ 3 , the submatrix of BµBB of order n22n-2n - 2, corresponding to the left hand side of (2.15). Again, since BµBB is a bdsw matrix, all the entries in its last row except the first and the last entries, are zero. Thus, all the entries in the last row of E3subscript3E_{3}E _ 3 are zero, so that detE3=0subscript30 E_{3}=0roman_det E _ 3 = 0. Thus (2.15) holds. We have shown that AAA has the inverse cyclic property, completing the proof. Let A=(111211221)matrix111211221A= -2&~{}~{}1&~{}~{}1 ~{}~{}2&-2&-1 = ( start_ROW start_CELL 1 end_CELL start_CELL - 1 end_CELL start_CELL - 1 end_CELL end_ROW start_ROW start_CELL - 2 end_CELL start_CELL 1 end_CELL start_CELL 1 end_CELL end_ROW start_ROW start_CELL 2 end_CELL start_CELL - 2 end_CELL start_CELL - 1 end_CELL end_ROW ). Then AAA satisfies the inverse cyclic property. Also, is a bdsw matrix. Consider the matrix C,¶C,C , obtained by changing AAA in precisely one entry (a32subscript32a_{32}a _ 32 ), given by Since c32 c31c12c11subscript32subscript31subscript12subscript11c_{32} _ 32 / c _ 31 c _ 12 c _ 11 , the matrix C¶CC does not possess the inverse cyclic property. Here, is not a bdsw matrix. It is easy to verify that any principal submatrix of an inverse cyclic matrix inherits the property. Also, if AAA is a nonsingular matrix having the inverse cyclic property and B=A1,µsuperscript1B=A^{-1},B = A ^ - 1 , then for all 1¤i¤n1,111 i n-1,1 ¤ i ¤ n - 1 , In this section, we present applications of Theorem 2.11 to the two classes of nonsingular ZZZ-matrices under study here, viz., inverse MMM-matrices and inverse NNN-matrices. For An,subscriptA M _ n ( R ) , we have the following: AAA is an inverse MMM-matrix such that A1superscript1A^{-1}A ^ - 1 possesses the bdsw structure if and only if A>00A>0A > 0, dc>00d-c>0d - c > 0 and AAA has the inverse cyclic property. Let AAA be an even order matrix. Then AAA is an inverse NNN-matrix, A1superscript1A^{-1}A ^ - 1 has the bdsw pattern if and only if A<00A<0A < 0, has the inverse cyclic property with dc<00d-c<0d - c < 0. Let AAA be an odd order matrix. Then AAA is an inverse NNN-matrix such that A1superscript1A^{-1}A ^ - 1 is endowed with the bdsw structure if and only if A<00A<0A < 0 has the inverse cyclic property and dc>00d-c>0d - c > 0. (1) Let AAA be an inverse MMM-matrix such that A1superscript1A^{-1}A ^ - 1 has the bdsw structure. Since A1superscript1A^{-1}A ^ - 1 is an MMM-matrix, then all the diagonal entries of AAA are nonzero and A=(A1)1¥0superscriptsuperscript110A=(A^{-1})^{-1} 0A = ( A ^ - 1 ) ^ - 1 ¥ 0. Moreover, the bdsw structure of A1superscript1A^{-1}A ^ - 1 , implies that AAA is a full matrix i.e., A>00A>0A > 0 and AAA has the inverse cyclic property (by Theorem 2.11). Also, the entries of A1=:B=(bij)A^{-1}=:B=(b_{ij})A ^ - 1 = : B = ( b _ i j ) are given by formula (2.7). Since BµBB is an MMM-matrix, i.e., bij¤0subscript0b_{ij} 0b _ i j ¤ 0, for i j,i j,i j , and the entries of AAA are positive, it follows from (2.7) that dc>00d-c>0d - c > 0. Conversely, assume that A>00A>0A > 0 has the inverse cyclic property and dc>00d-c>0d - c > 0. Then AAA is nonsingular and by Theorem 2.11, A1superscript1A^{-1}A ^ - 1 is a bdsw matrix. To show that A1superscript1A^{-1}A ^ - 1 also an MMM-matrix, it is enough to show that A1superscript1A^{-1}A ^ - 1 is a ZZZ-matrix, since A>00A>0A > 0. However, this is easy to see, by the formula for A1superscript1A^{-1}A ^ - 1 given in (2.7). Items (2) and (3) follow in an entirely similar manner, with the additional observation that the sign of the products appearing in the inverse formula depend on the odd-even parity of the number of factors. The following examples illustrate the above Theorem. Consider the matrix It may be verified that AAA has the inverse cyclic property and dc>00d-c>0d - c > 0. Also a bdsw MMM-matrix. Consider the (even order) matrix Then, AAA has the inverse cyclic property and dc<00d-c<0d - c < 0. Its inverse is given by which is a bdsw NNN-matrix. The odd order matrix is an inverse cyclic matrix, with dc>00d-c>0d - c > 0. Here, is a bdsw NNN-matrix. The following examples illustrate that the odd-even parity in the order of the matrix is indispensable in Theorem 3.1. The matrix of even order has the inverse cyclic property with dc>00d-c>0d - c > 0. While is a bdsw matrix, it is not an NNN-matrix. On the other hand, if then AAA is of odd order, dc<00d-c<0d - c < 0 and has the inverse cyclic property. Here is not even a ZZZ-matrix. We conclude this article with a result for inverse NNN-matrices, analogous to Theorem 1.10. Let ZZZ be defined as in Theorem 1.10. Let Î±1,Î±2,¦,Î±nsubscript¼1subscript¼2¦subscript¼ _ 1 , Î± _ 2 , ¦ , Î± _ n be nonpositive numbers and Then the following are equivalent: A1superscript1A^{-1}A ^ - 1 is an NNN-matrix with (A1)superscript1 ( A ^ - 1 ) being the simple nnn-cycle v1†v2†¦†vn†v1†subscript£1subscript£2†¦†subscript£†subscript£1v_{1} v_{2} v_{n} v_{1}v _ 1 † v _ 2 † ¦ † v _ n † v _ 1 , with loops. A<00A<0A < 0 satisfies Î±2<Î±1<0subscript¼2subscript¼10 _ 2 < Î± _ 1 < 0, Î±r=(Î±2)r1/(Î±1)r2,subscript¼superscriptsubscript¼21superscriptsubscript¼12 _ r = ( Î± _ 2 ) ^ r - 1 / ( Î± _ 1 ) ^ r - 2 , for r=3,¦,n3¦r=3, = 3 , ¦ , n. It may be verified that Suppose that (1) holds. Then A1superscript1A^{-1}A ^ - 1 is an NNN-matrix with the bdsw structure. By Theorem 3.1, we then have A<00A<0A < 0 and AAA possesses the inverse cyclic property. Further, dc<00d-c<0d - c < 0, if nnn even and dc>00d-c>0d - c > 0, for nnn odd. Moreover, the inverse cyclic property of AAA yields (b). Finally, the sign of dcd-cd - c for the odd-even parity of the order of the matrix, yields the inequality Î±2<Î±1<0subscript¼2subscript¼10 _ 2 < Î± _ 1 < 0. Conversely, suppose that A<00A<0A < 0 satisfies the conditions (a)(a)( a ) and (b)(b)( b ). Then, AAA is a full matrix with the inverse cyclic property. The inequalities Î±2<Î±1<0subscript¼2subscript¼10 _ 2 < Î± _ 1 < 0 imply that dc<00d-c<0d - c < 0, if nnn even and dc>00d-c>0d - c > 0, for nnn odd. By Theorem 3.1 again, A1superscript1A^{-1}A ^ - 1 is an NNN-matrix, with the bdsw structure, so that (A1)superscript1 ( A ^ - 1 ) is a simple cycle v1†v2†¦†vn†v1†subscript£1subscript£2†¦†subscript£†subscript£1v_{1} v_{2} v_{n} v_{1}v _ 1 † v _ 2 † ¦ † v _ n † v _ 1 , with loops, completing the proof. We propose two new classes of nonsingular matrices and establish a relationship between them. We obtain consequences for two subclasses of inverse ZZZ-matrices. It is natural to ask what versions of these results apply to other inverse Z-matrices, in general and the classes of N0subscript0N_{0}N _ 0 -matrices and F0subscript¹0F_{0}F _ 0 -matrices, in particular. This is a problem for a future study.",
        "keywords": "Key words and phrases: Digraph, Zğ�‘�Zitalic_Z-matrix, Mğ�‘€Mitalic_M-matrix, inverse Mğ�‘€Mitalic_M-matrix, Nğ�‘�Nitalic_N-matrix, inverse Nğ�‘�Nitalic_N-matrix"
    },
    {
        "id": 26,
        "title": "Exploring the refuge-induced bubbling phenomenon and harvesting in a three species food chain model that incorporates memory effect and odour effect",
        "abstract": "AbstractIn this study, an odour-mediated system is developed and studied. In an odor-mediated systems, the sense of smell or odour of species plays a critical role in the interactions between predators and prey. It is widely recognised in scientific literature that these systems are very common and essential across natural ecosystems. These systems are crucial for various behaviors, including foraging, mating, and avoiding predators. In this paper, it is assumed that the presence of prey odour aids the predator in its hunting efforts. It is assumed that both prey and intermediate predators seek refuge against their respective predators upon detecting the odour of their predators. In other words, the odour of predators assists prey species in evaluating the danger and seeking refuge for hiding. This model incorporates the prey species™ harvesting as well. We also explore the impact of fading memory on the system by incorporating fractional derivatives into the model. The conditions for both the existence and local stability of the non-negative equilibria are derived. The current model system undergoes both Hopf and transcritical bifurcation when the parameter values are appropriately chosen. The dynamic behaviour of the system is showcased and thoroughly analysed using a range of diagrams, highlighting the the impact of prey refuge and predator odour parameters. This paper extensively examines the long-term impacts of harvesting within the system. The extent to which prey odour influences the system is investigated, and it emerges that prey odour can play a significant function within the system. Many special circumstances are also comprehensively addressed, including the following: a) the absence of refuge, b) the absence of harvesting, and c) the absence of the prey odour. It has been noted that when the refuge for intermediate predators gets bigger, it becomes more challenging for all three populations to coexist within the system. Interestingly, when prey does not exhibit refuge behaviour, it is still possible for all three species to coexist. Furthermore, it is apparent that the prey refuge parameterm1subscript1m_{1}m _ 1 induces bubbling phenomena in the system. In addition, population fluctuations can be observed in all three species in the system when there is no harvesting. The presence of prey odour plays a significant role in promoting a long-term cohabitation dynamic within this specific system. It has been observed that when individuals within the system have a strong memory, it positively affects the stability of the system. Numerical simulations are conducted in order to demonstrate and validate the usefulness of the model being considered, therefore supporting the analytical conclusions.",
        "corpus": "In this study, an odour-mediated system is developed and studied. In an odor-mediated systems, the sense of smell or odour of species plays a critical role in the interactions between predators and prey. It is widely recognised in scientific literature that these systems are very common and essential across natural ecosystems. These systems are crucial for various behaviors, including foraging, mating, and avoiding predators. In this paper, it is assumed that the presence of prey odour aids the predator in its hunting efforts. It is assumed that both prey and intermediate predators seek refuge against their respective predators upon detecting the odour of their predators. In other words, the odour of predators assists prey species in evaluating the danger and seeking refuge for hiding. This model incorporates the prey species™ harvesting as well. We also explore the impact of fading memory on the system by incorporating fractional derivatives into the model. The conditions for both the existence and local stability of the non-negative equilibria are derived. The current model system undergoes both Hopf and transcritical bifurcation when the parameter values are appropriately chosen. The dynamic behaviour of the system is showcased and thoroughly analysed using a range of diagrams, highlighting the the impact of prey refuge and predator odour parameters. This paper extensively examines the long-term impacts of harvesting within the system. The extent to which prey odour influences the system is investigated, and it emerges that prey odour can play a significant function within the system. Many special circumstances are also comprehensively addressed, including the following: a) the absence of refuge, b) the absence of harvesting, and c) the absence of the prey odour. It has been noted that when the refuge for intermediate predators gets bigger, it becomes more challenging for all three populations to coexist within the system. Interestingly, when prey does not exhibit refuge behaviour, it is still possible for all three species to coexist. Furthermore, it is apparent that the prey refuge parameter m1subscript1m_{1}m _ 1 induces bubbling phenomena in the system. In addition, population fluctuations can be observed in all three species in the system when there is no harvesting. The presence of prey odour plays a significant role in promoting a long-term cohabitation dynamic within this specific system. It has been observed that when individuals within the system have a strong memory, it positively affects the stability of the system. Numerical simulations are conducted in order to demonstrate and validate the usefulness of the model being considered, therefore supporting the analytical conclusions. AMS subject classifications: 92B05; 92D25; 34D23; 34C25; 34A08. Keywords: Predator“prey; memory effect; odour effect, refuge; harvesting; stability; bifurcation In ecology, odour-mediated predator-prey systems can be distinguished by the pivotal function of the odour in the interactions between predators and prey. The odour of a species is one of the key elements that significantly affects the functional response in a predator-prey system. The olfactory sense plays a vital part in essential biological activities such as predator evasion, mate selection, trail and territory identification, and food acquisition [1]. It has been noticed that a considerable proportion of animal species heavily depend on olfaction as a means to capture and analyse environmental information. Marsupials (Marsupialia) have been observed utilising feather odour signals of crimson rosella (Platycercus elegans) as a means to evaluate the condition of nest hollows. This knowledge may assist them in detecting avian prey or enhancing their vigilance when competing with parrots (Psittaciformes) for nest hollows [2]. Insects possess the ability to detect plants by olfactory cues. The olfactory cues emitted by plants are also significant in the process of identifying and choosing food sources among mammals[3]. The parasitoid species Microplitis croceipes relies on kairomones emitted by the host organism for the purpose of locating it [4]. Female moths (Lepidoptera) have been observed to employ a fascinating mechanism involving the release of odorant compounds. These compounds, which are known to contain specialised enzymes, play a crucial role in the transmission of vital information among individuals of the same species [5]. Also, the use of olfactory cues by wolves (Canis lupus) in the pursuit of prey is a well-documented phenomenon [6]. The odours secreted by prey aid the grasshopper mouse (Onychomys leucogaster) in its predation efforts [7]. Although the impact of species™ odour is widely recognised in ecological literature, there is a limited amount of research that explores its effect in mathematical models of predator-prey systems. Bhattacharjee et al.[8] discussed a three species food chain model incorporating the effect of prey odour. Xu et al. [5] studied the effect of predator odour on prey species in a predator-prey system. Shen et al. [9] investigated a predator-prey model in the presence of predator odour disturbance. Odour-mediated systems exhibit significant diversity and are present in various ecological interactions. There are numerous diverse effects that can occur in an odor-mediated system. It is found that the odour of predators can elicit defence mechanisms or anti-predator behaviours in prey species, which is widely recognised in ecological literature [10, 11, 12, 13, 14, 15, 16]. One such defensive behaviour frequently observed in nature is the refuge phenomenon. Therefore, the existence of prey refuge in an odor-mediated system is a prevalent kind of diversification. The notion of refuges is extensively acknowledged and examined within the disciplines of biology and ecology [17, 18, 19, 20, 21, 22]. The term refuge-seeking behaviour pertains to a phenomenon observed in organisms whereby they actively place themselves in areas that are either secluded or inaccessible to predators as a means of seeking protection from predation. This particular method enables the organisms to efficiently avoid predation and improve their likelihood of survival. The phenomenon of prey hiding has been observed to potentially exert a stabilising effect on predator-prey dynamics [21, 23]. However, as far as the authors are aware, no study has been undertaken that explores the connection between a prey™s refuge behaviour and the odour of its predator. The prevalence of harvesting is an intriguing diversity among these odour-mediated systems. For instance, hunters utilise various scents, like doe urine, to effectively attract bucks and increase their chances of a successful hunt [24, 25]. Trappers use baits with potent scents, such as fish or honey, to entice bears into traps [26, 27], etc.. Throughout the course of human history, the practice of harvesting different kinds of animals and insects, whether through fishing, hunting, or resource extraction, has been a vital and indispensable endeavour. Nevertheless, the expansion of human populations, developments in technology, and the increasing worldwide need for food and resources have resulted in heightened harvesting practices that have the potential to exert pressure on ecological systems and pose a threat to biodiversity [31]. Intensive harvesting can harm populations, destabilise ecosystems, and lead to species extinction [32]. Thus, it makes sense to investigate the effect of harvesting in an odour-mediated predator prey system. Despite the presence of varied odour-mediated systems in ecological settings, there has been scant research conducted in the field of mathematical modelling considering the influence of odour on predator-prey systems. This study aims to examine the impact of prey odour on predators as well as the influence of predator odour on prey in a food chain that includes harvesting. To the best of the authors™ knowledge, there is currently no mathematical model in the literature that can accomplish the same objective. There is a general consensus [34, 35, 36] that living organisms possess memory that is connected to their physical structure or mental processes . The role of species memories in shaping ecological systems is of great importance. Thus, it is essential for a predator-prey mathematical model to consider this memory effect in order to enhance its relevance. Fractional order differential equations (FDEs) have additional benefits over integer order differential equations (ODEs) when it comes to modelling these processes. Fractional differential equations (FDEs) are operators that exhibit non-local behaviour, indicating that the future state of a function is influenced not exclusively by its current state but also by all of its prior states. On the other hand, integer order differential equations (ODEs) are limited to capturing only particular changes or features at a specific step of the process [37, 38]. Thus, fractional calculus has emerged as a crucial field of study for comprehending real-world problems throughout the years [21, 22, 37, 38, 39, 40]. The integration of fractional calculus has had a significant impact on intricate dynamic systems, leading to remarkable advancements in the field of ecology. Multiple fractional derivatives have been thoroughly discussed in the literature on fractional calculus, one of which is Caputo™s derivative. Given the limitations of the integer-order derivative (ODE) in collecting full memory and fully portraying the physical behaviour of the model, we will also examine our biological system by employing a fractional derivative in this paper. Based on the aforementioned discussion and literature review, we developed an odour-mediated food chain model comprising three species in this work. Predators are presumed to derive advantages from the odour of their prey species. Both prey and intermediate predators are believed to profit from the presence of predator odour, as they utilise refuge-seeking as a strategic reaction to predator odour in order to reduce predation rates. The act of harvesting of prey is also considered. We will analyse the same model from two distinct ecological perspectives: (1) Utilising fractional order derivatives to integrate the memory effect into the system. (2) Utilising integer order derivatives that indicate the absence of memory in the system. The paper is structured in the following manner: The paper™s primary contributions to the scientific literature are covered in Section (2). In Section (3), the problem is mathematically modelled. The Section (4) provides a comprehensive discussion on all the necessary prerequisites for the Caputo fractional order derivative. The concept of well-posedness, including properties such as positivity and boundedness, is addressed in Section (5). The paper discusses the existence and local stability of all ecologically feasible equilibrium points in Section (6). The occurrence of different bifurcations is explored in Section (7). The analytical findings are validated using numerical simulations in Section (8). Conclusions are drawn in Section (9). Furthermore, the visual representation of the paper™s structure may be seen in figure (1). Based on the aforementioned literature review and subsequent study, it becomes evident that in an odour-mediated predator-prey system, the species™ odour has a direct influence on the population dynamics of the system, and it is a prevalent phenomenon in predator-prey interactions. These odour-mediated systems exhibit significant diversity and are present in various ecological interactions. Even though a substantial amount of research has been conducted in the field of mathematical modelling of a predator-prey system, there are still substantial voids in the field that need to be addressed. The following outline highlights some of these gaps: (1) Although the ecological literature broadly recognises the influence of prey odour on predator species as well as the influence of predator odour on prey, the investigation of these effects within a mathematical model of predator-prey dynamics has been fairly limited. As far as the authors are aware, no mathematical model currently exists that examines an odour-mediated predator-prey system in which the odour of both species influences each other. 2) To the best of the authors knowledge, no research has been conducted in the realm of mathematical modelling of a predator-prey system that incorporates the concept of predator-odour induced refuge in prey. (3) Although it is well acknowledged in existing literature [41, 42, 43, 44] that a relationship exists between the odour of a species and the memory of its predators or prey, there has been a lack of exploration into an odour-mediated predator-prey system in relation to the memory effect within the context of mathematical modelling of population dynamics. To address these research gaps in the existing literature, an odour-mediated three species food chain model is proposed that examines the dynamics of a predator-prey system in the presence of prey odour, predator odour-induced prey refuge, and harvesting. It is proposed that the odour of prey aids predators in their predation efforts, but prey also get benefited from the odour of their predators since they instinctively seek refuge upon detecting their predator™s odour. In other words, the predator odour directly aids in prey refuge. The harvesting of the prey is also taken into account. The model aims to provide insights into the population dynamics of an odour-mediated predator-prey system. In this paper, we will explore the same model using both an ODE (system of Ordinary differential equations) framework and an FDE (system of Fractional differential equations) framework. The ODE framework of the system offers insights into the system in the absence of memory effect, whereas the FDE framework provides insights into the system in the presence of memory effect. This paper aims to investigate an odour-mediated predator-prey system, where organisms communicate and interact through odour signals, a common and essential feature across natural ecosystems. In these systems, an important type of diversity is the use of predator odour signals by prey to seek refuge and evade predation. Within the animal kingdom, the phenomenon of seeking refuge as a prey species in response to the odour of the predator is a commonly observed occurrence to avoid predation. Thus, it is crucial to incorporate prey refuge into the mathematical modelling of an odour-mediated predator-prey system in order to enhance its realism. Another notable diversification within an odour-mediated predator-prey system is the use of harvesting practices. It is observed that odour of a species can be used for efficient and effective resource extraction from the environment. Thus, when examining an odour mediated predator prey system, it is logical to incorporate the concept of harvesting. Thus, in this paper, our objective is to analyse the influence of odour on a prey harvested three-species food chain model, specifically considering the effects of prey odour on predators and predator odour on prey. Through this investigation, valuable insights into the ecological dynamics of refuge and odour effect in a prey harvested food chain can be gained. Inspired by the research undertaken by Bhattacharjee et al. [8], we put forward the following model: with initial conditions: . For simplicity, taking m1=mfpa1subscript1subscript“subscript1m_{1}=m_{fp}a_{1}m _ 1 = m _ f p a _ 1 and m2=mspa2subscript2subscript subscript2m_{2}=m_{sp}a_{2}m _ 2 = m _ s p a _ 2 , the system (1) becomes with initial conditions: . The system under consideration, as described in equation (2), involves a food chain consisting of three species. The variables x1subscript¥1x_{1}x _ 1 , x2subscript¥2x_{2}x _ 2 , and x3subscript¥3x_{3}x _ 3 represent the population densities of the prey, intermediate predator, and top predator, respectively. It is considered that the top predator (of density x3subscript¥3x_{3}x _ 3 ) preys upon the intermediate predator (of density x2subscript¥2x_{2}x _ 2 ), while the intermediate predator, in turn, hunts upon prey (of density x1subscript¥1x_{1}x _ 1 ). Logistic growth is considered for prey, with the parameter r1subscript1r_{1}r _ 1 denoting the intrinsic growth rate. It is considered that prey odour helps intermediate predators in hunting, as discussed in [8] and Î²½ is the coefficient of odour effect produced by a single prey. The harvesting of the prey is also considered. The variable qqq represents the catchability constant, while the parameter rrr reflects the level of harvesting effort. The variable r2subscript2r_{2}r _ 2 represents the feeding rate at which the intermediate predator consumes its prey, whereas r4subscript4r_{4}r _ 4 represents the feeding rate at which the top predator consumes the intermediate predator. The conversion rates for the intermediate predator and top predator are denoted as r3subscript3r_{3}r _ 3 and r5subscript5r_{5}r _ 5 respectively. Prey and intermediate predators are considered to instinctively seek refuge when they detect the odour of their predators and it is hypothesised that the amount of predator odour is directly proportional to the number of prey individuals seeking refuge. Here, m1=mfpa1subscript1subscript“subscript1m_{1}=m_{fp}a_{1}m _ 1 = m _ f p a _ 1 and m2=mspa2subscript2subscript subscript2m_{2}=m_{sp}a_{2}m _ 2 = m _ s p a _ 2 represent the quantity of prey and intermediate predators that take refuge when they detect the odour of their predators, rendering themselves unreachable. The prey™s awareness of potential threats is presumed to rise with the heightened intensity of predator odours. This results in competition to locate refuges. Consequently, as the amount of predator odour escalates, the likelihood of prey obtaining refuge diminishes. The values mfpsubscript“m_{fp}m _ f p and mspsubscript m_{sp}m _ s p denote the probability of prey and intermediate predator obtaining refuge in the presence of predator odour. Thus, 0<mfp,msp<1formulae-sequence0subscript“subscript 10<m_{fp},m_{sp}<10 < m _ f p , m _ s p < 1. The parameter a1subscript1a_{1}a _ 1 represents the coefficient of intensity of the intermediate predator™s odour. On the other hand, the value a2subscript2a_{2}a _ 2 represents the coefficient of intensity of the top predator™s odour. Here, it is assumed that 0<m1,m2<1formulae-sequence0subscript1subscript210<m_{1},m_{2}<10 < m _ 1 , m _ 2 < 1. The variables d1subscript1d_{1}d _ 1 and d2subscript2d_{2}d _ 2 represent the natural death rates of the intermediate predator and top predator, respectively. Figure (2) provides a visual representation of an ecological system that closely resembles the system to be investigated. In order to integrate the memory effect into the system (2), we have introduced a Caputo-type fractional-order derivative with an order of Î±¼ into the model system. This modification results in the following revised model: with initial conditions: x1(0)=x10>0subscript¥10superscriptsubscript¥100x_{1}(0)=x_{1}^{0}>0x _ 1 ( 0 ) = x _ 1 ^ 0 > 0 , x2(0)=x20>0subscript¥20superscriptsubscript¥200x_{2}(0)=x_{2}^{0}>0x _ 2 ( 0 ) = x _ 2 ^ 0 > 0 , x3(0)=x30>0subscript¥30superscriptsubscript¥300x_{3}(0)=x_{3}^{0}>0x _ 3 ( 0 ) = x _ 3 ^ 0 > 0. In this section, we provide a comprehensive review of various definitions, theorems, and lemmas that are pertinent to the Caputo fractional derivative. [48] The Caputo fractional derivative with order Î±¥0¼0 0Î± ¥ 0 for the continuous function f(t)Cn([t0,+),R)“¡superscript¶subscript¡0…f(t) C^{n}([t_{0},+ ( t ) C ^ n ( [ t _ 0 , + ) , R ) is defined as where Î“Î“  is the Gamma function and t0¤tsubscript¡0¡t_{0} tt _ 0 ¤ t. Here, the symbol Î±¼ represents the order of the fractional derivative and Î±(n1,n),nformulae-sequence¼1 ( n - 1 , n ) , n N . For Î±(0,1)¼01 ( 0 , 1 ) and considering the specific case of n = 1, [39] A point x0subscript¥0x_{0}x _ 0 is said to be an equilibrium point of the following Caputo fractional order dynamical system DtÎ±t0c(x(t))=Î (x),x(t0)>0formulae-sequencesuperscriptsubscriptsuperscriptsubscript·¡¼subscript¡0¥¡Î ¥¥subscript¡00{}^{c}_{t_{0}}D_{t}^{ c end_FLOATSUPERSCRIPT _ t _ 0 D _ t ^ Î± ( x ( t ) ) = roman_Î ( x ) , x ( t _ 0 ) > 0 iff Î (x0)=0Î subscript¥00 ( x _ 0 ) = 0. [49] Let us assume, f(t) be a continuous function on (0,T] and satisfies DtÎ±t0c(f(t))¤b1f(t)+b2superscriptsubscriptsuperscriptsubscript·¡¼subscript¡0“¡subscript1“¡subscript2{}^{c}_{t_{0}}D_{t}^{ c end_FLOATSUPERSCRIPT _ t _ 0 D _ t ^ Î± ( f ( t ) ) ¤ - b _ 1 f ( t ) + b _ 2 , f(0)=f0>0“0subscript“00f(0)=f_{0}>0f ( 0 ) = f _ 0 > 0, 0<Î±<10¼10< < Î± < 1, where b1 0subscript10b_{1} 0b _ 1 0 and b1,b2subscript1subscript2b_{1},b_{2} _ 1 , b _ 2 R. Then Where, EÎ±subscript¼E_{ _ Î± is the Mittag“Leffler function. [50] We consider a system denoted as DtÎ±t0c(H(t))=•(t,H)superscriptsubscriptsuperscriptsubscript·¡¼subscript¡0»¡italic-•¡»{}^{c}_{t_{0}}D_{t}^{ c end_FLOATSUPERSCRIPT _ t _ 0 D _ t ^ Î± ( H ( t ) ) = • ( t , H ), where t0>0subscript¡00t_{0}>0t _ 0 > 0 and H(t0)=Ht0»subscript¡0subscript»subscript¡0H(t_{0})=H_{t_{0}}H ( t _ 0 ) = H _ t _ 0 is the initial condition. The parameter Î±¼ belongs to the interval (0,1]01(0,1]( 0 , 1 ], and the function •:[t0,)Ã…†nÃn:italic-•†subscript¡0superscript n}• : [ t _ 0 , ) Ã … † R ^ n Ã n , where … is a subset of nsuperscript ^ n . Now, when it satisfies the local Lipschitz condition with respect to Hn»superscriptH R ^ n then, the presence of a unique solution is confirmed on the interval [t0,)Ã…†nÃn†subscript¡0superscript[t_{0}, n}[ t _ 0 , ) Ã … † R ^ n Ã n , where [51] Let us suppose that f(t),t0cDtÎ±(w(t))C[a,b]f(t),^{c}_{t_{0}}D_{t}^{ C[a,b]f ( t ) , ^ c _ t _ 0 D _ t ^ Î± ( w ( t ) ) C [ a , b ] and 0<Î±¤10¼10< 10 < Î± ¤ 1, then ( f(t) is a non-increasing function, t[a,b]for-all¡ t t [ a , b ], provided DtÎ±t0c(v(t))¤0superscriptsubscriptsuperscriptsubscript·¡¼subscript¡0£¡0{}^{c}_{t_{0}}D_{t}^{ 0start_FLOATSUPERSCRIPT c end_FLOATSUPERSCRIPT _ t _ 0 D _ t ^ Î± ( v ( t ) ) ¤ 0. ( f(t) is a non-decreasing function, t[a,b]for-all¡ t t [ a , b ], provided DtÎ±t0c(v(t))¥0superscriptsubscriptsuperscriptsubscript·¡¼subscript¡0£¡0{}^{c}_{t_{0}}D_{t}^{ 0start_FLOATSUPERSCRIPT c end_FLOATSUPERSCRIPT _ t _ 0 D _ t ^ Î± ( v ( t ) ) ¥ 0. [48, 52] Let us consider, a Caputo fractional order dynamical system as follows: where, 0<Î±<10¼10< < Î± < 1, g(t)=(g1(t),¦,gn(t))TRn”¡superscriptsubscript”1¡¦subscript”¡superscript…g(t)=(g_{1}(t),...,g_{n}(t))^{T} R^{n}g ( t ) = ( g _ 1 ( t ) , ¦ , g _ n ( t ) ) ^ T R ^ n and f:[f1,f2,¦,fn]:Rn†Rn:“subscript“1subscript“2¦subscript“:†superscript…superscript…f:[f_{1},f_{2},...,f_{n}]:R^{n} R^{n}f : [ f _ 1 , f _ 2 , ¦ , f _ n ] : R ^ n † R ^ n . Let, f(g)=0“superscript”0f(g^{*})=0f ( g ^ ) = 0, then gsuperscript”g^{*}g ^ is an equilibrium point of the abovementioned fractional system. Let us assunme J(g)=Î(†1,†2,¦.,†n)Î(g1,g2,¦.,gn)J(g^{*})= g_{2},....,g_{n})}J ( g ^ ) = / Î ( † _ 1 , † _ 2 , ¦ . , † _ n ) Î ( g _ 1 , g _ 2 , ¦ . , g _ n ) be the Jacobian matrix of the above system at equilibrium point g=g”superscript”g=g^{*}g = g ^ and jsubscript _ j , j=1,2,3,¦,n are the eigenvalues of J(g)½superscript”J(g^{*})J ( g ^ ). Then the equilibrium point gsuperscript”g^{*}g ^ is stable if and only if |arg(j)|¥Î±2”subscript¼‹2|arg( a r g ( _ j ) | ¥ / Î± 2 and eigenvalues with |arg(j)|=Î±2”subscript¼‹2|arg( a r g ( _ j ) | = / Î± 2 have the same geometric multiplicity and algebraic multiplicity. On the other hand, the equilibrium point gsuperscript”g^{*}g ^ is locally asymptotically stable if and only if |arg(j)|>Î±2”subscript¼‹2|arg( a r g ( _ j ) | > / Î± 2 and unstable if and only if there exists eigenvalue jsubscript _ j of the Jacobian matrix J(g)½superscript”J(g^{*})J ( g ^ ) such that |arg(j)|<Î±2”subscript¼‹2|arg( a r g ( _ j ) | < / Î± 2 . Establishing the well-posedness of a predator-prey system is essential to guarantee that the model is mathematically sound, biologically meaningful, and practically applicable. This ensures that the model will operate predictably, accurately represent real-world interactions, and function as a reliable instrument for scientific research and practical applications in ecology and conservation. In this section, we begin by examining the well-posedness of the system (2) and then proceed to analyse the system (3). All solutions of system (2) exhibit positivity. From the system (2), we have which gives dx1x1=Î¨(x1,x2,x3)subscript¥1subscript¥1Î¨subscript¥1subscript¥2subscript¥3 d x _ 1 x _ 1 = roman_Î¨ ( x _ 1 , x _ 2 , x _ 3 ), dx2x2=(x1,x2,x3)subscript¥2subscript¥2“subscript¥1subscript¥2subscript¥3 d x _ 2 x _ 2 = ( x _ 1 , x _ 2 , x _ 3 ), and dx3x3=Î©(x1,x2,x3)subscript¥3subscript¥3Î©subscript¥1subscript¥2subscript¥3 d x _ 3 x _ 3 = roman_Î© ( x _ 1 , x _ 2 , x _ 3 ). Where, Î¨(x1,x2,x3)=r1(1x1)r2(1m1)(1+Î²x1)x2qrÎ¨subscript¥1subscript¥2subscript¥3subscript11subscript¥1subscript21subscript11½subscript¥1subscript¥2 x_{1})x_{2}-qrroman_Î¨ ( x _ 1 , x _ 2 , x _ 3 ) = r _ 1 ( 1 - x _ 1 ) - r _ 2 ( 1 - m _ 1 ) ( 1 + Î² x _ 1 ) x _ 2 - q r, (x1,x2,x3)=r3r2(1m1)(1+Î²x1)x2r4(1m2)x3(1+bx2)d1“subscript¥1subscript¥2subscript¥3subscript3subscript21subscript11½subscript¥1subscript¥2subscript41subscript2subscript¥31subscript¥2subscript1 x_{1})x_{2}- m_{2})x_{3}}{(1+bx_{2})}-d_{1} ( x _ 1 , x _ 2 , x _ 3 ) = r _ 3 r _ 2 ( 1 - m _ 1 ) ( 1 + Î² x _ 1 ) x _ 2 - / r _ 4 ( 1 - m _ 2 ) x _ 3 ( 1 + b x _ 2 ) - d _ 1 , and Î©(x1,x2,x3)=r5r4(1m2)x2(1+bx2)d2.Î©subscript¥1subscript¥2subscript¥3subscript5subscript41subscript2subscript¥21subscript¥2subscript2 ( x _ 1 , x _ 2 , x _ 3 ) = / r _ 5 r _ 4 ( 1 - m _ 2 ) x _ 2 ( 1 + b x _ 2 ) - d _ 2 . Now, integrating we get x1(t)subscript¥1¡x_{1}(t)x _ 1 ( t )=x1(0)subscript¥10x_{1}(0)x _ 1 ( 0 ) exp [ «0t(Î¨(x1,x2,x3))tsuperscriptsubscript0¡Î¨subscript¥1subscript¥2subscript¥3differential-d¡ _ 0 ^ t ( roman_Î¨ ( x _ 1 , x _ 2 , x _ 3 ) ) d t ] , x2(t)subscript¥2¡x_{2}(t)x _ 2 ( t )=x2(0)subscript¥20x_{2}(0)x _ 2 ( 0 )exp [ «0t((x1,x2,x3))tsuperscriptsubscript0¡“subscript¥1subscript¥2subscript¥3differential-d¡ _ 0 ^ t ( ( x _ 1 , x _ 2 , x _ 3 ) ) d t ] , x3(t)subscript¥3¡x_{3}(t)x _ 3 ( t )=x3(0)subscript¥30x_{3}(0)x _ 3 ( 0 )exp [ «0t(Î©(x1,x2,x3))tsuperscriptsubscript0¡Î©subscript¥1subscript¥2subscript¥3differential-d¡ _ 0 ^ t ( roman_Î© ( x _ 1 , x _ 2 , x _ 3 ) ) d t ]. Hence, x1(t)>0subscript¥1¡0x_{1}(t)>0x _ 1 ( t ) > 0, x2(t)>0subscript¥2¡0x_{2}(t)>0x _ 2 ( t ) > 0, and x3(t)>0subscript¥3¡0x_{3}(t)>0x _ 3 ( t ) > 0. Hence, the theorem. All solutions of system (2) are bounded. To show the boundedness of all the solutions of the system (2), we define a function X(t)=x1(t)+x2(t)r3+x3(t)r3r5‹¡subscript¥1¡subscript¥2¡subscript3subscript¥3¡subscript3subscript5X(t)=x_{1}(t)+ ( t ) = x _ 1 ( t ) + / x _ 2 ( t ) r _ 3 + / x _ 3 ( t ) r _ 3 r _ 5 . Now, by differentiating X with respect to time t™ we get Now, using equations given in (2), we have Let VVV be any positive real number and V¤min(d1,d2)subscript1subscript2V min(d_{1},d_{2})V ¤ m i n ( d _ 1 , d _ 2 ). Then Now, by applying the notions of differential inequality, we obtain 0<X(t)¤Î©V(1eVt)+X(0)eVt0‹¡Î©1superscript¡‹0superscript¡0<X(t) < X ( t ) ¤ / roman_Î© V ( 1 - e ^ - V t ) + X ( 0 ) e ^ - V t . However, 0<X(t)¤Î©V0‹¡Î©0<X(t) < X ( t ) ¤ / roman_Î© V , when t††¡t † . Therefore, all the solutions of the system (3) are restricted to the region Î¶=((x1,x2,x3):0¤x1+x2r3+x3r3r5¤Î©V+Î,Î>0) x_{1}+ _{3}r_{5}} = ( ( x _ 1 , x _ 2 , x _ 3 ) : 0 ¤ x _ 1 + / x _ 2 r _ 3 + / x _ 3 r _ 3 r _ 5 ¤ / roman_Î© V + roman_Î , roman_Î > 0 ). The boundedness of the system (2) is consequently established. The solutions of the system (3) starting from (x1(0),x2(0),x3(0))R+3subscript¥10subscript¥20subscript¥30subscriptsuperscript…3(x_{1}(0),x_{2}(0),x_{3}(0)) R^{3}_{+}( x _ 1 ( 0 ) , x _ 2 ( 0 ) , x _ 3 ( 0 ) ) R ^ 3 _ + are non-negative and bounded in region D. To commence, let us demonstrate the non-negativity of the solutions x1(t)subscript¥1¡x_{1}(t)x _ 1 ( t ) of the system (3) i.e., x1(t)¥0subscript¥1¡0x_{1}(t) 0x _ 1 ( t ) ¥ 0, t¥t0for-all¡subscript¡0 t t_{0} t ¥ t _ 0 . To prove the non-negativity of the solutions of the system (3), we follow the approach given in [39]. Now, let us assume that the above inequality is not true, then ƒt>t0¡subscript¡0 t>t_{0}ƒ t > t _ 0 such that Now, using the first equation of the system (3) and the equation (6), we have By the use of Lemma (3), we find that This leads to a contradiction since x1(t1+)<0subscript¥1superscriptsubscript¡10x_{1}(t_{1}^{+})<0x _ 1 ( t _ 1 ^ + ) < 0. Thus, x1(t)¥0subscript¥1¡0x_{1}(t) 0x _ 1 ( t ) ¥ 0, t¥t0for-all¡subscript¡0 t t_{0} t ¥ t _ 0 . In a similar manner, we can ensure x2(t)¥0subscript¥2¡0x_{2}(t) 0x _ 2 ( t ) ¥ 0, x3(t)¥0,t¥t0formulae-sequencesubscript¥3¡0for-all¡subscript¡0x_{3}(t) 0, t t_{0}x _ 3 ( t ) ¥ 0 , t ¥ t _ 0 . Now, we have to prove the boundedness of the solutions of the system (3). To achieve this, we shall contemplate a function, W(t)=x1(t)+x2(t)r3+x3(t)r3r5Š¡subscript¥1¡subscript¥2¡subscript3subscript¥3¡subscript3subscript5W(t)=x_{1}(t)+ ( t ) = x _ 1 ( t ) + / x _ 2 ( t ) r _ 3 + / x _ 3 ( t ) r _ 3 r _ 5 . Thus, we have Now, considering any real number Î”Î” such that Let us consider, 0<Î”<min(d1,d2)0Î”subscript1subscript20< < roman_Î” < m i n ( d _ 1 , d _ 2 ) then we have, Now, utilising Lemma (1) provided in Section 3, we obtain where, EÎ±subscript¼E_{ _ Î± is the Mittag“Leffler function. Hence, Therefore, all the solutions of the system described in equation (3) originating in region R+3subscriptsuperscript…3R^{3}_{+}R ^ 3 _ + enters the region Î¥Î¥ defined by For nonnegative initial conditions, system (3) always exhibits unique solutions. We will utilise the methodology described in [53] in order to prove the existence of unique solutions of the system (3). Let us consider, the region Let, x=(x1,x2,x3)F¥subscript¥1subscript¥2subscript¥3¹x=(x_{1},x_{2},x_{3}) Fx = ( x _ 1 , x _ 2 , x _ 3 ) F and x¯=(x1¯,x2¯,x3¯)F¯¥¯subscript¥1¯subscript¥2¯subscript¥3¹ Fover¯ x = ( over¯ x _ 1 , over¯ x _ 2 , over¯ x _ 3 ) F. Now, let us consider, a function where, Now, we have we find, In a Similar way, we get From equations (7), (8), and (9), we get where, Y=max{y1,y2,y3}¥subscript¦1subscript¦2subscript¦3Y=max = m a x { y _ 1 , y _ 2 , y _ 3 } and y1=r1+r1|x1+x1¯|+r2(1m1)(1+r3)|x2¯|+r2(1m1)Î²(1+r3)|x1x2|+r2(1m1)Î²(1+r3)|x1¯x2|+|qr|subscript¦1subscript1subscript1subscript¥1¯subscript¥1subscript21subscript11subscript3¯subscript¥2subscript21subscript1½1subscript3subscript¥1subscript¥2subscript21subscript1½1subscript3¯subscript¥1subscript¥2y_{1}=r_{1}+r_{1}|x_{1}+ }(1-m_{1}) _{2}|+|qr|y _ 1 = r _ 1 + r _ 1 | x _ 1 + over¯ x _ 1 | + r _ 2 ( 1 - m _ 1 ) ( 1 + r _ 3 ) | over¯ x _ 2 | + r _ 2 ( 1 - m _ 1 ) Î² ( 1 + r _ 3 ) | x _ 1 x _ 2 | + r _ 2 ( 1 - m _ 1 ) Î² ( 1 + r _ 3 ) | over¯ x _ 1 x _ 2 | + | q r |, y2=r2(1m1)(1+r3)|x1|+r2(1m1)Î²(1+r3)|x2¯2|+r4(1m2)(1+r5)|x3¯(x2x2¯)(1+bx2)(1+bx2¯)|+|d1|subscript¦2subscript21subscript11subscript3subscript¥1subscript21subscript1½1subscript3superscript¯subscript¥22subscript41subscript21subscript5¯subscript¥3subscript¥2¯subscript¥21subscript¥21¯subscript¥2subscript1y_{2}=r_{2}(1-m_{1})(1+r_{3})|x_{1}|+r_{2}(1-m_{1}) {2}|+r_{4}(1-m_{2})(1+r_{5})| 1+b _ 2 = r _ 2 ( 1 - m _ 1 ) ( 1 + r _ 3 ) | x _ 1 | + r _ 2 ( 1 - m _ 1 ) Î² ( 1 + r _ 3 ) | over¯ x _ 2 ^ 2 | + r _ 4 ( 1 - m _ 2 ) ( 1 + r _ 5 ) | / over¯ x _ 3 ( x _ 2 - over¯ x _ 2 ) ( 1 + b x _ 2 ) ( 1 + b over¯ x _ 2 ) | + | d _ 1 |, and y3=r4(1m2)(1+r5)|x2(1+bx2)|+|d2|subscript¦3subscript41subscript21subscript5subscript¥21subscript¥2subscript2y_{3}=r_{4}(1-m_{2})(1+r_{5})| _ 3 = r _ 4 ( 1 - m _ 2 ) ( 1 + r _ 5 ) | / x _ 2 ( 1 + b x _ 2 ) | + | d _ 2 |. Therefore, the function G(x) fulfils the Lipschitz condition. Therefore, based on Lemma (2), it may be inferred that the system described in equation (3) has a unique solution inside the specified domain F. In the context of a predator-prey dynamical system, equilibrium points refer to specific values of the population sizes at which the rates of change of the populations cease to exist. These equilibrium points play a crucial role in understanding the dynamics of predator-prey interactions. These points delineate a state of equilibrium in which populations remain constant and exhibit no changes over time. A point (x1eq,x2eq,x3eq)superscriptsubscript¥1superscriptsubscript¥2superscriptsubscript¥3(x_{1}^{eq},x_{2}^{eq},x_{3}^{eq})( x _ 1 ^ e q , x _ 2 ^ e q , x _ 3 ^ e q ) is said to be an equilibrium point of the system (2) if they satisfy the following equations: Now, solving these equations we get four biologically feasible equilibrium points and these are given by Vanishing equilibrium point Ev(0,0,0)subscript£000E_{v}(0,0,0)E _ v ( 0 , 0 , 0 ), Axial equilibrium point Ea(1qrr1,0,0)subscript1subscript100E_{a}(1- _ a ( 1 - / q r r _ 1 , 0 , 0 ), Top predator free equilibrium point Et(A,B,0)subscript¡µ0E_{t}(A,B,0)E _ t ( A , B , 0 ), and Coexisting equilibrium point Ec(C,D,E)subscript¶·E_{c}(C,D,E)E _ c ( C , D , E ). Here, vanishing equilibrium point Ev(0,0,0)subscript£000E_{v}(0,0,0)E _ v ( 0 , 0 , 0 ) represents the situation in which all three species perish. Axial equilibrium point Ea(1qrr1,0,0)subscript1subscript100E_{a}(1- _ a ( 1 - / q r r _ 1 , 0 , 0 ) represents a situation in which only the prey persists and all the predators vanish. Top predator free equilibrium point Et(A,B,0)subscript¡µ0E_{t}(A,B,0)E _ t ( A , B , 0 ) represents the situation in which only the top predator vanishes while coexisting equilibrium point Ec(C,D,E)subscript¶·E_{c}(C,D,E)E _ c ( C , D , E ) signifies the situation in which all three species coexist. Morever, and Here, 1=(1+m1)r2r3(4Î²d1r2r3+m1r2r3)subscript”11subscript1subscript2subscript34½subscript1subscript2subscript3subscript1subscript2subscript3 d_{1}-r_{2}r_{3}+m_{1}r_{2}r_{3})} _ 1 = square-root ( - 1 + m _ 1 ) r _ 2 r _ 3 ( - 4 Î² d _ 1 - r _ 2 r _ 3 + m _ 1 r _ 2 r _ 3 ) , 2=(d1r12+(1+m1)(rqr1)(Î²rqr1Î²r1)r2r3)subscript”2subscript1superscriptsubscript121subscript1subscript1½subscript1½subscript1subscript2subscript3 rq-r_{1}- r_{1})r_{% 2}r_{3}) _ 2 = ( d _ 1 r _ 1 ^ 2 + ( - 1 + m _ 1 ) ( r q - r _ 1 ) ( Î² r q - r _ 1 - Î² r _ 1 ) r _ 2 r _ 3 ), 3=(d2(1+m1)2r22r3(1+m2)(d1r1(1+m1)(rqr1)r2r3)r4r5)subscript”3subscript2superscript1subscript12superscriptsubscript22subscript31subscript2subscript1subscript11subscript1subscript1subscript2subscript3subscript4subscript5 (rq-r_{1})r_{2}r_{3})r_{4}r_{5}) _ 3 = ( d _ 2 ( - 1 + m _ 1 ) ^ 2 r _ 2 ^ 2 r _ 3 - ( - 1 + m _ 2 ) ( d _ 1 r _ 1 - ( - 1 + m _ 1 ) ( r q - r _ 1 ) r _ 2 r _ 3 ) r _ 4 r _ 5 ), 4=(2d1d2r1+(rqr1)r3(d2(1+m1)r2+(1+m2)(rqr1)r4r5))subscript”42subscript1subscript2subscript1subscript1subscript3subscript21subscript1subscript21subscript2subscript1subscript4subscript5 rq-r_{1})r_{4}r_{5})) _ 4 = ( 2 d _ 1 d _ 2 r _ 1 + ( r q - r _ 1 ) r _ 3 ( d _ 2 ( - 1 + m _ 1 ) r _ 2 + ( - 1 + m _ 2 ) ( r q - r _ 1 ) r _ 4 r _ 5 ) ), 5=(r1(d2(1+m1)2r22r3+2(1+m2)(d1r1(1+m1)(rqr1)r2r3)r4r5)+Î²(1+m1)r2(2d1d2r1+(rqr1)r3(d2(1+m1)r2+2(1+m2)(rqr1)r4r5)))subscript”5subscript1subscript2superscript1subscript12superscriptsubscript22subscript321subscript2subscript1subscript11subscript1subscript1subscript2subscript3subscript4subscript5½1subscript1subscript22subscript1subscript2subscript1subscript1subscript3subscript21subscript1subscript221subscript21subscript4subscript5 1+m_{1})(rq-r_{1})r_{2}r_{3})r_{4}r_{5})+ +(rq-r_{1})r_{3}(d_{2}(-1+m_{1})r_{2}+2(-1+m_{2})(rq-r-1)r_{4}r_{5}))) _ 5 = ( r _ 1 ( - d _ 2 ( - 1 + m _ 1 ) ^ 2 r _ 2 ^ 2 r _ 3 + 2 ( - 1 + m _ 2 ) ( d _ 1 r _ 1 - ( - 1 + m _ 1 ) ( r q - r _ 1 ) r _ 2 r _ 3 ) r _ 4 r _ 5 ) + Î² ( - 1 + m _ 1 ) r _ 2 ( 2 d _ 1 d _ 2 r _ 1 + ( r q - r _ 1 ) r _ 3 ( d _ 2 ( - 1 + m _ 1 ) r _ 2 + 2 ( - 1 + m _ 2 ) ( r q - r - 1 ) r _ 4 r _ 5 ) ) ). Conditions of existence: Vanishing equilibrium point Ev(0,0,0)subscript£000E_{v}(0,0,0)E _ v ( 0 , 0 , 0 ) always exists. Axial equilibrium point Ea(1qrr1,0,0)subscript1subscript100E_{a}(1- _ a ( 1 - / q r r _ 1 , 0 , 0 ) exists iff r<r1qsubscript1r< < / r _ 1 q . Top predator free equilibrium point Et(A,B,0)subscript¡µ0E_{t}(A,B,0)E _ t ( A , B , 0 ) exists iff r<r1qsubscript1r< < / r _ 1 q and r2>d1r12Î²r2q2r3+6subscript2subscript1superscriptsubscript12½superscript2superscript2subscript3subscript”6r_{2}> r^{2}q^{2}r_{3}+ _ 2 > / - d _ 1 r _ 1 ^ 2 - Î² r ^ 2 q ^ 2 r _ 3 + _ 6 . Coexisting equilibrium point Ec(C,D,E)subscript¶·E_{c}(C,D,E)E _ c ( C , D , E ) exists iff r1>d2(1+m1)r2c1d2+(1+m2)r4r5subscript1subscript21subscript1subscript2subscript1subscript21subscript2subscript4subscript5r_{1}> _ 1 > / d _ 2 ( - 1 + m _ 1 ) r _ 2 c _ 1 d _ 2 + ( - 1 + m _ 2 ) r _ 4 r _ 5 , r3>d172(1+m1)8r29subscript3subscript1superscriptsubscript”721subscript1subscript”8subscript2subscript”9r_{3}>- _ 3 > - / d _ 1 _ 7 ^ 2 ( - 1 + m _ 1 ) _ 8 r _ 2 _ 9 , r4>c1d2r5m2r5subscript4subscript1subscript2subscript5subscript2subscript5r_{4}> _ 4 > / c _ 1 d _ 2 r _ 5 - m _ 2 r _ 5 , and r<c1d2r1+d2(r2m1r2)+(1+m2)r1r4r5q(c1d2+(1+m2)r4r5)subscript1subscript2subscript1subscript2subscript2subscript1subscript21subscript2subscript1subscript4subscript5subscript1subscript21subscript2subscript4subscript5r< _{1}d_{2}+(-1+m_{2})r_{4}r_{5})}r < / c _ 1 d _ 2 r _ 1 + d _ 2 ( r _ 2 - m _ 1 r _ 2 ) + ( - 1 + m _ 2 ) r _ 1 r _ 4 r _ 5 q ( c _ 1 d _ 2 + ( - 1 + m _ 2 ) r _ 4 r _ 5 ) . Here, 6=Î²r2m1q2r3+rqr1r3+2Î²rqr1r3rm1qr1r32Î²rm1qr1r3r12r3Î²r12r3+m1r12r3+Î²m1r12r3subscript”6½superscript2subscript1superscript2subscript3subscript1subscript32½subscript1subscript3subscript1subscript1subscript32½subscript1subscript1subscript3superscriptsubscript12subscript3½superscriptsubscript12subscript3subscript1superscriptsubscript12subscript3½subscript1superscriptsubscript12subscript3 r^{2}m_{1}q^{2}r_{3}+rqr_{1}r_{3}+2 rqr_{1}r_{3}-rm_{1}% qr_{1}r_{3}-2 rm_{1}qr_{1}r_{3}-r_{1}^{2}r_{3}- r_{1}^{2}r_{3}+m_{1}% r_{1}^{2}r_{3}+ m_{1}r_{1}^{2}r_{3} _ 6 = Î² r ^ 2 m _ 1 q ^ 2 r _ 3 + r q r _ 1 r _ 3 + 2 Î² r q r _ 1 r _ 3 - r m _ 1 q r _ 1 r _ 3 - 2 Î² r m _ 1 q r _ 1 r _ 3 - r _ 1 ^ 2 r _ 3 - Î² r _ 1 ^ 2 r _ 3 + m _ 1 r _ 1 ^ 2 r _ 3 + Î² m _ 1 r _ 1 ^ 2 r _ 3 , 7=(c1d2r1+Î²d2(1+m1)r2+(1+m2)r1r4r5)subscript”7subscript1subscript2subscript1½subscript21subscript1subscript21subscript2subscript1subscript4subscript5 d_{2}(-1+m_{1})r_{2}+(-1+m_{2})r_{1}r_{4}r_{% 5}) _ 7 = ( c _ 1 d _ 2 r _ 1 + Î² d _ 2 ( - 1 + m _ 1 ) r _ 2 + ( - 1 + m _ 2 ) r _ 1 r _ 4 r _ 5 ), 8=(Î²rqr1Î²r1)subscript”8½subscript1½subscript1 rq-r_{1}- r_{1}) _ 8 = ( Î² r q - r _ 1 - Î² r _ 1 ), 9=(c1d2+(1+m2)r4r5)(c1d2(rqr1)+d2(1+m1)r2+(1+m2)(rqr1)r4r5)subscript”9subscript1subscript21subscript2subscript4subscript5subscript1subscript2subscript1subscript21subscript1subscript21subscript2subscript1subscript4subscript5 1})r_{2}+(-1+m_{2})(rq-r_{1})r_{4}r_{5}) _ 9 = ( c _ 1 d _ 2 + ( - 1 + m _ 2 ) r _ 4 r _ 5 ) ( c _ 1 d _ 2 ( r q - r _ 1 ) + d _ 2 ( - 1 + m _ 1 ) r _ 2 + ( - 1 + m _ 2 ) ( r q - r _ 1 ) r _ 4 r _ 5 ). Local stability: The vanishing equilibrium point Ev(0,0,0)subscript£000E_{v}(0,0,0)E _ v ( 0 , 0 , 0 ) is locally stable iff r>r1qsubscript1r> > / r _ 1 q . The eigenvalues of the Jacobian matrix at Ev(0,0,0)subscript£000E_{v}(0,0,0)E _ v ( 0 , 0 , 0 ), denoted by Jvasubscript½£J_{va}J _ v a are d1subscript1-d_{1}- d _ 1 , d2subscript2-d_{2}- d _ 2 , and rq+r1subscript1-rq+r_{1}- r q + r _ 1 . The condition for all these eigenvalues to be negative is that r>r1qsubscript1r> > / r _ 1 q . Therefore, the result. The axial equilibrium point Ea(1qrr1,0,0)subscript1subscript100E_{a}(1- _ a ( 1 - / q r r _ 1 , 0 , 0 ) is locally stable iff r<r1qsubscript1r< < / r _ 1 q and r2<d1r12Î²r2q2r3+6subscript2subscript1superscriptsubscript12½superscript2superscript2subscript3subscript”6r_{2}< r^{2}q^{2}r_{3}+ _ 2 < / - d _ 1 r _ 1 ^ 2 - Î² r ^ 2 q ^ 2 r _ 3 + _ 6 . The eigenvalues of the Jacobian matrix Jaxsubscript½¥J_{ax}J _ a x around Ea(1qrr1,0,0)subscript1subscript100E_{a}(1- _ a ( 1 - / q r r _ 1 , 0 , 0 ) are d2subscript2-d_{2}- d _ 2 , rqr1subscript1rq-r_{1}r q - r _ 1 , and d1r12+Î²r2q2r2r3r26r12subscript1superscriptsubscript12½superscript2superscript2subscript2subscript3subscript2subscript”6superscriptsubscript12 r^{2}q^{2}r_{2}r_{3}-r_{2} - d _ 1 r _ 1 ^ 2 + Î² r ^ 2 q ^ 2 r _ 2 r _ 3 - r _ 2 _ 6 r _ 1 ^ 2 . Now, the axial equilibrium point Ea(1qrr1,0,0)subscript1subscript100E_{a}(1- _ a ( 1 - / q r r _ 1 , 0 , 0 ) is locally stable iff all the eigenvalues of Jaxsubscript½¥J_{ax}J _ a x are negative and all the eigenvalues of Jaxsubscript½¥J_{ax}J _ a x are negative iff r<r1qsubscript1r< < / r _ 1 q and r2<d1r12Î²r2q2r3+6subscript2subscript1superscriptsubscript12½superscript2superscript2subscript3subscript”6r_{2}< r^{2}q^{2}r_{3}+ _ 2 < / - d _ 1 r _ 1 ^ 2 - Î² r ^ 2 q ^ 2 r _ 3 + _ 6 . Hence, the proof. The top predator free equilibrium point Et(A,B,0)subscript¡µ0E_{t}(A,B,0)E _ t ( A , B , 0 ) is locally stable iff M1>0subscript10M_{1}>0M _ 1 > 0, M2>0subscript20M_{2}>0M _ 2 > 0, M3>0subscript30M_{3}>0M _ 3 > 0, and M1M2M3>0subscript1subscript2subscript30M_{1}M_{2}-M_{3}>0M _ 1 M _ 2 - M _ 3 > 0. The definitions of the symbols MisubscriptM_{i}M _ i are provided within the proof. The Routh-Hurwitz criterion is employed to assess the local stability of Et(A,B,0)subscript¡µ0E_{t}(A,B,0)E _ t ( A , B , 0 ). The Jacobian matrix of the system (2) around the top predator free equilibrium point Et(A,B,0)subscript¡µ0E_{t}(A,B,0)E _ t ( A , B , 0 ) is given by here, t11=(Î²rqr1Î²r1)(2Î²d1+r2r3m1r2r310)(2Î²2d1)subscript¡11½subscript1½subscript12½subscript1subscript2subscript3subscript1subscript2subscript3subscript”102superscript½2subscript1t_{11}= rq-r_{1}- r_{1})(2 d_{1}+r_{2}r_{3}-m_{1}r_{2}r_% {3}- _ 11 = / ( Î² r q - r _ 1 - Î² r _ 1 ) ( 2 Î² d _ 1 + r _ 2 r _ 3 - m _ 1 r _ 2 r _ 3 - _ 10 ) ( 2 Î² ^ 2 d _ 1 ) , t12=d1r3subscript¡12subscript1subscript3t_{12}=- _ 12 = - / d _ 1 r _ 3 , t21=10(r1((1+m1)r2r3+10)+Î²(2d1r1(rqr1)((1+m1)r2r3+10)))2Î²2d1(1+m1)r2subscript¡21subscript”10subscript11subscript1subscript2subscript3subscript”10½2subscript1subscript1subscript11subscript1subscript2subscript3subscript”102superscript½2subscript11subscript1subscript2t_{21}=- }r_{1}-(rq-r_{1})((-1+m_{1})r_{2}r_{3}+ })r_{2}}t _ 21 = - / _ 10 ( r _ 1 ( ( - 1 + m _ 1 ) r _ 2 r _ 3 + _ 10 ) + Î² ( - 2 d _ 1 r _ 1 - ( r q - r _ 1 ) ( ( - 1 + m _ 1 ) r _ 2 r _ 3 + _ 10 ) ) ) 2 Î² ^ 2 d _ 1 ( - 1 + m _ 1 ) r _ 2 , t23=(1+m2)(r1((1+m1)r2r3+10)+Î²(2d1r1+(rqr1)((1+m1)r2r3+10)))r42Î²2d1(1+m1)r2c1r1((1+m1)r2r3+10)+Î²c1(2d1r1+(rqr1)((1+m1)r2r3+10))subscript¡231subscript2subscript11subscript1subscript2subscript3subscript”10½2subscript1subscript1subscript11subscript1subscript2subscript3subscript”10subscript42superscript½2subscript11subscript1subscript2subscript1subscript11subscript1subscript2subscript3subscript”10½subscript12subscript1subscript1subscript11subscript1subscript2subscript3subscript”10t_{23}= _{1}+(rq-r_{1})((-1+m_{1})r_{2}r_{3}+ _{1})r_{2}-c_{1}r_{1}((-1+m_{1})r_{2}r_{3}+ c_{1}(2d_{1}r_{1% }+(rq-r_{1})((-1+m_{1})r_{2}r_{3}+ _ 23 = / ( - 1 + m _ 2 ) ( - r _ 1 ( ( - 1 + m _ 1 ) r _ 2 r _ 3 + _ 10 ) + Î² ( 2 d _ 1 r _ 1 + ( r q - r _ 1 ) ( ( - 1 + m _ 1 ) r _ 2 r _ 3 + _ 10 ) ) ) r _ 4 2 Î² ^ 2 d _ 1 ( - 1 + m _ 1 ) r _ 2 - c _ 1 r _ 1 ( ( - 1 + m _ 1 ) r _ 2 r _ 3 + _ 10 ) + Î² c _ 1 ( 2 d _ 1 r _ 1 + ( r q - r _ 1 ) ( ( - 1 + m _ 1 ) r _ 2 r _ 3 + _ 10 ) ) , t33=d2+((1+m2)(r1((1+m1)r2r3+10)+Î²(2d1r1(rqr1)((1+m1)r2r3+10)))r4r5)2Î²2d1(1+m1)r2c1r1((1+m1)r2r3+10)+Î²c1(2d1r1+(rqr1)((1+m1)r2r3+10))subscript¡33subscript21subscript2subscript11subscript1subscript2subscript3subscript”10½2subscript1subscript1subscript11subscript1subscript2subscript3subscript”10subscript4subscript52superscript½2subscript11subscript1subscript2subscript1subscript11subscript1subscript2subscript3subscript”10½subscript12subscript1subscript1subscript11subscript1subscript2subscript3subscript”10t_{33}=-d_{2}+ -2d_{1}r_{1}-(rq-r_{1})((-1+m_{1})r_{2}r_{3}+ ^{2}d_{1}(-1+m_{1})r_{2}-c_{1}r_{1}((-1+m_{1})r_{2}r_{3}+ c_% {1}(2d_{1}r_{1}+(rq-r_{1})((-1+m_{1})r_{2}r_{3}+ _ 33 = - d _ 2 + / ( ( - 1 + m _ 2 ) ( r _ 1 ( ( - 1 + m _ 1 ) r _ 2 r _ 3 + _ 10 ) + Î² ( - 2 d _ 1 r _ 1 - ( r q - r _ 1 ) ( ( - 1 + m _ 1 ) r _ 2 r _ 3 + _ 10 ) ) ) r _ 4 r _ 5 ) 2 Î² ^ 2 d _ 1 ( - 1 + m _ 1 ) r _ 2 - c _ 1 r _ 1 ( ( - 1 + m _ 1 ) r _ 2 r _ 3 + _ 10 ) + Î² c _ 1 ( 2 d _ 1 r _ 1 + ( r q - r _ 1 ) ( ( - 1 + m _ 1 ) r _ 2 r _ 3 + _ 10 ) ) , and 10=(1+m1)r2r3(4Î²d1+(1+m1)r2r3)subscript”101subscript1subscript2subscript34½subscript11subscript1subscript2subscript3 d_{1}+(-1+m_{1})r_{2}r_{3})} _ 10 = square-root ( - 1 + m _ 1 ) r _ 2 r _ 3 ( - 4 Î² d _ 1 + ( - 1 + m _ 1 ) r _ 2 r _ 3 ) . Taking ƒ3+M1ƒ2+M2ƒ+M3=0superscript3subscript1superscript2subscript2subscript30 ^ 3 + M _ 1 ƒ ^ 2 + M _ 2 ƒ + M _ 3 = 0 as the characteristic equation of Jtosubscript½¡J_{to}J _ t o , then M1=(t11+t33)subscript1subscript¡11subscript¡33M_{1}=-(t_{11}+t_{33})M _ 1 = - ( t _ 11 + t _ 33 ), M2=(t12t21t11t33)subscript2subscript¡12subscript¡21subscript¡11subscript¡33M_{2}=-(t_{12}t_{21}-t_{11}t_{33})M _ 2 = - ( t _ 12 t _ 21 - t _ 11 t _ 33 ), and M3=t12t21t33subscript3subscript¡12subscript¡21subscript¡33M_{3}=t_{12}t_{21}t_{33}M _ 3 = t _ 12 t _ 21 t _ 33 . Now, Et(A,B,0)subscript¡µ0E_{t}(A,B,0)E _ t ( A , B , 0 ) is locally asymptotically stable iff M1>0subscript10M_{1}>0M _ 1 > 0, M2>0subscript20M_{2}>0M _ 2 > 0, M3>0subscript30M_{3}>0M _ 3 > 0, and M1M2M3>0subscript1subscript2subscript30M_{1}M_{2}-M_{3}>0M _ 1 M _ 2 - M _ 3 > 0. Hence, proved. The coexisting equilibrium point Ec(C,D,E)subscript¶·E_{c}(C,D,E)E _ c ( C , D , E ) is locally stable if and only if N1subscript1N_{1}N _ 1 , N2subscript2N_{2}N _ 2 , and N3subscript3N_{3}N _ 3 are all positive, and if N1N2N3subscript1subscript2subscript3N_{1}N_{2}-N_{3}N _ 1 N _ 2 - N _ 3 is also positive. The symbols NisubscriptN_{i}N _ i are defined directly in the proof. The Jacobian matrix of the system (2) evaluated at the coexisting equilibrium point Ec(C,D,E)subscript¶·E_{c}(C,D,E)E _ c ( C , D , E ) is provided as follows: here, u11=c1d2(qrr1)+d2(1+m1)r2+(1+m2)(qrr1)r4r5c1d2+(1+m2)r4r5subscript11subscript1subscript2subscript1subscript21subscript1subscript21subscript2subscript1subscript4subscript5subscript1subscript21subscript2subscript4subscript5u_{11}= 4}r_{5}}{c_{1}d_{2}+(-1+m_{2})r_{4}r_{5}}u _ 11 = / c _ 1 d _ 2 ( q r - r _ 1 ) + d _ 2 ( - 1 + m _ 1 ) r _ 2 + ( - 1 + m _ 2 ) ( q r - r _ 1 ) r _ 4 r _ 5 c _ 1 d _ 2 + ( - 1 + m _ 2 ) r _ 4 r _ 5 , u12=(1+m1)(Î²qrr1Î²r1)r2(c1d2+(1+m2)r4r5)(c1d2(qrr1)+d2(1+m1)r2+(1+m2)(qrr1)r4r5)(c1d2r1+Î²d2(1+m1)r2+(1+m2)r1r4r5)2subscript121subscript1½subscript1½subscript1subscript2subscript1subscript21subscript2subscript4subscript5subscript1subscript2subscript1subscript21subscript1subscript21subscript2subscript1subscript4subscript5superscriptsubscript1subscript2subscript1½subscript21subscript1subscript21subscript2subscript1subscript4subscript52u_{12}= qr-r_{1}- r_{1})r_{2}(c_{1}d_{2}+(-1+m_{2})% r_{4}r_{5})(c_{1}d_{2}(qr-r_{1})+d_{2}(-1+m_{1})r_{2}+(-1+m_{2})(qr-r_{1})r_{4% }r_{5})}{(c_{1}d_{2}r_{1}+ d_{2}(-1+m_{1})r_{2}+(-1+m_{2})r_{1}r_{4}r_{5}% )^{2}}u _ 12 = / ( - 1 + m _ 1 ) ( Î² q r - r _ 1 - Î² r _ 1 ) r _ 2 ( c _ 1 d _ 2 + ( - 1 + m _ 2 ) r _ 4 r _ 5 ) ( c _ 1 d _ 2 ( q r - r _ 1 ) + d _ 2 ( - 1 + m _ 1 ) r _ 2 + ( - 1 + m _ 2 ) ( q r - r _ 1 ) r _ 4 r _ 5 ) ( c _ 1 d _ 2 r _ 1 + Î² d _ 2 ( - 1 + m _ 1 ) r _ 2 + ( - 1 + m _ 2 ) r _ 1 r _ 4 r _ 5 ) ^ 2 , u21=d2(1+m1)r2r3(r1(c1d2+(1+m2)r4r5)+Î²(2c1d2(qrr1)+d2(1+m1)r2+2(1+m2)(qrr1)r4r5))(c1d2+(1+m2)r4r5)(c1d2r1+Î²d2(1+m1)r2+(1+m2)r1r4r5)subscript21subscript21subscript1subscript2subscript3subscript1subscript1subscript21subscript2subscript4subscript5½2subscript1subscript2subscript1subscript21subscript1subscript221subscript2subscript1subscript4subscript5subscript1subscript21subscript2subscript4subscript5subscript1subscript2subscript1½subscript21subscript1subscript21subscript2subscript1subscript4subscript5u_{21}=- )+ _{5}))}{(c_{1}d_{2}+(-1+m_{2})r_{4}r_{5})(c_{1}d_{2}r_{1}+ d_{2}(-1+m_{1}% )r_{2}+(-1+m_{2})r_{1}r_{4}r_{5})}u _ 21 = - / d _ 2 ( - 1 + m _ 1 ) r _ 2 r _ 3 ( - r _ 1 ( c _ 1 d _ 2 + ( - 1 + m _ 2 ) r _ 4 r _ 5 ) + Î² ( 2 c _ 1 d _ 2 ( q r - r _ 1 ) + d _ 2 ( - 1 + m _ 1 ) r _ 2 + 2 ( - 1 + m _ 2 ) ( q r - r _ 1 ) r _ 4 r _ 5 ) ) ( c _ 1 d _ 2 + ( - 1 + m _ 2 ) r _ 4 r _ 5 ) ( c _ 1 d _ 2 r _ 1 + Î² d _ 2 ( - 1 + m _ 1 ) r _ 2 + ( - 1 + m _ 2 ) r _ 1 r _ 4 r _ 5 ) , u22=c1d2(Î²2d1d22(1+m1)2r22+c12d2211(1+m2)r1r4r512+Î²(1+m1)(1+m2)r2r4r513+c1d214)((1+m2)r4r5(c1d2r1+Î²d2(1+m1)r2+(1+m2)r1r4r5)2)subscript22subscript1subscript2superscript½2subscript1superscriptsubscript22superscript1subscript12superscriptsubscript22superscriptsubscript12superscriptsubscript22subscript”111subscript2subscript1subscript4subscript5subscript”12½1subscript11subscript2subscript2subscript4subscript5subscript”13subscript1subscript2subscript”141subscript2subscript4subscript5superscriptsubscript1subscript2subscript1½subscript21subscript1subscript21subscript2subscript1subscript4subscript52u_{22}= 2}d_{2}^{2} +m_{2})r_{2}r_{4}r_{5} }(c_{1}d_{2}r_{1}+ d_{2}(-1+m_{1})r_{2}+(-1+m_{2})r_{1}r_{4}r_{5})^{2})}u _ 22 = / c _ 1 d _ 2 ( Î² ^ 2 d _ 1 d _ 2 ^ 2 ( - 1 + m _ 1 ) ^ 2 r _ 2 ^ 2 + c _ 1 ^ 2 d _ 2 ^ 2 _ 11 - ( - 1 + m _ 2 ) r _ 1 r _ 4 r _ 5 _ 12 + Î² ( - 1 + m _ 1 ) ( - 1 + m _ 2 ) r _ 2 r _ 4 r _ 5 _ 13 + c _ 1 d _ 2 _ 14 ) ( ( - 1 + m _ 2 ) r _ 4 r _ 5 ( c _ 1 d _ 2 r _ 1 + Î² d _ 2 ( - 1 + m _ 1 ) r _ 2 + ( - 1 + m _ 2 ) r _ 1 r _ 4 r _ 5 ) ^ 2 ) , u23=d2r5subscript23subscript2subscript5u_{23}=- _ 23 = - / d _ 2 r _ 5 , u32=(1m2)(c1d2+(1+m2)r4r5)(Î²2d1d22(1+m1)2r22+c12d2211(1+m2)r1r4r512+Î²(1+m1)(1+m2)r2r4r513+c1d214)(1+m2)2r4(c1d2r1+Î²d2(1+m1)r2+(1+m2)r1r4r5)2subscript321subscript2subscript1subscript21subscript2subscript4subscript5superscript½2subscript1superscriptsubscript22superscript1subscript12superscriptsubscript22superscriptsubscript12superscriptsubscript22subscript”111subscript2subscript1subscript4subscript5subscript”12½1subscript11subscript2subscript2subscript4subscript5subscript”13subscript1subscript2subscript”14superscript1subscript22subscript4superscriptsubscript1subscript2subscript1½subscript21subscript1subscript21subscript2subscript1subscript4subscript52u_{32}= }(-1+m_{1})^{2}r_{2}^{2}+c_{1}^{2}d_{2}^{2} 5} d_{2}(-1+m_{1})r_{2}+(% -1+m_{2})r_{1}r_{4}r_{5})^{2}}u _ 32 = / ( 1 - m _ 2 ) ( c _ 1 d _ 2 + ( - 1 + m _ 2 ) r _ 4 r _ 5 ) ( Î² ^ 2 d _ 1 d _ 2 ^ 2 ( - 1 + m _ 1 ) ^ 2 r _ 2 ^ 2 + c _ 1 ^ 2 d _ 2 ^ 2 _ 11 - ( - 1 + m _ 2 ) r _ 1 r _ 4 r _ 5 _ 12 + Î² ( - 1 + m _ 1 ) ( - 1 + m _ 2 ) r _ 2 r _ 4 r _ 5 _ 13 + c _ 1 d _ 2 _ 14 ) ( - 1 + m _ 2 ) ^ 2 r _ 4 ( c _ 1 d _ 2 r _ 1 + Î² d _ 2 ( - 1 + m _ 1 ) r _ 2 + ( - 1 + m _ 2 ) r _ 1 r _ 4 r _ 5 ) ^ 2 , and 11=(d1r12+(1+m1)(qrr1)(Î²qrr1Î²r1)r2r3)subscript”11subscript1superscriptsubscript121subscript1subscript1½subscript1½subscript1subscript2subscript3 qr-r_{1}- r_{1})r_% {2}r_{3}) _ 11 = ( d _ 1 r _ 1 ^ 2 + ( - 1 + m _ 1 ) ( q r - r _ 1 ) ( Î² q r - r _ 1 - Î² r _ 1 ) r _ 2 r _ 3 ), 12=(d2(1+m1)2r22r3(1+m2)(d1r1(1+m1)(qrr1)r2r3)r4r5)subscript”12subscript2superscript1subscript12superscriptsubscript22subscript31subscript2subscript1subscript11subscript1subscript1subscript2subscript3subscript4subscript5 )(qr-r_{1})r_{2}r_{3})r_{4}r_{5}) _ 12 = ( d _ 2 ( - 1 + m _ 1 ) ^ 2 r _ 2 ^ 2 r _ 3 - ( - 1 + m _ 2 ) ( d _ 1 r _ 1 - ( - 1 + m _ 1 ) ( q r - r _ 1 ) r _ 2 r _ 3 ) r _ 4 r _ 5 ), 13=(2d1d2r1+(qrr1)r3(d2(1+m1)r2+(1+m2)(qrr1)r4r5))subscript”132subscript1subscript2subscript1subscript1subscript3subscript21subscript1subscript21subscript2subscript1subscript4subscript5 qr-r_{1})r_{4}r_{5})) _ 13 = ( 2 d _ 1 d _ 2 r _ 1 + ( q r - r _ 1 ) r _ 3 ( d _ 2 ( - 1 + m _ 1 ) r _ 2 + ( - 1 + m _ 2 ) ( q r - r _ 1 ) r _ 4 r _ 5 ) ), 14=(r1(d2(1+m1)2r22r3+2(1+m2)(d1r1(1+m1)(qrr1)r2r3)r4r5)+Î²(1+m1)r2(2d1d2r1+(qrr1)r3(d2(1+m1)r2+2(1+m2)(qrr1)r4r5)))subscript”14subscript1subscript2superscript1subscript12superscriptsubscript22subscript321subscript2subscript1subscript11subscript1subscript1subscript2subscript3subscript4subscript5½1subscript1subscript22subscript1subscript2subscript1subscript1subscript3subscript21subscript1subscript221subscript2subscript1subscript4subscript5 -1+m_{1})(qr-r_{1})r_{2}r_{3})r_{4}r_{5})+ }+(qr-r_{1})r_{3}(d_{2}(-1+m_{1})r_{2}+2(-1+m_{2})(qr-r_{1})r_{4}r_{5}))) _ 14 = ( r _ 1 ( - d _ 2 ( - 1 + m _ 1 ) ^ 2 r _ 2 ^ 2 r _ 3 + 2 ( - 1 + m _ 2 ) ( d _ 1 r _ 1 - ( - 1 + m _ 1 ) ( q r - r _ 1 ) r _ 2 r _ 3 ) r _ 4 r _ 5 ) + Î² ( - 1 + m _ 1 ) r _ 2 ( 2 d _ 1 d _ 2 r _ 1 + ( q r - r _ 1 ) r _ 3 ( d _ 2 ( - 1 + m _ 1 ) r _ 2 + 2 ( - 1 + m _ 2 ) ( q r - r _ 1 ) r _ 4 r _ 5 ) ) ). Now, let us consider 3+N12+N2+N3=0superscriptitalic-3subscript1superscriptitalic-2subscript2italic-subscript30 ^ 3 + N _ 1 ^ 2 + N _ 2 + N _ 3 = 0 be the characteristic equation of Jcosubscript½J_{co}J _ c o . Then, N1=(u11+u22)subscript1subscript11subscript22N_{1}=-(u_{11}+u_{22})N _ 1 = - ( u _ 11 + u _ 22 ), N2=u12u21+u11u22u23u32subscript2subscript12subscript21subscript11subscript22subscript23subscript32N_{2}=-u_{12}u_{21}+u_{11}u_{22}-u_{23}u_{32}N _ 2 = - u _ 12 u _ 21 + u _ 11 u _ 22 - u _ 23 u _ 32 , and N3=u11u23u32subscript3subscript11subscript23subscript32N_{3}=u_{11}u_{23}u_{32}N _ 3 = u _ 11 u _ 23 u _ 32 . Thus, the coexisting equilibrium point Ec(C,D,E)subscript¶·E_{c}(C,D,E)E _ c ( C , D , E ) is locally stable if and only if Ni>0subscript0N_{i}>0N _ i > 0 and N1N2N3>0subscript1subscript2subscript30N_{1}N_{2}-N_{3}>0N _ 1 N _ 2 - N _ 3 > 0 for i=1,2,3, according to the Routh-Hurwitz criterion. Therefore, the proof. The equilibrium points of both the ODE system (2) and the FODE system(3) remain unchanged. Therefore, the criterion for the existence of all equilibrium points in system (3) is identical to that of system (2). Thus, for the existence of the equilibrium points of the bio-system (3), one can refer to Subsection (6.1). So, in this subsection, we discuss the local stability conditions of the equilibrium points of the bio-system (3). The subsequent theorems discuss the local stability criteria of all equilibrium points. The vanishing equilibrium point Ev(0,0,0)subscript£000E_{v}(0,0,0)E _ v ( 0 , 0 , 0 ) is locally stable if r>r1qsubscript1r> > / r _ 1 q . To find the local stability criterion of the vanishing equilibrium point Ev(0,0,0)subscript£000E_{v}(0,0,0)E _ v ( 0 , 0 , 0 ), we have to compute the Jacobian matrix of the system (3) at Ev(0,0,0)subscript£000E_{v}(0,0,0)E _ v ( 0 , 0 , 0 ) The diagonal elements of the Jacobian matrix Jvasubscript½£J_{va}J _ v a are the eigenvalues of Jvasubscript½£J_{va}J _ v a . The entries rq+r1,d1,subscript1subscript1-rq+r_{1},-d_{1},- r q + r _ 1 , - d _ 1 , and d2subscript2-d_{2}- d _ 2 are the eigenvalues of Jvasubscript½£J_{va}J _ v a . Now, utilising Lemma (4), we get the vanishing equilibrium point Ev(0,0,0)subscript£000E_{v}(0,0,0)E _ v ( 0 , 0 , 0 ) is locally stable if r>r1qsubscript1r> > / r _ 1 q . Hence, the theorem. The axial equilibrium point Ea(1qrr1,0,0)subscript1subscript100E_{a}(1- _ a ( 1 - / q r r _ 1 , 0 , 0 ) is locally stable if the conditions ( r<r1qsubscript1r< < / r _ 1 q and ( r2<d1r12Î²r2q2r3+6subscript2subscript1superscriptsubscript12½superscript2superscript2subscript3subscript”6r_{2}< r^{2}q^{2}r_{3}+ _ 2 < / - d _ 1 r _ 1 ^ 2 - Î² r ^ 2 q ^ 2 r _ 3 + _ 6 hold. The Jacobian matrix of the system (3) around Ea(1qrr1,0,0)subscript1subscript100E_{a}(1- _ a ( 1 - / q r r _ 1 , 0 , 0 ) is as follows: Obviously, the diagonal elements Î¾a,1=rqr1subscript1subscript1 _ a , 1 = r q - r _ 1 , Î¾a,2=d1r12+Î²r2q2r2r3r26r12subscript2subscript1superscriptsubscript12½superscript2superscript2subscript2subscript3subscript2subscript”6superscriptsubscript12 r^{2}q^{2}r_{2}r_{3}-r_{2} {1}^{2}}Î¾ _ a , 2 = / - d _ 1 r _ 1 ^ 2 + Î² r ^ 2 q ^ 2 r _ 2 r _ 3 - r _ 2 _ 6 r _ 1 ^ 2 , and Î¾a,3=d2subscript3subscript2 _ a , 3 = - d _ 2 are the eigenvalues of the Jacobian matrix Jvasubscript½£J_{va}J _ v a . Now, to study the local stability of the system (3) around the equilibrium point EasubscriptE_{a}E _ a , we utilise Lemma (4). Here, |arg(Î¾a,1)|=>Î³2”subscript1‹¾‹2|arg( a r g ( Î¾ _ a , 1 ) | = > / Î³ 2 if r<r1qsubscript1r< < / r _ 1 q , |arg(Î¾a,2)|=>Î³2”subscript2‹¾‹2|arg( a r g ( Î¾ _ a , 2 ) | = > / Î³ 2 if r2<d1r12Î²r2q2r3+6subscript2subscript1superscriptsubscript12½superscript2superscript2subscript3subscript”6r_{2}< r^{2}q^{2}r_{3}+ _ 2 < / - d _ 1 r _ 1 ^ 2 - Î² r ^ 2 q ^ 2 r _ 3 + _ 6 , and |argÎ¾a,3)|=>Î³2|arg a r g Î¾ _ a , 3 ) | = > / Î³ 2 . Hence, the system (3) is locally asymptotically stable around the axial equilibrium point EasubscriptE_{a}E _ a if the conditions ( r<r1qsubscript1r< < / r _ 1 q and ( r2<d1r12Î²r2q2r3+6subscript2subscript1superscriptsubscript12½superscript2superscript2subscript3subscript”6r_{2}< r^{2}q^{2}r_{3}+ _ 2 < / - d _ 1 r _ 1 ^ 2 - Î² r ^ 2 q ^ 2 r _ 3 + _ 6 hold. The system (3) shows local stability around Et(A,B,0)subscript¡µ0E_{t}(A,B,0)E _ t ( A , B , 0 ) is locally stable if the criteria ( t33<0subscript¡330t_{33}<0t _ 33 < 0, ( t112+4t12t21<0superscriptsubscript¡1124subscript¡12subscript¡210t_{11}^{2}+4t_{12}t_{21}<0t _ 11 ^ 2 + 4 t _ 12 t _ 21 < 0, and ( t11<0subscript¡110t_{11}<0t _ 11 < 0 hold. The Jacobian matrix of the system (3) around the top predator free equilibrium point Et(A,B,0)subscript¡µ0E_{t}(A,B,0)E _ t ( A , B , 0 ) is Jtosubscript½¡J_{to}J _ t o , already given in subsection (6.1). The characteristic equation of Jtosubscript½¡J_{to}J _ t o can be written as where C1=t11subscript¶1subscript¡11C_{1}=t_{11}C _ 1 = t _ 11 and C2=t12t21subscript¶2subscript¡12subscript¡21C_{2}=-t_{12}t_{21}C _ 2 = - t _ 12 t _ 21 . Let us consider the three eigenvalues of Jtosubscript½¡J_{to}J _ t o : Î¾t,1subscript¡1 _ t , 1 , Î¾t,2subscript¡2 _ t , 2 , and Î¾t,3subscript¡3 _ t , 3 . Let the equation t33Î½subscript¡33t_{33}- _ 33 - Î½ give Î¾t,1subscript¡1 _ t , 1 , and from the equation Î½2C1Î½+C2superscript2superscriptsubscript¶1superscriptsubscript¶2 ^ 2 - C _ 1 ^ Î½ + C _ 2 ^ , we get Î¾t,2subscript¡2 _ t , 2 and Î¾t,3subscript¡3 _ t , 3 . Now, according to Lemma (4), the system (3) shows local stability around the equilibrium point Etsubscript¡E_{t}E _ t if |arg(Î¾t,1)|=>Î³2”subscript¡1‹¾‹2|arg( a r g ( Î¾ _ t , 1 ) | = > / Î³ 2 , |arg(Î¾t,2)|=>Î³2”subscript¡2‹¾‹2|arg( a r g ( Î¾ _ t , 2 ) | = > / Î³ 2 , and |arg(Î¾t,3)|=>Î³2”subscript¡3‹¾‹2|arg( a r g ( Î¾ _ t , 3 ) | = > / Î³ 2 . If the conditions ( t33<0subscript¡330t_{33}<0t _ 33 < 0, ( t112+4t12t21<0superscriptsubscript¡1124subscript¡12subscript¡210t_{11}^{2}+4t_{12}t_{21}<0t _ 11 ^ 2 + 4 t _ 12 t _ 21 < 0, and ( t11<0subscript¡110t_{11}<0t _ 11 < 0 satisfies, then |arg(Î¾t,1)|=>Î³2”subscript¡1‹¾‹2|arg( a r g ( Î¾ _ t , 1 ) | = > / Î³ 2 , |arg(Î¾t,2)|=>Î³2”subscript¡2‹¾‹2|arg( a r g ( Î¾ _ t , 2 ) | = > / Î³ 2 , and |arg(Î¾t,3)|=>Î³2”subscript¡3‹¾‹2|arg( a r g ( Î¾ _ t , 3 ) | = > / Î³ 2 . Thus, the system (3) shows local stability around the equilibrium point Etsubscript¡E_{t}E _ t if the criteria ( t33<0subscript¡330t_{33}<0t _ 33 < 0, ( t112+4t12t21<0superscriptsubscript¡1124subscript¡12subscript¡210t_{11}^{2}+4t_{12}t_{21}<0t _ 11 ^ 2 + 4 t _ 12 t _ 21 < 0, and ( t11<0subscript¡110t_{11}<0t _ 11 < 0 hold. Hence, the theorem. If any of the criteria ( ( and ( specified in the proof are satisfied, the coexisting equilibrium point Ec(C,D,E)subscript¶·E_{c}(C,D,E)E _ c ( C , D , E ) is locally asymptotically stable. In order to determine the local stability conditions of the system (3) in the vicinity of the coexistence equilibrium point Ec(C,D,E)subscript¶·E_{c}(C,D,E)E _ c ( C , D , E ), it is necessary to calculate the Jacobian matrix of the system (3) around the coexistence equilibrium point EcsubscriptE_{c}E _ c . The Jacobian matrix of the system (3) around the coexistence equilibrium point Ec(C,D,E)subscript¶·E_{c}(C,D,E)E _ c ( C , D , E ) is denoted as Jcosubscript½J_{co}J _ c o , as previously provided in subsection (6.1). Now, let us consider, ±=3+N12+N2+N3italic-±superscriptitalic-3subscript1superscriptitalic-2subscript2italic-subscript3 = ^ 3 + N _ 1 ^ 2 + N _ 2 + N _ 3 . The establishment of local asymptotic stability for the equilibrium point Ec(C,D,E)subscript¶·E_{c}(C,D,E)E _ c ( C , D , E ) in the system (3) is contingent upon the satisfaction of any of the following conditions [54]: ( If Î”(±)>0Î”italic-±0 ( ± ) > 0, N1>0subscript10N_{1}>0N _ 1 > 0, N3>0subscript30N_{3}>0N _ 3 > 0, and N1N2N3>0subscript1subscript2subscript30N_{1}N_{2}-N_{3}>0N _ 1 N _ 2 - N _ 3 > 0. ( If Î”(±)<0Î”italic-±0 ( ± ) < 0, then N1¥0subscript10N_{1} 0N _ 1 ¥ 0, N2¥0subscript20N_{2} 0N _ 2 ¥ 0, N3>0subscript30N_{3}>0N _ 3 > 0, and Î³<23¾23 < / 2 3 . ( If Î”(±)<0Î”italic-±0 ( ± ) < 0, N1>0subscript10N_{1}>0N _ 1 > 0, N2>0subscript20N_{2}>0N _ 2 > 0, N1N2=N3subscript1subscript2subscript3N_{1}N_{2}=N_{3}N _ 1 N _ 2 = N _ 3 , and Î³(0,1)¾01 ( 0 , 1 ). Here, Î”(±)=18N1N2N3+(N1N2)24(N1)2N34(N2)227(N3)2Î”italic-±18subscript1subscript2subscript3superscriptsubscript1subscript224superscriptsubscript12subscript34superscriptsubscript2227superscriptsubscript32 {2}-27(N_{3})^{2}roman_Î” ( ± ) = 18 N _ 1 N _ 2 N _ 3 + ( N _ 1 N _ 2 ) ^ 2 - 4 ( N _ 1 ) ^ 2 N _ 3 - 4 ( N _ 2 ) ^ 2 - 27 ( N _ 3 ) ^ 2 . In a subsequent section, the numerical verification of the local stability of Ec(C,D,E)subscript¶·E_{c}(C,D,E)E _ c ( C , D , E ) will be conducted. Bifurcation in ecology denotes a substantial modification in the configuration or conduct of an ecological system resulting from variations in factors. The bifurcation point refers to the precise instant when a system undergoes a change from one stable state to another, possibly leading to different population densities. Bifurcations can have substantial consequences for the management and conservation of ecosystems. Transcritical bifurcation is a phenomenon in dynamical systems when the stability of two equilibrium points, one stable and one unstable, switch as a parameter is changed. This phenomena is of great significance when it comes to comprehending diverse ecological and biological systems, as it can elucidate changes in population dynamics, species interactions, and ecosystem states. Theorems pertaining to this can be found below. The system (2) undergoes a transcritical bifurcation near the equilibrium point EasubscriptE_{a}E _ a at the critical value r1subscript1r_{1}r _ 1 =qr=r1tbpsuperscriptsubscript1¡qr=r_{1}^{tbp}q r = r _ 1 ^ t b p . The Jacobian matrix of the system (2) around EasubscriptE_{a}E _ a is Jaxsubscript½¥J_{ax}J _ a x as mentioned in the previous section. Using r1=qr=r1tbpsubscript1superscriptsubscript1¡r_{1}=qr=r_{1}^{tbp}r _ 1 = q r = r _ 1 ^ t b p , we get Now, we assume that the zero eigenvalues of (Jax)r1=r1tbpsubscriptsubscript½¥subscript1superscriptsubscript1¡(J_{ax})_{r_{1}=r_{1}^{tbp}}( J _ a x ) _ r _ 1 = r _ 1 ^ t b p and (Jax)r1=r1tbptsubscriptsuperscriptsubscript½¥¡subscript1superscriptsubscript1¡(J_{ax})^{t}_{r_{1}=r_{1}^{tbp}}( J _ a x ) ^ t _ r _ 1 = r _ 1 ^ t b p correspond to two eigenvectors, U1subscript1U_{1}U _ 1 and U2subscript2U_{2}U _ 2 , respectively. After some computation, we get U1=(u11,u12,u13)t=(1,0,0)tsubscript1superscriptsubscript11subscript12subscript13¡superscript100¡U_{1}=(u_{11},u_{12},u_{13})^{t}=(1,0,0)^{t}U _ 1 = ( u _ 11 , u _ 12 , u _ 13 ) ^ t = ( 1 , 0 , 0 ) ^ t and U2=(u21,u22,u23)t=(1,0,0)tsubscript2superscriptsubscript21subscript22subscript23¡superscript100¡U_{2}=(u_{21},u_{22},u_{23})^{t}=(1,0,0)^{t}U _ 2 = ( u _ 21 , u _ 22 , u _ 23 ) ^ t = ( 1 , 0 , 0 ) ^ t . Now, the theorem put forward by Sotomayor [55] is used to demonstrate the occurrence of a transcritical bifurcation at r1=qr=r1tbpsubscript1superscriptsubscript1¡r_{1}=qr=r_{1}^{tbp}r _ 1 = q r = r _ 1 ^ t b p in the vicinity of EasubscriptE_{a}E _ a . Outlined below are the prerequisites for transcritical bifurcation, as stated in Sotomayor™s theorem [55]. Zr1(Ea;r1tbp)=[000],D(Zr1(E1;r1tbp))U1=[qr00000000][100]=[qr00],formulae-sequencesubscriptsubscript1subscriptsuperscriptsubscript1¡matrix000·subscriptsubscript1subscript1superscriptsubscript1¡subscript1matrix00000000matrix100matrix00Z_{r_{1}}(E_{a};r_{1}^{tbp})= 0 0 0&0&0 0&0&0 0 0 0 0 _ r _ 1 ( E _ a ; r _ 1 ^ t b p ) = [ start_ROW start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL end_ROW ] , D ( Z _ r _ 1 ( E _ 1 ; r _ 1 ^ t b p ) ) U _ 1 = [ start_ROW start_CELL q r end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW ] [ start_ROW start_CELL 1 end_CELL end_ROW start_ROW start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL end_ROW ] = [ start_ROW start_CELL q r end_CELL end_ROW start_ROW start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL end_ROW ] , and D2(Zr1(E1;r1tbp))(U1,U1)=[2r1+2(1+m1)r22(1+m1)r2r30]superscript·2subscriptsubscript1subscript1superscriptsubscript1¡subscript1subscript1matrix2subscript121subscript1subscript221subscript1subscript2subscript30D^{2}(Z_{r_{1}}(E_{1};r_{1}^{tbp}))(U_{1},U_{1})= _{1})r_{2} -2(-1+m_{1})r_{2}r_{3} 0 ^ 2 ( Z _ r _ 1 ( E _ 1 ; r _ 1 ^ t b p ) ) ( U _ 1 , U _ 1 ) = [ start_ROW start_CELL - 2 r _ 1 + 2 ( - 1 + m _ 1 ) r _ 2 end_CELL end_ROW start_ROW start_CELL - 2 ( - 1 + m _ 1 ) r _ 2 r _ 3 end_CELL end_ROW start_ROW start_CELL 0 end_CELL end_ROW ] . Therefore, Thus, the application of Sotomayor™s theorem [55] proves the presence of a transcritical bifurcation around EasubscriptE_{a}E _ a at r1subscript1r_{1}r _ 1 =qr=r1tbpsuperscriptsubscript1¡qr=r_{1}^{tbp}q r = r _ 1 ^ t b p . In addition, there are additional parameters that can be utilised as bifurcation parameters. The Hopf bifurcation is a well-known and extensively studied phenomenon in the field of dynamical systems. It has been observed in various domains, including ecological models, where it plays a crucial role in understanding the qualitative changes in system behaviour as a parameter is systematically varied. The comprehension of Hopf bifurcation in the field of ecology holds significant importance for ecologists, as it enables them to comprehend the inherent capacity for intricate, cyclical patterns in the dynamics of populations. The following theorem establishes the conditions for the existence of Hopf bifurcation in the system (2) around the interior equilibrium point EcsubscriptE_{c}E _ c . The necessary and sufficient criteria for the system (2) to experience a Hopf bifurcation at m1=m1hbsubscript1superscriptsubscript1m_{1}=m_{1}^{hb}m _ 1 = m _ 1 ^ h b are as follows: ( Ni(m1hb)>0subscriptsuperscriptsubscript10N_{i}(m_{1}^{hb})>0N _ i ( m _ 1 ^ h b ) > 0, i=1,2,3 ( N1(m1hb)N2(m1hb)=N3(m1hb)subscript1superscriptsubscript1subscript2superscriptsubscript1subscript3superscriptsubscript1N_{1}(m_{1}^{hb})N_{2}(m_{1}^{hb})=N_{3}(m_{1}^{hb})N _ 1 ( m _ 1 ^ h b ) N _ 2 ( m _ 1 ^ h b ) = N _ 3 ( m _ 1 ^ h b ), ( _ 1 ( m _ 1 ^ h b ) N2²superscriptsubscript2²N_{2}^{{}^{ _ 2 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT (m1hbsuperscriptsubscript1m_{1}^{hb}m _ 1 ^ h b ) +N2(m1hb)subscript2superscriptsubscript1N_{2}(m_{1}^{hb})N _ 2 ( m _ 1 ^ h b ) N1²superscriptsubscript1²N_{1}^{{}^{ _ 1 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT (m1hbsuperscriptsubscript1m_{1}^{hb}m _ 1 ^ h b ) -N3²superscriptsubscript3²N_{3}^{{}^{ _ 3 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT (m1hbsuperscriptsubscript1m_{1}^{hb}m _ 1 ^ h b ) 0absent0 0 0, j=1,2,3, here, N1subscript1N_{1}N _ 1 , N2subscript2N_{2}N _ 2 , and N3subscript3N_{3}N _ 3 are already defined in the theorem (8). Î1subscript¿1 _ 1 , Î2subscript¿2 _ 2 , and Î3subscript¿3 _ 3 are the roots of the characteristic equation of the Jacobian matrix Jcosubscript½J_{co}J _ c o . Taking m1=m1hbsubscript1superscriptsubscript1m_{1}=m_{1}^{hb}m _ 1 = m _ 1 ^ h b , the characteristic equation of the Jacobian matrix Jcosubscript½J_{co}J _ c o reduces to now, the roots of the equation (12) are Î1=N1subscript¿1subscript1 _ 1 = - N _ 1 and Î2,3=±iN2subscript¿23plus-or-minussubscript2 i _ 2 , 3 = ± i square-root N _ 2 . Let us consider, the roots of the characteristic equation for m1(m1hbµ,m1hb+µ)subscript1superscriptsubscript1italic-µsuperscriptsubscript1italic-µm_{1} _ 1 ( m _ 1 ^ h b - µ , m _ 1 ^ h b + µ ), µ>0italic-µ0 > 0 are Now, we need to confirm the transversality condition ( By substituting Î2(m1)=Î¦1(m1)+iÎ¦2(m1)subscript¿2subscript1subscriptÎ¦1subscript1subscriptÎ¦2subscript1 _ 2 ( m _ 1 ) = roman_Î¦ _ 1 ( m _ 1 ) + i roman_Î¦ _ 2 ( m _ 1 ) into equation (11)11 and subsequently differentiating and separating the real and imaginary parts, we obtain where, P(m1)=3Î¦12(m1)3Î¦22(m1)+N2(m1)+2N1(m1)Î¦1(m1)ƒsubscript13superscriptsubscriptÎ¦12subscript13superscriptsubscriptÎ¦22subscript1subscript2subscript12subscript1subscript1subscriptÎ¦1subscript1P(m_{1})=3 ( m _ 1 ) = 3 roman_Î¦ _ 1 ^ 2 ( m _ 1 ) - 3 roman_Î¦ _ 2 ^ 2 ( m _ 1 ) + N _ 2 ( m _ 1 ) + 2 N _ 1 ( m _ 1 ) roman_Î¦ _ 1 ( m _ 1 ) , Q(m1)=6Î¦1(m1)Î¦2(m1)+2N1(m1)Î¦2(m1)subscript16subscriptÎ¦1subscript1subscriptÎ¦2subscript12subscript1subscript1subscriptÎ¦2subscript1Q(m_{1})=6 ( m _ 1 ) = 6 roman_Î¦ _ 1 ( m _ 1 ) roman_Î¦ _ 2 ( m _ 1 ) + 2 N _ 1 ( m _ 1 ) roman_Î¦ _ 2 ( m _ 1 ) , and R(m1)=N1²(m1)Î¦12(m1)N1²(m1)Î¦22(m1)+N2²(m1)Î¦1(m1)+N3²(m1)…subscript1superscriptsubscript1²subscript1superscriptsubscriptÎ¦12subscript1superscriptsubscript1²subscript1superscriptsubscriptÎ¦22subscript1superscriptsubscript2²subscript1subscriptÎ¦1subscript1superscriptsubscript3²subscript1R(m_{1})=N_{1}^{{}^{ 1}) ( m _ 1 ) = N _ 1 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ) roman_Î¦ _ 1 ^ 2 ( m _ 1 ) - N _ 1 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ) roman_Î¦ _ 2 ^ 2 ( m _ 1 ) + N _ 2 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ) roman_Î¦ _ 1 ( m _ 1 ) + N _ 3 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ), S(m1)=N2²(m1)Î¦2(m1)+2Î¦1(m1)Î¦2(m1)†subscript1superscriptsubscript2²subscript1subscriptÎ¦2subscript12subscriptÎ¦1subscript1subscriptÎ¦2subscript1S(m_{1})=N_{2}^{{}^{ {1})S ( m _ 1 ) = N _ 2 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ) roman_Î¦ _ 2 ( m _ 1 ) + 2 roman_Î¦ _ 1 ( m _ 1 ) roman_Î¦ _ 2 ( m _ 1 ). Now, we get, P(m1hb)=2N2(m1hb)ƒsuperscriptsubscript12subscript2superscriptsubscript1P(m_{1}^{hb})=-2N_{2}(m_{1}^{hb})P ( m _ 1 ^ h b ) = - 2 N _ 2 ( m _ 1 ^ h b ) , Q(m1hb)=2N1(m1hb)N2(m1hb)superscriptsubscript12subscript1superscriptsubscript1subscript2superscriptsubscript1Q(m_{1}^{hb})=2N_{1}(m_{1}^{hb}) ( m _ 1 ^ h b ) = 2 N _ 1 ( m _ 1 ^ h b ) square-root N _ 2 ( m _ 1 ^ h b ) , and R(m1hb)=N3²(m1hb)N1²(m1hb)N2(m1hb)…superscriptsubscript1superscriptsubscript3²superscriptsubscript1superscriptsubscript1²superscriptsubscript1subscript2superscriptsubscript1R(m_{1}^{hb})=N_{3}^{{}^{ _{2}(m_{1}^{hb})R ( m _ 1 ^ h b ) = N _ 3 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ^ h b ) - N _ 1 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ^ h b ) N _ 2 ( m _ 1 ^ h b ) , S(m1hb)=N2(m1hb)N2²(m1hb)†superscriptsubscript1subscript2superscriptsubscript1superscriptsubscript2²superscriptsubscript1S(m_{1}^{hb})= ( m _ 1 ^ h b ) = square-root N _ 2 ( m _ 1 ^ h b ) N _ 2 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ^ h b ). Therefore, the following equation is obtained Now, the transversality condition is met when N1²N2N3²+N1N2 ²0N_{1}^{{}^{ 0N _ 1 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT N _ 2 - N _ 3 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT + N _ 1 N _ 2 start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT 0 and Î1(m1hb)=N1 0subscript¿1superscriptsubscript1subscript10 0Î _ 1 ( m _ 1 ^ h b ) = - N _ 1 0 hold true. This suggests that a Hopf bifurcation takes place at the equilibrium point EcsubscriptE_{c}E _ c when the value of m1subscript1m_{1}m _ 1 reaches the critical value m1hbsuperscriptsubscript1m_{1}^{hb}m _ 1 ^ h b . Therefore, the theorem follows. In the previous sections, we already established some analytical conclusions. In this section, we conduct a numerical investigation into the dynamics of the prey-predator systems (2) and (3) and substantiate our analytical conclusions by utilizing some hypothetical parameter values. The analytical results obtained in previous sections are further supported by the utilisation of numerous figures generated by the use of the Mathematica and MATLAB software packages. In this section, we employ numerical methods to confirm the local stability of the equilibrium points of the integer order system (2) and the fractional order system (3) discussed in the preceding sections. At first, we will examine the local stability criteria of the equilibrium points of the system specified in the equation (2). In order to achieve this objective, we will examine some theoretical values of parameters. To verify the local stability of the vanishing equilibrium point Evsubscript£E_{v}E _ v , we consider the parameter values: r1=0.46subscript10.46r_{1}=0.46r _ 1 = 0.46, r2=0.32subscript20.32r_{2}=0.32r _ 2 = 0.32, r3=0.5subscript30.5r_{3}=0.5r _ 3 = 0.5, r4=0.002subscript40.002r_{4}=0.002r _ 4 = 0.002, r5=0.38subscript50.38r_{5}=0.38r _ 5 = 0.38, Î²=4.047½4.047 = 4.047, m1=0.72subscript10.72m_{1}=0.72m _ 1 = 0.72, m2=0.17subscript20.17m_{2}=0.17m _ 2 = 0.17, d1=0.096subscript10.096d_{1}=0.096d _ 1 = 0.096, d2=0.279subscript20.279d_{2}=0.279d _ 2 = 0.279, c1=3.33subscript13.33c_{1}=3.33c _ 1 = 3.33, q=1.351.35q=1.35q = 1.35, and r=0.380.38r=0.38r = 0.38. The eigenvalues of the Jacobian matrix Jvasubscript½£J_{va}J _ v a of the system (2) around the vanishing equilibrium point Evsubscript£E_{v}E _ v are 0.096<00.0960-0.096<0- 0.096 < 0, 0.279<00.2790-0.279<0- 0.279 < 0, and 0.053<00.0530-0.053<0- 0.053 < 0 for these parameter values. Therefore, the vanishing equilibrium point Evsubscript£E_{v}E _ v is locally asymptotically stable, as depicted in figure (3). In addition, the numerical conditions necessary for the local stability of the equilibrium point Evsubscript£E_{v}E _ v , as stated in theorem (5), are also met, with r1q=0.34<rsubscript10.34 r _ 1 q = 0.34 < r. By reducing the value of the parameter qqq i.e., considering the following parameter values: r1=0.46subscript10.46r_{1}=0.46r _ 1 = 0.46, r2=0.32subscript20.32r_{2}=0.32r _ 2 = 0.32, r3=0.5subscript30.5r_{3}=0.5r _ 3 = 0.5, r4=0.002subscript40.002r_{4}=0.002r _ 4 = 0.002, r5=0.38subscript50.38r_{5}=0.38r _ 5 = 0.38, Î²=4.047½4.047 = 4.047, m1=0.72subscript10.72m_{1}=0.72m _ 1 = 0.72, m2=0.17subscript20.17m_{2}=0.17m _ 2 = 0.17, d1=0.096subscript10.096d_{1}=0.096d _ 1 = 0.096, d2=0.279subscript20.279d_{2}=0.279d _ 2 = 0.279, c1=3.33subscript13.33c_{1}=3.33c _ 1 = 3.33, q=0.8380.838q=0.838q = 0.838, and r=0.380.38r=0.38r = 0.38, we can establish the local stability of the axial equilibrium point EasubscriptE_{a}E _ a . The eigenvalues of the Jacobian matrix Jaxsubscript½¥J_{ax}J _ a x of the system (2) around the axial equilibrium point EasubscriptE_{a}E _ a are -0.279, -0.141341, and -0.0651173. Since all of the eigenvalues are negative, the local stability of the axial equilibrium point is established. Furthermore, it meets all the numerical requirements stated in theorem (6). Figure (3) provides visual evidence of the local stability of the axial equilibrium point EasubscriptE_{a}E _ a . On further reduction of the parameter qqq to q=0.0190.019q=0.019q = 0.019 and keeping the other parameter values the same as previously mentioned, we obtain that M1=0.682878>0subscript10.6828780M_{1}=0.682878>0M _ 1 = 0.682878 > 0, M2=0.140642>0subscript20.1406420M_{2}=0.140642>0M _ 2 = 0.140642 > 0, M3=0.00780181>0subscript30.007801810M_{3}=0.00780181>0M _ 3 = 0.00780181 > 0, and M1M2M3=0.0882396>0subscript1subscript2subscript30.08823960M_{1}M_{2}-M_{3}=0.0882396>0M _ 1 M _ 2 - M _ 3 = 0.0882396 > 0. Based on the theorem (7), it can be inferred that the top predator free equilibrium point Etsubscript¡E_{t}E _ t is locally stable. The precise depiction can be found in figure (3). Now, in order to establish the local stability of the coexisting equilibrium point EcsubscriptE_{c}E _ c , we examine the parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r3=1subscript31r_{3}=1r _ 3 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, r2=1subscript21r_{2}=1r _ 2 = 1, m2=0.6subscript20.6m_{2}=0.6m _ 2 = 0.6, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r4=3subscript43r_{4}=3r _ 4 = 3, c1=1subscript11c_{1}=1c _ 1 = 1, r5=1subscript51r_{5}=1r _ 5 = 1, q=0.50.5q=0.5q = 0.5, and r=0.010.01r=0.01r = 0.01. For these parameter values, we get the values N1=1.57033subscript11.57033N_{1}=1.57033N _ 1 = 1.57033, N2=0.0862411subscript20.0862411N_{2}=0.0862411N _ 2 = 0.0862411, N3=0.0774249subscript30.0774249N_{3}=0.0774249N _ 3 = 0.0774249, and N1N2N3=0.0580017subscript1subscript2subscript30.0580017N_{1}N_{2}-N_{3}=0.0580017N _ 1 N _ 2 - N _ 3 = 0.0580017. Since N1subscript1N_{1}N _ 1 , N2subscript2N_{2}N _ 2 , N3subscript3N_{3}N _ 3 , and N1N2N3subscript1subscript2subscript3N_{1}N_{2}-N_{3}N _ 1 N _ 2 - N _ 3 are all positive, it can be concluded that the coexisting equilibrium point EcsubscriptE_{c}E _ c is locally stable based on theorem (8). Figure (3) perfectly illustrates the local stability of all the ecologically feasible equilibrium points, including the coexisting equilibrium point EcsubscriptE_{c}E _ c . In this section, we will perform numerical validation of the theoretical results concerning the local stability of the equilibrium points of the system (3) that have been outlined in the preceding sections. In order to validate the theorem numerically, we will examine the parameter values: r1=0.46subscript10.46r_{1}=0.46r _ 1 = 0.46, r2=0.32subscript20.32r_{2}=0.32r _ 2 = 0.32, r3=0.5subscript30.5r_{3}=0.5r _ 3 = 0.5, r4=0.002subscript40.002r_{4}=0.002r _ 4 = 0.002, r5=0.38subscript50.38r_{5}=0.38r _ 5 = 0.38, Î²=4.047½4.047 = 4.047, m1=0.72subscript10.72m_{1}=0.72m _ 1 = 0.72, m2=0.17subscript20.17m_{2}=0.17m _ 2 = 0.17, d1=0.096subscript10.096d_{1}=0.096d _ 1 = 0.096, d2=0.279subscript20.279d_{2}=0.279d _ 2 = 0.279, c1=3.33subscript13.33c_{1}=3.33c _ 1 = 3.33, q=1.351.35q=1.35q = 1.35, r=0.380.38r=0.38r = 0.38 and Î±=0.98¼0.98 = 0.98. Given the parameter values, r>0.34=r1q0.34subscript1r>0.34= > 0.34 = / r _ 1 q , i.e., the condition outlined in theorem (9) is satisfied, resulting in the local stability of Ev(0,0,0)subscript£000E_{v}(0,0,0)E _ v ( 0 , 0 , 0 ). Figure (4(a)) illustrates this. Now, we will investigate a different set of parameter values: r1=0.46subscript10.46r_{1}=0.46r _ 1 = 0.46, r2=0.32subscript20.32r_{2}=0.32r _ 2 = 0.32, r3=0.5subscript30.5r_{3}=0.5r _ 3 = 0.5, r4=0.002subscript40.002r_{4}=0.002r _ 4 = 0.002, r5=0.38subscript50.38r_{5}=0.38r _ 5 = 0.38, Î²=4.047½4.047 = 4.047, m1=0.72subscript10.72m_{1}=0.72m _ 1 = 0.72, m2=0.17subscript20.17m_{2}=0.17m _ 2 = 0.17, d1=0.096subscript10.096d_{1}=0.096d _ 1 = 0.096, d2=0.279subscript20.279d_{2}=0.279d _ 2 = 0.279, c1=3.33subscript13.33c_{1}=3.33c _ 1 = 3.33, q=0.8380.838q=0.838q = 0.838, r=0.380.38r=0.38r = 0.38 and Î±=0.98¼0.98 = 0.98. With respect to the given parameter values, it can be concluded that all the conditions stated in theorem (10) are satisfied. Hence, Ea(1qrr1,0,0)subscript1subscript100E_{a}(1- _ a ( 1 - / q r r _ 1 , 0 , 0 ) is locally stable. The local stability of Ea(1qrr1,0,0)subscript1subscript100E_{a}(1- _ a ( 1 - / q r r _ 1 , 0 , 0 ) is illustrated properly in figure (4(b)). We set the parameter values as follows: r1=0.46subscript10.46r_{1}=0.46r _ 1 = 0.46, r2=0.32subscript20.32r_{2}=0.32r _ 2 = 0.32, r3=0.5subscript30.5r_{3}=0.5r _ 3 = 0.5, r4=0.002subscript40.002r_{4}=0.002r _ 4 = 0.002, r5=0.38subscript50.38r_{5}=0.38r _ 5 = 0.38, Î²=4.047½4.047 = 4.047, m1=0.72subscript10.72m_{1}=0.72m _ 1 = 0.72, m2=0.17subscript20.17m_{2}=0.17m _ 2 = 0.17, d1=0.096subscript10.096d_{1}=0.096d _ 1 = 0.096, d2=0.279subscript20.279d_{2}=0.279d _ 2 = 0.279, c1=3.33subscript13.33c_{1}=3.33c _ 1 = 3.33, q=0.0190.019q=0.019q = 0.019, r=0.380.38r=0.38r = 0.38 and Î±=0.98¼0.98 = 0.98. Since all the requirements stated in theorem (11) are satisfied, Etsubscript¡E_{t}E _ t is locally stable. Figure (4(c)) illustrates this accurately. In order to demonstrate the correctness of theorem (12), we utilise certain parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r3=1subscript31r_{3}=1r _ 3 = 1, Î²=4.047½4.047 = 4.047, r2=1subscript21r_{2}=1r _ 2 = 1, m1=0.9471subscript10.9471m_{1}=0.9471m _ 1 = 0.9471, m2=0.17subscript20.17m_{2}=0.17m _ 2 = 0.17, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r4=3subscript43r_{4}=3r _ 4 = 3, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, r5=1subscript51r_{5}=1r _ 5 = 1, q=0.50.5q=0.5q = 0.5, r=0.010.01r=0.01r = 0.01 and Î±=0.98¼0.98 = 0.98. Given the above parameter values, we calculate the following values: Î”(±)=0.0024>0Î”italic-±0.00240 ( ± ) = 0.0024 > 0, N1=1.98>0subscript11.980N_{1}=1.98>0N _ 1 = 1.98 > 0, N3=0.00002>0subscript30.000020N_{3}=0.00002>0N _ 3 = 0.00002 > 0, and N1N2N3=0.05>0subscript1subscript2subscript30.050N_{1}N_{2}-N_{3}=0.05>0N _ 1 N _ 2 - N _ 3 = 0.05 > 0. Therefore, all the conditions specified by theorem (12) are met. Thus, the local asymptotic stability of the fixed point Ec(C,D,E)subscript¶·E_{c}(C,D,E)E _ c ( C , D , E ) in the system (3) has been verified. This is clearly evident in figure (4(d)). Figure (3) provides a clear visual representation of the local stability of each of the equilibrium points, which are ecologically feasible in the system (2), including the coexisting equilibrium point EcsubscriptE_{c}E _ c . On the other hand, figure (4) provides an explicit graphical illustration of the local stability of all of the equilibrium points that are ecologically feasible for the system (3). From an ecological standpoint, it has been observed that when the rate at which a prey species reproduces naturally becomes lower than the rate at which it is harvested, the prey species is at risk of extinction. Consequently, both the predator species that depend on the prey also face extinction. This leads to the establishment of a stable equilibrium point where all three species vanish. In addition, in this scenario, the presence of the axial equilibrium point becomes unattainable, mirroring the natural occurrence where the intrinsic growth rate of the prey species falls below the harvesting rate, rendering survival impossible for the prey. Therefore, in order for the prey to survive, its growth rate must exceed its harvesting rate. Therefore, in the previously mentioned bio-systems (2) and (3), it is possible for all three species to become extinct under certain conditions. In certain situations, it is also possible for the predator species to face extinction while the prey species thrives. Interestingly, when the value of qqq is set to 0.0190.0190.0190.019 and other parameters are assigned values such as r1=0.46subscript10.46r_{1}=0.46r _ 1 = 0.46, r2=0.32subscript20.32r_{2}=0.32r _ 2 = 0.32, r3=0.5subscript30.5r_{3}=0.5r _ 3 = 0.5, r4=0.002subscript40.002r_{4}=0.002r _ 4 = 0.002, r5=0.38subscript50.38r_{5}=0.38r _ 5 = 0.38, Î²=4.047½4.047 = 4.047, m1=0.72subscript10.72m_{1}=0.72m _ 1 = 0.72, m2=0.17subscript20.17m_{2}=0.17m _ 2 = 0.17, d1=0.096subscript10.096d_{1}=0.096d _ 1 = 0.096, d2=0.279subscript20.279d_{2}=0.279d _ 2 = 0.279, c1=3.33subscript13.33c_{1}=3.33c _ 1 = 3.33, and r=0.380.38r=0.38r = 0.38, it is observed that only the top predator faces extinction, allowing the intermediate predator and the prey to persist within the system, regardless of the memory effect. By increasing the value of qqq to q=0.8380.838q=0.838q = 0.838, both the top predator and the intermediate predator disappear, leaving only the prey species in the system. Furthermore, if the value of qqq is increased slightly to q=1.351.35q=1.35q = 1.35, all three species face extinction and the system under investigation collapses. From an ecological perspective, the population size of the prey diminishes as the amount being harvested rises. As a result, the food supply declines for the intermediate predators, which has an adverse effect on their growth rate. This, in turn, hinders the growth of the top predator because of the reduced availability of food that comes from intermediate predators. As a result, either species could become extinct inside the system due to the catchability coefficient (qqq). This makes the parameter qqq a very critical component of the system under consideration. In addition, it is noticed that, given some parametric settings, the coexistence of all three species within the system is conceivable. All the observations mentioned above are pertinent to both systems (2) and (3). Memory in predator-prey systems pertains to the capacity of organisms to recollect previous encounters and adapt their behaviour or strategy in accordance with those recollections. In the past, ecological models have primarily concentrated on immediate interactions wherein predators react to the present availability of prey and vice versa. However, the inclusion of memory in predator-prey models offers an additional level of richness and realism. Following that, we constructed the model (3) to investigate the impact of memory on the bio-system (2). The fractional order Î±¼ in the model (3) represents the degree of memory that is influencing the system under investigation. As the fractional order Î±†0†¼0 0Î± † 0, the system demonstrates a greater level of memory. On the other hand, as the value of Î±†1†¼1 1Î± † 1, the system becomes increasingly devoid of memory. Studies have shown that as the fractional order goes up, there is an accompanying reduction in retention of memory [56, 57, 58]. In order to have a deeper comprehension of the impact of memory on the system (3), we examine several scenarios by adjusting different parameter values. Figure (6) displays four distinct instances. In the first case, we discuss the situation in which the system (3) has a value of Î±=1¼1 = 1 , which is equivalent to the system (2). The second instance involves considering the system (3) with the value of Î±=0.98¼0.98 = 0.98. In the third scenario, the system (3) is analysed with a value of Î±=0.9¼0.9 = 0.9, whereas in the fourth scenario, the system (3) is studied with a value of Î±=0.85¼0.85 = 0.85. Figure (6) clearly demonstrates that when the order of the fractional derivative decreases, i.e., Î±†0†¼0 0Î± † 0, the stability of the system under discussion improves. In figure (6(a)), we can clearly see highly concentrated fluctuations in the populations of the three species, whereas in figure (6(b)), the fluctuations gradually disappear after a certain amount of time. Furthermore, it can be observed from figures (6(c)) and (6(d)) that the scale of fluctuations diminishes as the fractional order reduces, inducing more stability within the system. Similarly, figure (7) illustrates that as the value of Î±¼ decreases from 1 to 0, the fluctuations within the system become stabilised, demonstrating the impact of memory on the system. In a similar way figure (8) illustrates that in the system (3) where the prey species and intermediate predators do not exhibit refuge behaviour against their predators, the instability of the system increases as the species™ memory diminishes. The same conclusion may be deduced from figures (9) and (10). Figure (9) illustrates the impact of fading memory on the system (3) in the absence of any harvesting. Figure (10) illustrates the impact of memory on the system (3) in the absence of prey odour effect. Hence, based on the aforementioned figures, it has been noted that individuals who suffer from memory loss or a deterioration in their ability to recall prior events can have a negative impact on the stability of the corresponding system. In this part, we analyse the impact of the parameters related to the refuge triggered by the predator odour in the aforementioned system. At first, we analyse the effects of parameter m1subscript1m_{1}m _ 1 on the system (2). The figure (5(a)) provide a clear and visually appealing demonstration of the several bifurcations that occur when the parameter m1subscript1m_{1}m _ 1 is altered. We accomplish this by assuming other parameter values like r1=2subscript12r_{1}=2r _ 1 = 2, r3=1subscript31r_{3}=1r _ 3 = 1, Î²=0.01½0.01 = 0.01, r2=1subscript21r_{2}=1r _ 2 = 1, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r4=3subscript43r_{4}=3r _ 4 = 3, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, r5=1subscript51r_{5}=1r _ 5 = 1, r=0.010.01r=0.01r = 0.01, only altering the parameter m1subscript1m_{1}m _ 1 . If the value of parameter m1subscript1m_{1}m _ 1 is below a threshold value of m1=0.4498=m1hb1subscript10.4498superscriptsubscript11m_{1}=0.4498=m_{1}^{hb1}m _ 1 = 0.4498 = m _ 1 ^ h b 1 , then the system (2) exhibits stability around the fixed point EcsubscriptE_{c}E _ c . Given the following set of parameter values, it can be readily confirmed numerically: r1=2subscript12r_{1}=2r _ 1 = 2, Î²=0.01½0.01 = 0.01, m1=0.3subscript10.3m_{1}=0.3m _ 1 = 0.3, r2=1subscript21r_{2}=1r _ 2 = 1, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, r3=1subscript31r_{3}=1r _ 3 = 1, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r4=3subscript43r_{4}=3r _ 4 = 3, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, r5=1subscript51r_{5}=1r _ 5 = 1, q=0.50.5q=0.5q = 0.5, and r=0.010.01r=0.01r = 0.01. For this particular set of parameter values, we obtain the following results: N1=1.53>0subscript11.530N_{1}=1.53>0N _ 1 = 1.53 > 0, N2=0.13>0subscript20.130N_{2}=0.13>0N _ 2 = 0.13 > 0, N3=0.18>0subscript30.180N_{3}=0.18>0N _ 3 = 0.18 > 0, and N1N2N3=0.027>0subscript1subscript2subscript30.0270N_{1}N_{2}-N_{3}=0.027>0N _ 1 N _ 2 - N _ 3 = 0.027 > 0. This means that all the requirements needed to achieve stability given in theorem (8) are satisfied. This confirms the stability of EcsubscriptE_{c}E _ c . This is shown in figure (11(a)). When the value of m1subscript1m_{1}m _ 1 reaches a certain threshold, known as the Hopf bifurcation point, the system described by equation (2) undergoes a significant shift in stability. Numerical verification of the existence of a Hopf bifurcation can also be accomplished by applying the theorem (14). Given the threshold value of m1=m1hb1subscript1superscriptsubscript11m_{1}=m_{1}^{hb1}m _ 1 = m _ 1 ^ h b 1 and keeping other parameter values unchanged, we find the following values: N1=1.6>0subscript11.60N_{1}=1.6>0N _ 1 = 1.6 > 0, N2=0.07>0subscript20.070N_{2}=0.07>0N _ 2 = 0.07 > 0, N3=0.12>0subscript30.120N_{3}=0.12>0N _ 3 = 0.12 > 0, N1N2N3=0subscript1subscript2subscript30N_{1}N_{2}-N_{3}=0N _ 1 N _ 2 - N _ 3 = 0, and N1(m1hb1)N2²(m1hb1)+N2(m1hb1)N1²(m1hb1)N3²(m1hb)=0.07 0subscript1superscriptsubscript11superscriptsubscript2²superscriptsubscript11subscript2superscriptsubscript11superscriptsubscript1²superscriptsubscript11superscriptsubscript3²superscriptsubscript10.070N_{1}(m_{1}^{hb1})N_{2}^{{}^{ ^{ 0N _ 1 ( m _ 1 ^ h b 1 ) N _ 2 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ^ h b 1 ) + N _ 2 ( m _ 1 ^ h b 1 ) N _ 1 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ^ h b 1 ) - N _ 3 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ^ h b ) = - 0.07 0. This confirms the existence of a Hopf bifurcation at m1=m1hb1subscript1superscriptsubscript11m_{1}=m_{1}^{hb1}m _ 1 = m _ 1 ^ h b 1 . Furthermore, if the value of m1subscript1m_{1}m _ 1 is slightly elevated, the system (2) demonstrates instability. Assuming the value of m1subscript1m_{1}m _ 1 is 0.50.50.50.5, while keeping all other parameter values the same, we find that N1N2N3=0.001<0subscript1subscript2subscript30.0010N_{1}N_{2}-N_{3}=-0.001<0N _ 1 N _ 2 - N _ 3 = - 0.001 < 0. This result corroborates the instability of the system near EcsubscriptE_{c}E _ c . This situation is shown visually in figure (11(b)). Another Hopf bifurcation occurs when the parameter m1subscript1m_{1}m _ 1 is increased to m1=0.5295=m1hb2subscript10.5295superscriptsubscript12m_{1}=0.5295=m_{1}^{hb2}m _ 1 = 0.5295 = m _ 1 ^ h b 2 , resulting in a change in the stability of the system (2). When m1=0.5295=m1hb2subscript10.5295superscriptsubscript12m_{1}=0.5295=m_{1}^{hb2}m _ 1 = 0.5295 = m _ 1 ^ h b 2 and all other parameter values remain the same, all the conditions of theorem (14) are satisfied. This can be seen as N1=1.7>0subscript11.70N_{1}=1.7>0N _ 1 = 1.7 > 0, N2=0.05>0subscript20.050N_{2}=0.05>0N _ 2 = 0.05 > 0, N3=0.09>0subscript30.090N_{3}=0.09>0N _ 3 = 0.09 > 0, N1N2N3=0subscript1subscript2subscript30N_{1}N_{2}-N_{3}=0N _ 1 N _ 2 - N _ 3 = 0, and N1(m1hb1)N2²(m1hb1)+N2(m1hb1)N1²(m1hb1)N3²(m1hb)=0.07 0subscript1superscriptsubscript11superscriptsubscript2²superscriptsubscript11subscript2superscriptsubscript11superscriptsubscript1²superscriptsubscript11superscriptsubscript3²superscriptsubscript10.070N_{1}(m_{1}^{hb1})N_{2}^{{}^{ ^{ 0N _ 1 ( m _ 1 ^ h b 1 ) N _ 2 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ^ h b 1 ) + N _ 2 ( m _ 1 ^ h b 1 ) N _ 1 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ^ h b 1 ) - N _ 3 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 1 ^ h b ) = 0.07 0. This validates the presence of a second Hopf bifurcation at m1=0.5295=m1hb2subscript10.5295superscriptsubscript12m_{1}=0.5295=m_{1}^{hb2}m _ 1 = 0.5295 = m _ 1 ^ h b 2 . If the values of m1subscript1m_{1}m _ 1 exceed m1hb2superscriptsubscript12m_{1}^{hb2}m _ 1 ^ h b 2 , the system (2) demonstrates stability in the vicinity of the fixed point EcsubscriptE_{c}E _ c . Numerical confirmation can be obtained by evaluating the parameter values. For example, when m1=0.6>m1hb2subscript10.6superscriptsubscript12m_{1}=0.6>m_{1}^{hb2}m _ 1 = 0.6 > m _ 1 ^ h b 2 and all other parameters are unaltered, we find that N1=1.75>0subscript11.750N_{1}=1.75>0N _ 1 = 1.75 > 0, N2=0.04>0subscript20.040N_{2}=0.04>0N _ 2 = 0.04 > 0, N3=0.06>0subscript30.060N_{3}=0.06>0N _ 3 = 0.06 > 0, and N1N2N3=0.01>0subscript1subscript2subscript30.010N_{1}N_{2}-N_{3}=0.01>0N _ 1 N _ 2 - N _ 3 = 0.01 > 0. Figure (11(c)) depicts this same scenario. On the other hand, if we slightly increase the value of the parameter m1subscript1m_{1}m _ 1 , an interesting phenomenon occurs. At a certain point, denoted as m1=0.73375185=m1tbpsubscript10.73375185superscriptsubscript1¡m_{1}=0.73375185=m_{1}^{tbp}m _ 1 = 0.73375185 = m _ 1 ^ t b p , a transcritical bifurcation occurs. This results in the system (2) being unstable around EcsubscriptE_{c}E _ c , whereas the fixed point EasubscriptE_{a}E _ a becomes stable. Numerically, when m1subscript1m_{1}m _ 1 is equal to 0.8 and all other parameters remain unchanged, we obtain r<4=r1q4subscript1r<4= < 4 = / r _ 1 q and r2=1<d1r12Î²r2q2r3+6subscript21subscript1superscriptsubscript12½superscript2superscript2subscript3subscript”6r_{2}=1< r^{2}q^{2}r_{3}+ _ 2 = 1 < / - d _ 1 r _ 1 ^ 2 - Î² r ^ 2 q ^ 2 r _ 3 + _ 6 . Additionally, we have N3=0.03<0subscript30.030N_{3}=-0.03<0N _ 3 = - 0.03 < 0. Therefore, it is evident that EasubscriptE_{a}E _ a is stable while EcsubscriptE_{c}E _ c is unstable. The precise depiction of the particular scenario can be observed in figure (11(d)). Based on the preceding discussion, it is evident that the parameter m1subscript1m_{1}m _ 1 plays a significant role in the system (2). By looking at the situation from an an ecological standpoint, it has been observed that when the parameter m1subscript1m_{1}m _ 1 is set to a very low value, the prey population tends to remain small. On the other hand, when the amount of prey refuge increases, it has a direct positive impact on the population of the prey species, resulting in increase in the prey population as well. As the parameter m1subscript1m_{1}m _ 1 increases, an interesting phenomenon occurs. At a certain value, fluctuations in the population of the three species are observed. These fluctuations are subsequently stabilised by the occurrence of another Hopf bifurcation at a slightly elevated value of the parameter m1subscript1m_{1}m _ 1 . By increasing the parameter m1subscript1m_{1}m _ 1 , a remarkable phenomenon known as a transcritical bifurcation occurs. This leads to an unstable coexistence equilibrium, making it impossible for all three populations to cohabit. Now, we will examine the significance of the parameter m2subscript2m_{2}m _ 2 in the system (2). To do this, we set other parameters to values like r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r2=1subscript21r_{2}=1r _ 2 = 1, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r4=3subscript43r_{4}=3r _ 4 = 3, c1=1subscript11c_{1}=1c _ 1 = 1, r3=1subscript31r_{3}=1r _ 3 = 1, q=0.50.5q=0.5q = 0.5, r=0.010.01r=0.01r = 0.01, and change only m2subscript2m_{2}m _ 2 . Based on the information provided in figure (5(b)), it is clear that the parameter m2subscript2m_{2}m _ 2 plays a significant role in the system (2). For all values of the parameter m2subscript2m_{2}m _ 2 that are less than a threshold value m2=0.503528=m2hbpsubscript20.503528superscriptsubscript2m_{2}=0.503528=m_{2}^{hbp}m _ 2 = 0.503528 = m _ 2 ^ h b p , and with all other parameter values remaining the same, the system (2) exhibits instability around the fixed point EcsubscriptE_{c}E _ c . More precisely, the system (2) exhibits oscillations in populations. For instance, when we set the parameter values as follows: r1=2subscript12r_{1}=2r _ 1 = 2, r4=3subscript43r_{4}=3r _ 4 = 3, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, r2=1subscript21r_{2}=1r _ 2 = 1, m2=0.4subscript20.4m_{2}=0.4m _ 2 = 0.4, r3=1subscript31r_{3}=1r _ 3 = 1, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, r5=1subscript51r_{5}=1r _ 5 = 1, and r=0.010.01r=0.01r = 0.01, the expression N1N2N3subscript1subscript2subscript3N_{1}N_{2}-N_{3}N _ 1 N _ 2 - N _ 3 evaluates to -0.02, which is less than 0. This confirms the instability of EcsubscriptE_{c}E _ c . It is shown in figure (12(a)). The value m2=0.503528=m2hbpsubscript20.503528superscriptsubscript2m_{2}=0.503528=m_{2}^{hbp}m _ 2 = 0.503528 = m _ 2 ^ h b p represents a Hopf bifurcation point, as it fulfils all the criteria stated in theorem (14). When m2=0.503528=m2hbpsubscript20.503528superscriptsubscript2m_{2}=0.503528=m_{2}^{hbp}m _ 2 = 0.503528 = m _ 2 ^ h b p , we find that N2=0.065>0subscript20.0650N_{2}=0.065>0N _ 2 = 0.065 > 0, N3=0.10>0subscript30.100N_{3}=0.10>0N _ 3 = 0.10 > 0, N1N2N3=0subscript1subscript2subscript30N_{1}N_{2}-N_{3}=0N _ 1 N _ 2 - N _ 3 = 0, and N1(m2hbp)N2²(m2hbp)+N2(m2hbp)N1²(m2hbp)N3²(m2hbp)=0.02 0subscript1superscriptsubscript2superscriptsubscript2²superscriptsubscript2subscript2superscriptsubscript2superscriptsubscript1²superscriptsubscript2superscriptsubscript3²superscriptsubscript20.020N_{1}(m_{2}^{hbp})N_{2}^{{}^{ ^{ 0N _ 1 ( m _ 2 ^ h b p ) N _ 2 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 2 ^ h b p ) + N _ 2 ( m _ 2 ^ h b p ) N _ 1 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 2 ^ h b p ) - N _ 3 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( m _ 2 ^ h b p ) = 0.02 0. Therefore, it is numerically confirmed that a Hopf bifurcation occurs at m2=0.503528=m2hbpsubscript20.503528superscriptsubscript2m_{2}=0.503528=m_{2}^{hbp}m _ 2 = 0.503528 = m _ 2 ^ h b p . This Hopf bifurcation stabilizes the system until a transcritical bifurcation occurs, which alters its stability. This transcritical bifurcation occurs at m2=0.74958126=m2tbpsubscript20.74958126superscriptsubscript2¡m_{2}=0.74958126=m_{2}^{tbp}m _ 2 = 0.74958126 = m _ 2 ^ t b p , along with other parameter values that remain unchanged. For example, when the parameter values are set as follows: r1=2subscript12r_{1}=2r _ 1 = 2, r4=3subscript43r_{4}=3r _ 4 = 3, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.6subscript20.6m_{2}=0.6m _ 2 = 0.6, r2=1subscript21r_{2}=1r _ 2 = 1, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r3=1subscript31r_{3}=1r _ 3 = 1, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r5=1subscript51r_{5}=1r _ 5 = 1, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, and r=0.010.01r=0.01r = 0.01, the resulting values are N1=1.57>0subscript11.570N_{1}=1.57>0N _ 1 = 1.57 > 0, N2=0.086>0subscript20.0860N_{2}=0.086>0N _ 2 = 0.086 > 0, N3=0.077>0subscript30.0770N_{3}=0.077>0N _ 3 = 0.077 > 0, and N1N2N3=0.058>0subscript1subscript2subscript30.0580N_{1}N_{2}-N_{3}=0.058>0N _ 1 N _ 2 - N _ 3 = 0.058 > 0. This confirms the stability of EcsubscriptE_{c}E _ c . This particular situation is illustrated in figure (12(b)). For a different set of parameter values, specifically r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, r4=3subscript43r_{4}=3r _ 4 = 3, m2=0.8subscript20.8m_{2}=0.8m _ 2 = 0.8, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r3=1subscript31r_{3}=1r _ 3 = 1, c1=1subscript11c_{1}=1c _ 1 = 1, r2=1subscript21r_{2}=1r _ 2 = 1, q=0.50.5q=0.5q = 0.5, and r=0.010.01r=0.01r = 0.01, it has been determined that N1N2N3subscript1subscript2subscript3N_{1}N_{2}-N_{3}N _ 1 N _ 2 - N _ 3 is equal to -9.39, which is less than 0. Moreover, all the conditions outlined in theorem (7) are also met, confirming the instability of EcsubscriptE_{c}E _ c and the stability of Etsubscript¡E_{t}E _ t . The precise depiction of the particular circumstance can be observed in figure (12(c)). When the intermediate predator seeks shelter from the top predator™s predation pressure, the system (2) becomes more intricate and intriguing. It is evident that when the parameter m2subscript2m_{2}m _ 2 is extremely low, oscillations in the population of all three species within the system occur. However, as this parameter increases, the fluctuations gradually decrease. Eventually, after reaching a certain value, these fluctuations stabilise due to the occurrence of a Hopf bifurcation. As the parameter m2subscript2m_{2}m _ 2 increases, the population of the intermediate predator similarly increases. However, the population of prey continues to decline, a trend that can be rationalised from an ecological standpoint. When the parameter m2subscript2m_{2}m _ 2 reaches a higher value, a transcritical bifurcation takes place, resulting in the inability of all three species to coexist within the system (2). This phenomenon can be readily understood from an ecological standpoint. When the intermediate predator strategically enhances its protection against predation by the top predator in response to the top predator™s odour, thereby reducing its vulnerability to being hunted. Consequently, the predation pressure on the prey by the intermediate predator increases, leading to a substantial decline in the prey population. As a result, the coexistence of all three species within the system becomes extremely difficult. Some special cases: In this portion, we are going to explore three distinct specific situations that may arise in the system (2). In the first scenario, we focus on the situation where the prey is unable to detect the odour of the predator, that is, when a1=0subscript10a_{1}=0a _ 1 = 0, resulting in m1=mfpa1=0subscript1subscript“subscript10m_{1}=m_{fp}a_{1}=0m _ 1 = m _ f p a _ 1 = 0. Consequently, they are unpredictably attacked by camouflaged predators, leaving them little opportunity to respond or seek refuge. In the second scenario, we address a circumstance where the intermediate predator cannot sense the odour of their predator or whether the top predator conceals their odour, specifically when a2=0subscript20a_{2}=0a _ 2 = 0, leading to m2=mspa2=0subscript2subscript subscript20m_{2}=m_{sp}a_{2}=0m _ 2 = m _ s p a _ 2 = 0. In the third scenario, our attention is directed towards a situation in which the intermediate predator and the prey are unable to effectively utilise refuge to protect themselves from their predators for a variety of reasons, that is, when m1=0=m2subscript10subscript2m_{1}=0=m_{2}m _ 1 = 0 = m _ 2 . Scenario 1: When the predator™s odour is undetectable to the prey species, leading to a lack of refuge activity in prey (m1=0subscriptm10m_{1}=0m _ 1 = 0) In this situation, the prey species is unable to detect the odour of the intermediate predator, or the predator masks their odour, i.e., when a1=0subscript10a_{1}=0a _ 1 = 0, implying m1=mfpa1=0subscript1subscript“subscript10m_{1}=m_{fp}a_{1}=0m _ 1 = m _ f p a _ 1 = 0. In an attempt to gain a better understanding of the situation when m1=0subscript10m_{1}=0m _ 1 = 0, we conducted a series of simulations and obtained a range of figures. Based on the figure (13), it has been determined that when the parameter m1subscript1m_{1}m _ 1 is set to 0, along with the following values for the other parameters: r1=2subscript12r_{1}=2r _ 1 = 2, Î²=0.01½0.01 = 0.01, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r4=3subscript43r_{4}=3r _ 4 = 3, r5=1subscript51r_{5}=1r _ 5 = 1, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, r2=1subscript21r_{2}=1r _ 2 = 1, r3=1subscript31r_{3}=1r _ 3 = 1, q=0.50.5q=0.5q = 0.5, and r=0.010.01r=0.01r = 0.01, the system (2) exhibits stability around EcsubscriptE_{c}E _ c . This is indicated by the fact that all the values N1=1.32subscript11.32N_{1}=1.32N _ 1 = 1.32, N2=0.29subscript20.29N_{2}=0.29N _ 2 = 0.29, N3=0.024subscript30.024N_{3}=0.024N _ 3 = 0.024, and N1N2N3=0.14subscript1subscript2subscript30.14N_{1}N_{2}-N_{3}=0.14N _ 1 N _ 2 - N _ 3 = 0.14 are positive. Figure (13(b)) demonstrates the time series of all three populations within the system (2). The figure was obtained using an initial value of (0.8,0.8,0.8). On the other hand, figure (13(a)) illustrates the phase portrait with three different initial conditions: (0.1,0.8,0.1), (0.1,0.1,0.1), and (0.8,0.8,0.8). From an ecological perspective, it can be inferred that if the prey species does not exhibit any refuge behaviour, then all three species can coexist within the system (2). In the natural world, it is quite common for predators to employ chemical camouflage to mask their odours, making it difficult for their prey to detect them [59, 60]. As an example, numerous insect species thrive within social insect colonies, preying on colony members by concealing their own odour. When the prey has no refuge from the predation pressure exerted by the intermediate predator, it becomes apparent that all three populations can persist, even though the prey population density remains very low. This is consistent with ecological reasoning, as when prey have no refuge from their predators, the predators may easily pursue them, leading to a potential decline in the prey population. It is intriguing to see that even without the presence of an appropriate refuge for prey, the system we are discussing can nonetheless achieve the coexistence of all three species. Scenario 2: When the top predator™s odour induces no refuge in the intermediate predators, or when intermediate predators are unable to detect the odours of the top predator (m2=0subscriptm20m_{2}=0m _ 2 = 0) In this particular case, we are investigating a scenario in which the intermediate predator is incapable of perceiving the odour of the first predator, therefore leaving it defenceless against predation by the top predator. That is, when a2=0subscript20a_{2}=0a _ 2 = 0, which means m2=mfpa2=0subscript2subscript“subscript20m_{2}=m_{fp}a_{2}=0m _ 2 = m _ f p a _ 2 = 0. Alternatively, if the intermediate predator does have a refuge, the top predator is capable of overcoming it, resulting in m2=0subscript20m_{2}=0m _ 2 = 0. This phenomenon can occur in the natural world when certain predators conceal their scents in order to avoid detection by their prey. Additionally, some predators possess physical adaptations or exhibit specific behaviours that enable them to access or infiltrate shelters or hiding places [59, 60, 61]. In order to comprehend the situation where m2=0subscript20m_{2}=0m _ 2 = 0, we conducted multiple simulations, which resulted in diverse figures. We utilise the following parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0subscript20m_{2}=0m _ 2 = 0, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r2=1subscript21r_{2}=1r _ 2 = 1, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r3=1subscript31r_{3}=1r _ 3 = 1, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, r4=3subscript43r_{4}=3r _ 4 = 3, and r=0.010.01r=0.01r = 0.01, in order to generate figure (14). Figure (14(b)) depicts the temporal progression of the three populations in the system (2). The initial point (0.8,0.8,0.8) is used to generate figure (14(b)). In a similar way, figure (14(a)) illustrates the phase portrait of all three populations when m2=0subscript20m_{2}=0m _ 2 = 0. The starting point (0.8,0.8,0.8) is utilised to produce figure (14(a)). Based on the information presented in figure (14), it is apparent that the system (2) exhibits instability near EcsubscriptE_{c}E _ c when the intermediate predator does not have any refuge against predation by the top predator. It is interesting to observe that the populations of all three species experience fluctuations when there is no refuge for the intermediate predator against the predation pressure from the top predator. Thus, all three species may persist within the system in the absence of the intermediate predator™s refuge behaviour against the top predator™s predation effort, but population levels for all species remain fluctuant. This affirms the significance of the parameter m2subscript2m_{2}m _ 2 within the system (2). Scenario 3: When there is no refuge behaviour in both the prey species and the intermediate predator, i.e., when m1=0=m2subscriptm10subscriptm2m_{1}=0=m_{2}m _ 1 = 0 = m _ 2 occur simultaneously In the above paragraphs, we have already covered two unique situations. In one scenario, the prey is unable to detect the odours of the intermediate predators, which leads to them not utilising refuge. However, the intermediate predator is able to use refuge as a defence against the top predator, in response to the top predator™s odour. In another scenario, prey seek refuge from intermediate predators, but the intermediate predators either do not utilise refuge or it proves ineffective against the top predator. This can be attributed to factors such as the intermediate predator™s inability to detect the odour of the top predator or the top predator™s ability to mask their odour (Scenario 2). In this part, we will explore a scenario where neither the prey nor the intermediate predators utilise refuge, or where refuge proves ineffective in protecting them from their respective predators (Scenario 3). In order to have a better understanding of this particular situation, we conducted several simulations with the following parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r4=3subscript43r_{4}=3r _ 4 = 3, Î²=0.01½0.01 = 0.01, m1=0subscript10m_{1}=0m _ 1 = 0, r2=1subscript21r_{2}=1r _ 2 = 1, m2=0subscript20m_{2}=0m _ 2 = 0, r3=1subscript31r_{3}=1r _ 3 = 1, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r5=1subscript51r_{5}=1r _ 5 = 1, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, and r=0.010.01r=0.01r = 0.01. By utilising these specific parameter values, we have generated figures (15(a)) and (15(b)). The time series of all three populations when the parameters m1subscript1m_{1}m _ 1 and m2subscript2m_{2}m _ 2 are both equal to zero is depicted in figure (15(a)). Additionally, the phase portrait of all three populations within the system (2) is illustrated in figure (15(b)). The initial condition utilised for obtaining the aforementioned two figures is (0.8, 0.6, 0.8). It is clear that when m1=0=m2subscript10subscript2m_{1}=0=m_{2}m _ 1 = 0 = m _ 2 , the system (2) exhibits instability around EcsubscriptE_{c}E _ c . Furthermore, the system (2) exhibits oscillations in the populations of the three species within the system when m1=0=m2subscript10subscript2m_{1}=0=m_{2}m _ 1 = 0 = m _ 2 . In addition, the presence of a limit cycle may be observed in figure (15(b)). Hence, it may be inferred that the absence of the refuge phenomenon prevents a stable coexistence of all three species within the system. That is, in the absence of refuge behaviour in the prey species and the intermediate predator species in order to defend themselves from their predators, the populations of all three species consistently fluctuate. Thus, the importance of the refuge phenomenon within the system is firmly established. Within this section, we will examine the occurrence of the bubbling phenomenon within the system (2). Occasionally, a bifurcation diagram may exhibit a closed-loop configuration resembling a bubble like structure due to the emergence and vanishing of oscillations originating from two Hopf bifurcation points. This occurrence is commonly known as the bubbling phenomenon. Many researchers have acknowledged the occurrence of bubbling phenomena in predator-prey models[62, 63, 64, 65, 66, 67]. The system (2) exhibits the bubbling phenomenon when certain parameter values are fixed and only the parameter m1subscript1m_{1}m _ 1 is altered. The fixed parameter values are: r1=2subscript12r_{1}=2r _ 1 = 2, r3=1subscript31r_{3}=1r _ 3 = 1, Î²=0.01½0.01 = 0.01, r2=1subscript21r_{2}=1r _ 2 = 1, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r4=3subscript43r_{4}=3r _ 4 = 3, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, r5=1subscript51r_{5}=1r _ 5 = 1, r=0.010.01r=0.01r = 0.01. The emergence of a bubble of oscillations between two hopf bifurcation points, m1=0.4498=m1hb1subscript10.4498superscriptsubscript11m_{1}=0.4498=m_{1}^{hb1}m _ 1 = 0.4498 = m _ 1 ^ h b 1 and m1=0.5295=m1hb2subscript10.5295superscriptsubscript12m_{1}=0.5295=m_{1}^{hb2}m _ 1 = 0.5295 = m _ 1 ^ h b 2 , respectively, is properly illustrated in figures (5(a)) and (16). Following the initial Hopf bifurcation at m1=m1hb1subscript1superscriptsubscript11m_{1}=m_{1}^{hb1}m _ 1 = m _ 1 ^ h b 1 , there is a noticeable presence of oscillations in the populations of the three species. As the parameter m1subscript1m_{1}m _ 1 continues to rise, the amplitude of these oscillations steadily increases. However, after reaching a certain threshold, the amplitude begins to diminish and eventually stabilises at the second Hopf bifurcation at m1=m1hb2subscript1superscriptsubscript12m_{1}=m_{1}^{hb2}m _ 1 = m _ 1 ^ h b 2 . Thereby, the significance of the parameter m1subscript1m_{1}m _ 1 within the system (2) is further emphasised. The figure (5(a)) clearly illustrates the bubbling phenomenon in the x1m1subscript¥1subscript1x_{1}-m_{1}x _ 1 - m _ 1 plane. On the other hand, figures (16(a)) and (16(b)) illustrate the presence of a fluctuation bubble resulting from the appearance of two Hopf bifurcations at m1=m1hb1subscript1superscriptsubscript11m_{1}=m_{1}^{hb1}m _ 1 = m _ 1 ^ h b 1 and m1=m1hb2subscript1superscriptsubscript12m_{1}=m_{1}^{hb2}m _ 1 = m _ 1 ^ h b 2 on the x2m1subscript¥2subscript1x_{2}-m_{1}x _ 2 - m _ 1 and x3m1subscript¥3subscript1x_{3}-m_{1}x _ 3 - m _ 1 planes, respectively. In the given system (1), the parameters associated with the odour of the intermediate predator and top predator are denoted as a1subscript1a_{1}a _ 1 and a2subscript2a_{2}a _ 2 , respectively. With the following parameter values set, the figure (17) is produced: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.6subscript20.6m_{2}=0.6m _ 2 = 0.6, r2=1subscript21r_{2}=1r _ 2 = 1, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r3=1subscript31r_{3}=1r _ 3 = 1, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r4=3subscript43r_{4}=3r _ 4 = 3, a2=1subscript21a_{2}=1a _ 2 = 1, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, r=0.010.01r=0.01r = 0.01, and continuously varying the parameter a1subscript1a_{1}a _ 1 . Based on the figure (17), it is evident that the parameter a1subscript1a_{1}a _ 1 has a noticeable impact on the system (2). If the parameter a1<3.024subscript13.024a_{1}<-3.024a _ 1 < - 3.024, the top predator within the system is at risk of extinction. This is shown in figures (17(a)) and (17(b)). This is because a transcritical bifurcation occurs at a1=3.024subscript13.024a_{1}=-3.024a _ 1 = - 3.024. In ecological circumstance, this is not possible as the parameter a1subscript1a_{1}a _ 1 is always positive. However, when 3.024<a1<1.4483.024subscript11.448-3.024<a_{1}<1.448- 3.024 < a _ 1 < 1.448, then it is observed that the prey, intermediate predator and the top predator, all coexist within the system (2). This is evident in figure (17(c)). Moreover, there is another transcritical bifurcation that occurs at a1=1.448subscript11.448a_{1}=1.448a _ 1 = 1.448. As a result, the system once again faces the extinction of the top predator. This is illustrated in figure (17(d)). On the other hand, there is another transcritical bifurcation that takes place at a1=1.503subscript11.503a_{1}=1.503a _ 1 = 1.503. This bifurcation results in the removal of predators, leaving only the prey species in the system, as seen in figure (17(e)). Thus, the parameter a1subscript1a_{1}a _ 1 is capable of causing transcritical bifurcations in the system (2). Consequently, the coexistence of all species in the system is highly dependent on the value of this parameter. Figure (18(a)) represents the coexistence equilibrium curve which is generated using some fixed parameter values and constantly changing the value of the parameter a2subscript2a_{2}a _ 2 . The fixed parameter values are: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.6subscript20.6m_{2}=0.6m _ 2 = 0.6, r2=1subscript21r_{2}=1r _ 2 = 1, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r3=1subscript31r_{3}=1r _ 3 = 1, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r4=3subscript43r_{4}=3r _ 4 = 3, a1=1subscript11a_{1}=1a _ 1 = 1, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, and r=0.010.01r=0.01r = 0.01. Based on the information shown in figure (18(a)), it is clear that the parameter a2subscript2a_{2}a _ 2 plays a significant role in the system (2). It has the ability to cause a Hopf bifurcation and a transcritical bifurcation within the system. When a2<0.839221subscript20.839221a_{2}<0.839221a _ 2 < 0.8392211, all three populations within the system experiences fluctuations that are later stabilised by a hopf bifurcation occurring at a2=0.839221subscript20.839221a_{2}=0.839221a _ 2 = 0.839221. This is illustrated in the figure (18(b)). Therefore, for a range of values between 0.8392210.8392210.8392210.839221 and 1.2493021.2493021.2493021.249302 for a2subscript2a_{2}a _ 2 , the system (2) exhibits the coexistence of all three species. The evidence is readily apparent in the figure (18(c)). Nevertheless, when a2>1.249302subscript21.249302a_{2}>1.249302a _ 2 > 1.249302, the top predator disappears from the system, rendering coexistence unattainable as a transcritical bifurcation takes place at a2=1.249302subscript21.249302a_{2}=1.249302a _ 2 = 1.249302. This is depicted in the figure (18(d)). Thus, the significance of the role parameter a2subscript2a_{2}a _ 2 in either facilitating or preventing coexistence is well established. In this part, we investigate the consequences resulting from the act of harvesting in the system (2). Initially, we will try to elucidate the significance of the catchability constant qqq in the system (2). To be able to accomplish this, we have conducted simulations involving several figures that pertain to the significance of the catchability constant qqq. Figure (5(c)) demonstrates that changes in the parameter qqq can lead to various forms of bifurcations. Figure (19) shows phase portraits at different values of the parameter qqq while keeping the other parameter values unchanged. The fixed parameter values are: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, r2=1subscript21r_{2}=1r _ 2 = 1, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, r4=3subscript43r_{4}=3r _ 4 = 3, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, r3=1subscript31r_{3}=1r _ 3 = 1, and r=0.010.01r=0.01r = 0.01. From figure (5(c)), it is apparent that changing the parameter qqq leads to a Hopf bifurcation, which in turn affects the stability of the system (2). For the given parameter values, namely r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, r2=1subscript21r_{2}=1r _ 2 = 1, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r4=3subscript43r_{4}=3r _ 4 = 3, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.30.3q=0.3q = 0.3, r3=1subscript31r_{3}=1r _ 3 = 1, and r=0.010.01r=0.01r = 0.01, when the value of qqq is less than a certain threshold q=1.04003=qhbp1.04003superscriptq=1.04003=q^{hbp}q = 1.04003 = q ^ h b p , the system described by equation (2) exhibits instability around the point EcsubscriptE_{c}E _ c . We obtain the values: N1=1.68>0subscript11.680N_{1}=1.68>0N _ 1 = 1.68 > 0, N2=0.064>0subscript20.0640N_{2}=0.064>0N _ 2 = 0.064 > 0, N3=0.11>0subscript30.110N_{3}=0.11>0N _ 3 = 0.11 > 0, and N1N2N3=0.0018<0subscript1subscript2subscript30.00180N_{1}N_{2}-N_{3}=-0.0018<0N _ 1 N _ 2 - N _ 3 = - 0.0018 < 0, confirming the instability of EcsubscriptE_{c}E _ c . Figure (19(a)) depicts the precise scenario. More precisely, the system (2) exhibits fluctuations in the populations of the three species. As a result of a Hopf bifurcation at q=1.04003=qhbp1.04003superscriptq=1.04003=q^{hbp}q = 1.04003 = q ^ h b p , the stability of the system (2) changes and becomes stable for all parameter values q>qhbpsuperscriptq>q^{hbp}q > q ^ h b p , while keeping all other parameter values unchanged. When the value of qqq is set to 1.04003=qhbp1.04003superscript1.04003=q^{hbp}1.04003 = q ^ h b p , keeping all other parameters unchanged, we observe that N1=1.6>0subscript11.60N_{1}=1.6>0N _ 1 = 1.6 > 0, N2=0.06>0subscript20.060N_{2}=0.06>0N _ 2 = 0.06 > 0, N3=0.1>0subscript30.10N_{3}=0.1>0N _ 3 = 0.1 > 0, N1N2N3=0subscript1subscript2subscript30N_{1}N_{2}-N_{3}=0N _ 1 N _ 2 - N _ 3 = 0, and N1(qhbp)N2²(qhbp)+N2(qhbp)N1²(qhbp)N3²(qhbp)=0.002 0subscript1superscriptsuperscriptsubscript2²superscriptsubscript2superscriptsuperscriptsubscript1²superscriptsuperscriptsubscript3²superscript0.0020N_{1}(q^{hbp})N_{2}^{{}^{ ^{hbp})-N_{3}^{{}^{ 0N _ 1 ( q ^ h b p ) N _ 2 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( q ^ h b p ) + N _ 2 ( q ^ h b p ) N _ 1 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( q ^ h b p ) - N _ 3 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( q ^ h b p ) = 0.002 0. These findings confirm the presence of a Hopf bifurcation. It can be seen that the system (2) exhibits stable characteristics for the parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r2=1subscript21r_{2}=1r _ 2 = 1, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r3=1subscript31r_{3}=1r _ 3 = 1, c1=1subscript11c_{1}=1c _ 1 = 1, q=22q=2q = 2, r4=3subscript43r_{4}=3r _ 4 = 3, and r=0.010.01r=0.01r = 0.01, as depicted in figure (19(b)). Based on the given parameter values, it can be observed that N1=1.66>0subscript11.660N_{1}=1.66>0N _ 1 = 1.66 > 0, N2=0.06>0subscript20.060N_{2}=0.06>0N _ 2 = 0.06 > 0, N3=0.1>0subscript30.10N_{3}=0.1>0N _ 3 = 0.1 > 0, and N1N2N3=0.002>0subscript1subscript2subscript30.0020N_{1}N_{2}-N_{3}=0.002>0N _ 1 N _ 2 - N _ 3 = 0.002 > 0. These results provide evidence for the stability of EcsubscriptE_{c}E _ c . This discussion provides us with compelling evidence that highlights the significance of the parameter qqq throughout the system (2). From the preceding discussions, it is evident that when the catchability constant qqq is extremely low, meaning that the likelihood of capturing prey through harvesting efforts is small, the stable coexistence of all three populations becomes challenging. This is due to fluctuations in the populations of all three species within the system (2). This implies that coexistence is feasible in this scenario, albeit the population continues to fluctuate. As the likelihood of capturing prey, represented by the catchability constant qqq, grows, a stable coexistence of all three species becomes possible due to the emergence of a Hopf bifurcation. This indicates that for the three species in the system to coexist, a high catchability constant qqq is required. Although a higher catchability constant qqq can lead to a reduction in the prey population over time in the long-term dynamics. Thus, the catchability constant qqq plays a crucial role within the system. Now, we will shift our attention to the significance of the parameter rrr in the system (2). In order to gain a deeper comprehension of the influence of this parameter on the system, we conduct simulations using various values of the parameter rrr. In order to produce the figure (5(d)), we need to change the parameter rrr and employ the following parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, r2=1subscript21r_{2}=1r _ 2 = 1, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, r3=1subscript31r_{3}=1r _ 3 = 1, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, r4=3subscript43r_{4}=3r _ 4 = 3, q=0.50.5q=0.5q = 0.5. This figure elegantly depicts the manifestation of several bifurcations as the parameter rrr undergoes modifications. In order to obtain figure (20(a)), we utilise the following parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r2=1subscript21r_{2}=1r _ 2 = 1, r3=1subscript31r_{3}=1r _ 3 = 1, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, r4=3subscript43r_{4}=3r _ 4 = 3, and r=0.0150.015r=0.015r = 0.015. Alternatively, in order to simulate figure (20(b)), we employ the following parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r2=1subscript21r_{2}=1r _ 2 = 1, r3=1subscript31r_{3}=1r _ 3 = 1, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, r4=3subscript43r_{4}=3r _ 4 = 3, and r=0.040.04r=0.04r = 0.04. Based on the information provided in figure (5(d)), it can be inferred that if the parameter rrr is smaller than r=0.0208005=rhbp0.0208005superscriptr=0.0208005=r^{hbp}r = 0.0208005 = r ^ h b p , the system (2) exhibits instability in the vicinity of EcsubscriptE_{c}E _ c . When the parameter values are set to r=0.0150.015r=0.015r = 0.015, while keeping all other parameter values unchanged, we find that N1=1.67>0subscript11.670N_{1}=1.67>0N _ 1 = 1.67 > 0, N2=0.064>0subscript20.0640N_{2}=0.064>0N _ 2 = 0.064 > 0, N3=0.10>0subscript30.100N_{3}=0.10>0N _ 3 = 0.10 > 0, and N1N2N3=0.0007<0subscript1subscript2subscript30.00070N_{1}N_{2}-N_{3}=-0.0007<0N _ 1 N _ 2 - N _ 3 = - 0.0007 < 0. This confirms the instability of EcsubscriptE_{c}E _ c . This is accurately illustrated in figure (20(a)). A Hopf bifurcation occurs at the value of rrr equal to r=0.0208005=rhbp0.0208005superscriptr=0.0208005=r^{hbp}r = 0.0208005 = r ^ h b p . This is evident as all the criteria specified in theorem (14) are met for these parameter values. More precisely, we have N1=1.6>0subscript11.60N_{1}=1.6>0N _ 1 = 1.6 > 0, N2=0.06>0subscript20.060N_{2}=0.06>0N _ 2 = 0.06 > 0, N3=0.1>0subscript30.10N_{3}=0.1>0N _ 3 = 0.1 > 0, N1N2N3=0subscript1subscript2subscript30N_{1}N_{2}-N_{3}=0N _ 1 N _ 2 - N _ 3 = 0, and N1(qhbp)N2²(qhbp)+N2(qhbp)N1²(qhbp)N3²(qhbp)=0.125 0subscript1superscriptsuperscriptsubscript2²superscriptsubscript2superscriptsuperscriptsubscript1²superscriptsuperscriptsubscript3²superscript0.1250N_{1}(q^{hbp})N_{2}^{{}^{ ^{hbp})-N_{3}^{{}^{ 0N _ 1 ( q ^ h b p ) N _ 2 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( q ^ h b p ) + N _ 2 ( q ^ h b p ) N _ 1 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( q ^ h b p ) - N _ 3 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( q ^ h b p ) = 0.125 0. The system (2) exhibits stability around EcsubscriptE_{c}E _ c for all values of the parameter rtbp>r>rhbpsuperscript¡superscriptr^{tbp}>r>r^{hbp}r ^ t b p > r > r ^ h b p . It can be established numerically through the use of the following parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, r2=1subscript21r_{2}=1r _ 2 = 1, r3=1subscript31r_{3}=1r _ 3 = 1, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, r4=3subscript43r_{4}=3r _ 4 = 3, and r=0.040.04r=0.04r = 0.04. At these parameter values, N1=1.6>0subscript11.60N_{1}=1.6>0N _ 1 = 1.6 > 0, N2=0.06>0subscript20.060N_{2}=0.06>0N _ 2 = 0.06 > 0, N3=0.1>0subscript30.10N_{3}=0.1>0N _ 3 = 0.1 > 0, and N1N2N3=0.002>0subscript1subscript2subscript30.0020N_{1}N_{2}-N_{3}=0.002>0N _ 1 N _ 2 - N _ 3 = 0.002 > 0. This substantiates the stability of EcsubscriptE_{c}E _ c in accordance with theorem (8). This is accurately demonstrated in figure (20(b)). A transcritical bifurcation occurs when the value of rrr reaches r=1.5074136=rtbp1.5074136superscript¡r=1.5074136=r^{tbp}r = 1.5074136 = r ^ t b p , causing a shift in the stability of the system (2) and making it unstable. In order to have a better understanding of this exact situation, we use parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r2=1subscript21r_{2}=1r _ 2 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r3=1subscript31r_{3}=1r _ 3 = 1, r4=3subscript43r_{4}=3r _ 4 = 3, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r5=1subscript51r_{5}=1r _ 5 = 1, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, and r=1.61.6r=1.6r = 1.6. At these parameter values, we notice that N3subscript3N_{3}N _ 3 is equal to -0.01, which is less than 0. This demonstrates the instability of EcsubscriptE_{c}E _ c as per theorem (8). Furthermore, for these specific parameter values, all the conditions outlined in theorem (7) are met, indicating the stability of Etsubscript¡E_{t}E _ t . This is precisely shown in figure (20(c)). The level of harvesting effort plays a crucial role in predator-prey systems, as it has far-reaching effects on both the target species and the overall ecosystem. Efficiently managing the harvesting effort is crucial for maintaining the population™s long-term sustainability. The preceding calculations show that as the amount of harvesting effort increases, the prey population decreases, which is consistent with biological principles. In addition, it has been observed that when the level of harvesting effort is low, the populations of all three species exhibit fluctuations. When the level of harvesting effort is increased, the system becomes stable, allowing for the coexistence of all three populations. This is the result of the manifestation of a Hopf bifurcation. Interestingly, when the level of harvesting effort is further increased, the coexistence of all species becomes unattainable, and the top predator becomes extinct. Some special cases: In this part, we delve into certain unique scenarios that often arise related to the harvesting of prey species within natural settings. In the first scenario, our focus is on a case where the catchability coefficient (q)(q)( q ) is insignificant, even with normal harvesting effort. In the second scenario, we consider a situation where the harvesting effort (r)(r)( r ) is minimal, while the catchability coefficient (q)(q)( q ) remains within normal levels. In the third scenario, there is a situation where both the harvesting effort (r)(r)( r ) and the catchability coefficient (q)(q)( q ) are negligible at the same time. Scenario 1: When the catchability coefficient (q)q(q)( q ) is negligible, even with normal harvesting effort This scenario represents that despite considerable exertion in the process of harvesting, no individuals from the population are being harvested. This indicates the presence of some sort of obstacle impeding the successful capture of prey, despite considerable exertion. It is essential to identify and tackle the underlying causes, which can be technological, behavioural, environmental, or related to population absence, in order to effectively manage and implement sustainable harvesting procedures. In order to have a clearer understanding of this specific situation, we employ certain parameter values to simulate this exact scenario. The parameter values are as follows: r1=2subscript12r_{1}=2r _ 1 = 2, r4=3subscript43r_{4}=3r _ 4 = 3, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r2=1subscript21r_{2}=1r _ 2 = 1, r3=1subscript31r_{3}=1r _ 3 = 1, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, q=00q=0q = 0, r5=1subscript51r_{5}=1r _ 5 = 1, and r=0.010.01r=0.01r = 0.01. By utilising these specific parameter values, we have generated figure (21). The time series of all three populations at q=00q=0q = 0 is depicted in figure (21(a)). The phase portrait depicting the dynamics of the three populations is illustrated in figure (21(b)). Both the figures (21(a)) and (21(b)) are generated using the initial value (0.8,0.6,0.8). When the catchability coefficient is insignificant, the stable cohabitation of the three species is not possible, even with the standard harvesting effort. This is clearly shown in figure (21), where fluctuations in the populations of the three species occur within the system (2) when q=00q=0q = 0. Moreover, the occurrence of a limit cycle can also be seen in figure (21(b)). Looking at it from an ecological standpoint, it becomes evident that when the catchability coefficient is negligible, meaning that only a few prey are harvested even with consistent harvesting efforts, it results in population fluctuations among the three species. Thus, to promote a harmonious coexistence among all species in the system (2), it is essential to maintain a substantial level of harvesting output. Thus, the catchability coefficient (q)(q)( q ) plays a crucial role in maintaining the coexistence of all three species in the system. Scenario 2: When the harvesting effort (r)r(r)( r ) is minimal, while the catchability coefficient (q)q(q)( q ) remains within normal levels In this scenario, only a small amount of effort invested in harvesting results in a significantly large number of catches, suggesting a high catchability coefficient. This scenario can occur in an ecological setting. For example, in a small region where prey are densely populated and are easily affected by the harvesting technique used, minimal effort could lead to a significant increase in the number of prey caught. Utilising highly effective harvesting equipment or technology can result in a significant increase in the rate of catching preys, while requiring minimal work. For instance, sophisticated nets, traps, or fishing methods that are very efficient in capturing the desired species. Gaining comprehension of these circumstances can aid in formulating more effective harvesting tactics and management protocols. In order to simulate this exact scenario, the following values of the parameters are used: r1=2subscript12r_{1}=2r _ 1 = 2, r4=3subscript43r_{4}=3r _ 4 = 3, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, r2=1subscript21r_{2}=1r _ 2 = 1, r3=1subscript31r_{3}=1r _ 3 = 1, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, r5=1subscript51r_{5}=1r _ 5 = 1, q=0.50.5q=0.5q = 0.5, and r=0.0010.001r=0.001r = 0.001. Based on the given parameter values, figure (22) has been generated. From this figure, it is understandable the manner in which the system™s long-term dynamics are affected by the harvesting effort (r)(r)( r ). When the harvesting effort is kept to a minimum, fluctuations among the populations of the three species within the system (2) are observed. This makes it extremely challenging for the three species to stably coexist in such circumstances. Increasing the harvesting effort enhances the stability of coexistence, among the three species. That is why it is a crucial parameter within the system (2). Figures (22(a)) and (22(b)) show the phase portrait and time series, respectively, illustrating the dynamics of all three populations under minimal harvesting effort. Scenario 3: When there is a situation where both the harvesting effort (r)r(r)( r ) and the catchability coefficient (q)q(q)( q ) are negligible at the same time In this situation, it can be deduced that there is no allocation of resources or efforts towards harvesting the prey, resulting in the absence of any individuals being captured. Without any harvesting effort, there would be no catch. This scenario is simple and straightforward, as the lack of effort results in no interaction with the prey population. This scenario could arise in designated conservation areas where the act of harvesting is strictly forbidden, resulting in no attempts being made to harvest prey. It can also happen in situations where there is no desire or requirement for capturing the prey, resulting in little effort and inability to capture them. We utilise the following parameter values to gain a deeper understanding of this particular scenario: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.01½0.01 = 0.01, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, r2=1subscript21r_{2}=1r _ 2 = 1, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r3=1subscript31r_{3}=1r _ 3 = 1, r4=3subscript43r_{4}=3r _ 4 = 3, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, q=00q=0q = 0, and r=00r=0r = 0. By utilising these specific parameter settings, we have conducted a series of simulations and acquired diverse figures. Figures (23(a)) and (23(b)) depict the phase portrait and time series, respectively, of the three populations in the system (2) in the absence of harvesting. These results demonstrate that achieving steady coexistence in the above described system is extremely difficult without harvesting. Population fluctuations are observable in all three species in the system (2) when harvesting is not taking place. Therefore, the significance of harvesting in this system is explicitly established. Within this part, we explore the impact of prey odour on the system (2). The parameter Î²½ in this system reflects the coefficient that quantifies the impact of prey odour. Figure (5(e)) vividly demonstrates the significance of the parameter Î²½ Figure (5(e)) is obtained using the following parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.5½0.5 = 0.5, r2=1subscript21r_{2}=1r _ 2 = 1, r3=1subscript31r_{3}=1r _ 3 = 1, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, r4=3subscript43r_{4}=3r _ 4 = 3, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, and r=0.010.01r=0.01r = 0.01. Based on figure (5(e)), it is apparent that the parameter Î²½ significantly contributes to the stability of the system (2). Fluctuations in the populations of the three species within the system can be detected when the parameter value Î²<0.020337=Î²hbp½0.020337superscript½ < 0.020337 = Î² ^ h b p . As an example, we will examine the parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0.015<Î²hbp½0.015superscript½ = 0.015 < Î² ^ h b p , m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r2=1subscript21r_{2}=1r _ 2 = 1, r3=1subscript31r_{3}=1r _ 3 = 1, c1=1subscript11c_{1}=1c _ 1 = 1, q=0.50.5q=0.5q = 0.5, r4=3subscript43r_{4}=3r _ 4 = 3, and r=0.010.01r=0.01r = 0.01. Given this parameter configuration, we get the values N1=1.6>0subscript11.60N_{1}=1.6>0N _ 1 = 1.6 > 0, N2=0.06>0subscript20.060N_{2}=0.06>0N _ 2 = 0.06 > 0, N3=0.1>0subscript30.10N_{3}=0.1>0N _ 3 = 0.1 > 0, and N1N2N3=0.0007<0subscript1subscript2subscript30.00070N_{1}N_{2}-N_{3}=-0.0007<0N _ 1 N _ 2 - N _ 3 = - 0.0007 < 0. Therefore, this verifies the instability of EcsubscriptE_{c}E _ c . This is readily apparent in figure (24(a)). A Hopf bifurcation occurs at Î²=0.020337=Î²hbp½0.020337superscript½ = 0.020337 = Î² ^ h b p , leading to the stabilisation of the system. Given the parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=Î²hbp½superscript½ = Î² ^ h b p , m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, r2=1subscript21r_{2}=1r _ 2 = 1, r3=1subscript31r_{3}=1r _ 3 = 1, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, c1=1subscript11c_{1}=1c _ 1 = 1, r4=3subscript43r_{4}=3r _ 4 = 3, q=0.50.5q=0.5q = 0.5, and r=0.010.01r=0.01r = 0.01, we find that N1=1.6>0subscript11.60N_{1}=1.6>0N _ 1 = 1.6 > 0, N2=0.06>0subscript20.060N_{2}=0.06>0N _ 2 = 0.06 > 0, N3=0.1>0subscript30.10N_{3}=0.1>0N _ 3 = 0.1 > 0, N1N2N3=0subscript1subscript2subscript30N_{1}N_{2}-N_{3}=0N _ 1 N _ 2 - N _ 3 = 0, and N1(qhbp)N2²(qhbp)+N2(qhbp)N1²(qhbp)N3²(qhbp)=0.133 0subscript1superscriptsuperscriptsubscript2²superscriptsubscript2superscriptsuperscriptsubscript1²superscriptsuperscriptsubscript3²superscript0.1330N_{1}(q^{hbp})N_{2}^{{}^{ ^{hbp})-N_{3}^{{}^{ 0N _ 1 ( q ^ h b p ) N _ 2 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( q ^ h b p ) + N _ 2 ( q ^ h b p ) N _ 1 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( q ^ h b p ) - N _ 3 ^ start_FLOATSUPERSCRIPT ² end_FLOATSUPERSCRIPT ( q ^ h b p ) = 0.133 0. Therefore, all the requirements outlined in theorem (14) for the existence of Hopf bifurcation are met, resulting in a Hopf bifurcation at Î²=Î²hbp½superscript½ = Î² ^ h b p . When the parameter value Î²½ is set to Î²=0.04>Î²hbp½0.04superscript½ = 0.04 > Î² ^ h b p , and all other parameter values remaining unchanged, we have the following results: N1=1.6subscript11.6N_{1}=1.6N _ 1 = 1.6, N2=0.07subscript20.07N_{2}=0.07N _ 2 = 0.07, N3=0.1subscript30.1N_{3}=0.1N _ 3 = 0.1, and N1N2N3=0.002subscript1subscript2subscript30.002N_{1}N_{2}-N_{3}=0.002N _ 1 N _ 2 - N _ 3 = 0.002. All of these values are greater than zero. Therefore, based on theorem (8), the system (2) displays stability near EcsubscriptE_{c}E _ c given the specified parameter values. This specific situation is depicted in figure (24(b)). Based on the previous discussion, it readily apparent that when the coefficient of prey odour effect increases, the intermediate predator becomes more attracted to the prey due to the odour released by the prey. As a result, the population of prey species decreases due to an increase in the encounter rate between the intermediate predator and prey, leading to a higher predation rate. This is precisely what happens within the system (2) as evident from figure (5(e)). Interestingly, it has been noted that a minimal amount of prey odour effect renders the stable coexistence of all three species within the system unattainable, resulting in oscillations in the populations of the three species. However, when the intensity of the prey odour is significantly enhanced, a Hopf bifurcation occurs, resulting in the long-term coexistence of all three populations within the system (2). Therefore, it has been discovered that the influence of prey odour aids in fostering a state of lasting cohabitation within this particular system. Special case: When the effect of prey odour is negligible, i.e., Î²=0Î²0 = 0 In this portion, we aim to examine the scenario in which the influence of prey odour becomes insignificant within the system (2), specifically when Î²=0½0 = 0. In this scenario, the predator is unable to utilise scent as a means of locating its prey, hence the impact of prey odour in predation becomes negligible. In order to comprehend the scenario in which Î²=0½0 = 0, we conducted many simulations utilising specific parameter values: r1=2subscript12r_{1}=2r _ 1 = 2, r5=1subscript51r_{5}=1r _ 5 = 1, Î²=0½0 = 0, m1=0.5subscript10.5m_{1}=0.5m _ 1 = 0.5, m2=0.5subscript20.5m_{2}=0.5m _ 2 = 0.5, d1=0.25subscript10.25d_{1}=0.25d _ 1 = 0.25, d2=0.5subscript20.5d_{2}=0.5d _ 2 = 0.5, r2=1subscript21r_{2}=1r _ 2 = 1, r3=1subscript31r_{3}=1r _ 3 = 1, c1=1subscript11c_{1}=1c _ 1 = 1, r4=3subscript43r_{4}=3r _ 4 = 3, q=0.50.5q=0.5q = 0.5, and r=0.010.01r=0.01r = 0.01. We have produced two figures illustrating the circumstance where Î²=0½0 = 0. These are figures (25(a)) and (25(b)). Both of these figures are produced using the initial value of (0.8, 0.6, 0.8). Figures (25(a)) and (25(b)) illustrate the time series and phase portrait of the three populations species within the system (2), respectively. Looking at the aforementioned two figures, it is clear that the populations of the three species experience oscillations when there is no influence of prey odour in the system, specifically when Î²=0½0 = 0. To be more precise, the system (2) exhibits instability in the vicinity of EcsubscriptE_{c}E _ c . On top of that, figure (25(b)) clearly displays the presence of a limit cycle. In terms of ecology, the absence of the prey odour effect in the system makes it difficult for all three species to cohabit in a stable manner over a long period of time. This phenomenon, where the impact of prey odour becomes negligible within the system, is frequently seen in nature. As mentioned before, it has been discovered that predator species frequently mask their scents to avoid being detected by prey, enabling them to launch surprise attacks. Similarly, it has been observed that many prey species use similar tactics to evade detection by predators. Some prey species have been observed using the scent of other non-prey species as a way to conceal their own odour and avoid being detected by predators [68, 69, 70, 71]. For example, some caterpillars of the butterflies (Biston robustum) and (Mechanitis polymnia) have shown the remarkable ability to imitate the scent of the plants they eat and live on. This clever strategy helps them avoid being detected by predatory ants [69, 71]. The limpet species (Notoacmea palacea) employs chemical mimicry to decrease the likelihood of being preyed upon by matching the organisms it feeds on and resides with [68, 71]. Based on the numerical evidence provided, it appears that when the impact of prey odour on predators becomes too little, oscillations in the populations of all three species emerge. This ultimately leads to an unstable state of coexistence in the long term. Once the parameter Î²½ surpasses a specific non-zero value, the coexistence of all species becomes stable. This suggests that the intermediate predator™s level of attraction to the prey, caused by the prey™s odour, plays a crucial role in the coexistence of the three species within the system under investigation. Our study delves into a model that investigates the intricate dynamics of a odour-mediated three species food chain. This model incorporates a commonly observed phenomenon, notably the influence of odour. It incorporates predator odour-induced prey refuge and prey harvesting. The model equations are formulated in two different types: one employing ordinary differential equations and the other utilising Caputo fractional-order differential equations to incorporate the memory effect. The paper explores the fundamental prerequisites, such as the existence, non-negativity, and uniqueness of the solutions of the system. The model™s biologically plausible equilibrium states are established. All ecologically feasible equilibrium points are analysed in terms of their local stability. The article extensively covers the phenomenon of bifurcation, with a particular focus on the occurrence of Hopf bifurcations. An analysis has been conducted to compare the two systems, ODE (2) and FDE (3). We perform numerical simulations with biologically attainable parameters to illustrate the behaviour of the system close to the equilibrium points. It is found that our system can attain a maximum four equilibrium states: the vanishing equilibrium point Evsubscript£E_{v}E _ v , the axial equilibrium point EasubscriptE_{a}E _ a , the top predator free equilibrium point Etsubscript¡E_{t}E _ t , and the coexisting equilibrium point EcsubscriptE_{c}E _ c . From an ecological standpoint, the vanishing equilibrium point Evsubscript£E_{v}E _ v represents the state in which all three species in the bio-system become extinct and the system breaks down. The axial equilibrium point EasubscriptE_{a}E _ a represents the situation in which the two predators vanish, leaving only the prey species. The top predator free equilibrium point Etsubscript¡E_{t}E _ t indicates the state in which only the top predator becomes extinct. The coexisting equilibrium point EcsubscriptE_{c}E _ c reflects the scenario in which all three species coexist in the system. Based on our research findings, it can be inferred that if the rate of harvesting exceeds the growth rate of the prey species, the system will ultimately reach the vanishing equilibrium point, resulting in the extinction of all three species and the collapse of the system. The local stability of the vanishing equilibrium point Evsubscript£E_{v}E _ v precludes the possibility of the existence of the axial equilibrium point EasubscriptE_{a}E _ a and the top predator free equilibrium point Etsubscript¡E_{t}E _ t . Ecologically, reaching the vanishing equilibrium point refers to the instant at which all species in our system become extinct. At this point, there is no possibility of any growth in the species unless we reintroduce them, which is biologically true. The local stability of both the axial and top predator free equilibrium points negates the local stability of the vanishing equilibrium scores. From a biological perspective, this means that when the bio-system reaches a state where either just the prey survives in the system or the top predator becomes extinct from the system, the system has no way to collapse. In other words, all three species cannot become extinct simultaneously, as evidenced from our model. Upon satisfying certain parametric conditions, our discussed bio-system can achieve the coexistence equilibrium point. From an ecological point of view, it is evident that the coexistence of all three species is feasible within this system. Existing literature has already established that when prey species detect the presence of a nearby predator through olfactory cues, they become vigilant and employ various anti-predator strategies to minimise encounters. One effective strategy is for prey species to seek refuge in order to prevent predators from reaching them. In this study, we examine the consequences of refuge behaviour exhibited by prey and intermediate predators in response to their respective predators™ odour. The prey™s refuge behaviour towards the intermediate predator has been discovered to have a significant impact on the system. This parameter has the potential to trigger two Hopf bifurcations in the system, resulting in oscillations in the populations of the three species and giving birth to the intriguing bubbling phenomenon. Moreover, this parameter has the power to trigger a transcritical bifurcation in the system, altering the stability of the equilibrium points. Remarkably, it has been seen that when prey species seek refuge at a high rate, it can lead to the extinction of predators, making coexistence impossible. Moreover, the influence of the intermediate predator™s odour on the dynamics of the system is established. This impact is readily apparent, as the parameter related to the intermediate predator™s odour can trigger numerous transcritical bifurcations in the system. It has been discovered that it has a substantial influence on facilitating the coexistence of all three species in the system. Similarly, the refuge behaviour of the intermediate predator against the top predator also plays a crucial role in the long-term dynamics of the system. This parameter can additionally trigger oscillations in the system by inducing a Hopf bifurcation. Interestingly, an increase in this parameter may contribute to an overall reduction in the size of the prey population. Thus, the act of seeking refuge performed by individuals to avoid being preyed upon can have a significant impact on the long-term dynamics of the system. Additionally, it has been observed that the parameter associated with the odour of the top predator can trigger both a Hopf bifurcation and a transcritical bifurcation in the system. When the concentration of the top predator™s odour is insignificant, no refuge is observed among the population of intermediate predators. However, even under such a scenario, it is still conceivable for the three species to coexist. Though fluctuations in the populations of all three species have been noted in this scenario. The study reveals that prey harvesting parameters have the potential to induce both Hopf bifurcation and transcritical bifurcations in the system, thus highlighting the significance of these parameters in the system. When the rate of successfully capturing prey through harvesting activities is low, it becomes challenging for all three species to maintain a stable coexistence. Furthermore, it is evident that for the three species in the system to coexist, a substantial catchability constant qqq is required, despite the fact that a greater catchability constant qqq can lead to a reduction in the prey population in the long-term dynamics. Additionally, it has been noted that an increase in the level of harvesting effort leads to a drop in the prey population, aligning with established biological principles. Surprisingly, when the intensity of the harvesting effort reaches an exceedingly high value, it becomes impossible for all species to cohabit, and the top predator faces the risk of extinction. It has been found that the level of attraction of the intermediate predator to the prey, influenced by the prey™s odour, is a critical factor in the coexistence of the three species in the system. It is intriguing that the absence of the prey odour effect in the system poses a challenge for all three species to live together, leading to fluctuations in their populations. However, with a significant increase in the strength of the prey odour™s effect, a Hopf bifurcation arises, leading to the sustained coexistence of all three populations in the system. In addition, as the impact of prey odour becomes stronger, the intermediate predator becomes more drawn to the prey, leading to a decrease in the population of the prey species due to an increase in the rate of encounters between the intermediate predator and the prey. This ultimately results in an elevated predation rate. The effect of memory on the system is also studied using a Caputo-type fractional-order derivative of order Î±¼ into the modelling of the system. It is observed that as the order of the fractional derivative decreases, the system under consideration becomes more and more stable. From an ecological perspective, it is possible to infer that in this system, as an individual™s memory retention ability diminishes, they become unable to recall their previous experiences pertaining to their early life history. As a result, their consciousness of their immediate surroundings drops, leading to the instabilities and fluctuating cohabitation of the three species within the system. Therefore, it can be concluded that the memory of the individuals in this system is crucial for promoting the stability of the cohabitation of the three species within the system. Furthermore, this model can be broadened to include the relationship between a species™ odour and its rate of harvesting. In this study, we have found, through a literature survey, a substantial body of ecological studies indicating the correlation between the odour of a species and its harvesting rate, although this aspect is not addressed in this document. This opens avenues for further investigation. Data Availability Statement No data of any source has been used in this study.",
        "keywords": ""
    }
]